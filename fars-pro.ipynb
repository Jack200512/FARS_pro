{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12717151,"sourceType":"datasetVersion","datasetId":8037752},{"sourceId":12717167,"sourceType":"datasetVersion","datasetId":8037757}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -U faiss-cpu chromadb tiktoken langchain langchain-community sentence-transformers unstructured transformers accelerate argparse numpy os re string sys faiss","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-08-11T05:49:28.103287Z","iopub.execute_input":"2025-08-11T05:49:28.103516Z","iopub.status.idle":"2025-08-11T05:49:34.091007Z","shell.execute_reply.started":"2025-08-11T05:49:28.103491Z","shell.execute_reply":"2025-08-11T05:49:34.090264Z"}},"outputs":[{"name":"stdout","text":"Collecting faiss-cpu\n  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\nCollecting chromadb\n  Downloading chromadb-1.0.16-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\nCollecting tiktoken\n  Downloading tiktoken-0.11.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\nCollecting langchain\n  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\nCollecting langchain-community\n  Downloading langchain_community-0.3.27-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\nCollecting sentence-transformers\n  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\nCollecting unstructured\n  Downloading unstructured-0.18.11-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nCollecting transformers\n  Downloading transformers-4.55.0-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nCollecting accelerate\n  Downloading accelerate-1.10.0-py3-none-any.whl.metadata (19 kB)\nCollecting argparse\n  Downloading argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nCollecting numpy\n  Downloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade --force-reinstall --no-cache-dir faiss-cpu\n","metadata":{"trusted":true,"_kg_hide-output":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-08-11T05:49:38.584956Z","iopub.execute_input":"2025-08-11T05:49:38.585656Z","iopub.status.idle":"2025-08-11T05:49:48.097991Z","shell.execute_reply.started":"2025-08-11T05:49:38.585619Z","shell.execute_reply":"2025-08-11T05:49:48.097302Z"}},"outputs":[{"name":"stdout","text":"Collecting faiss-cpu\n  Downloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.0 kB)\nCollecting numpy<3.0,>=1.25.0 (from faiss-cpu)\n  Downloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting packaging (from faiss-cpu)\n  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\nDownloading faiss_cpu-1.11.0.post1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m294.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading numpy-2.3.2-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m298.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m263.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: packaging, numpy, faiss-cpu\n  Attempting uninstall: packaging\n    Found existing installation: packaging 25.0\n    Uninstalling packaging-25.0:\n      Successfully uninstalled packaging-25.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.2 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.3 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.2 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.2 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.2 which is incompatible.\ncupy-cuda12x 13.4.1 requires numpy<2.3,>=1.22, but you have numpy 2.3.2 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.2 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\nydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.2 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nlangchain-core 0.3.66 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.2 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed faiss-cpu-1.11.0.post1 numpy-2.3.2 packaging-25.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip uninstall -y transformers sentence_transformers scikit-learn scipy numpy\n\n!pip install --no-cache-dir --upgrade numpy==1.26.4\n!pip install --no-cache-dir --upgrade scipy==1.11.4\n!pip install --no-cache-dir --upgrade scikit-learn==1.3.2\n!pip install --no-cache-dir --upgrade transformers==4.37.2\n!pip install --no-cache-dir --upgrade sentence-transformers==2.2.2\n!pip install huggingface_hub==0.23.2","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-08-11T05:49:52.139826Z","iopub.execute_input":"2025-08-11T05:49:52.140132Z","iopub.status.idle":"2025-08-11T05:51:51.326986Z","shell.execute_reply.started":"2025-08-11T05:49:52.140091Z","shell.execute_reply":"2025-08-11T05:51:51.326001Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: transformers 4.52.4\nUninstalling transformers-4.52.4:\n  Successfully uninstalled transformers-4.52.4\nFound existing installation: sentence-transformers 4.1.0\nUninstalling sentence-transformers-4.1.0:\n  Successfully uninstalled sentence-transformers-4.1.0\nFound existing installation: scikit-learn 1.2.2\nUninstalling scikit-learn-1.2.2:\n  Successfully uninstalled scikit-learn-1.2.2\nFound existing installation: scipy 1.15.3\nUninstalling scipy-1.15.3:\n  Successfully uninstalled scipy-1.15.3\nFound existing installation: numpy 2.3.2\nUninstalling numpy-2.3.2:\n  Successfully uninstalled numpy-2.3.2\nCollecting numpy==1.26.4\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m172.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, which is not installed.\nlibpysal 4.9.2 requires scipy>=1.8, which is not installed.\ntreelite 4.4.1 requires scipy, which is not installed.\ncuml-cu12 25.2.1 requires scipy>=1.8.0, which is not installed.\nwoodwork 0.31.0 requires scikit-learn>=1.1.0, which is not installed.\nwoodwork 0.31.0 requires scipy>=1.10.0, which is not installed.\nydata-profiling 4.16.1 requires scipy<1.16,>=1.4.1, which is not installed.\nboruta 0.4.3 requires scikit-learn>=0.17.1, which is not installed.\nboruta 0.4.3 requires scipy>=0.17.0, which is not installed.\nbayesian-optimization 3.0.0 requires scikit-learn<2.0.0,>=1.0.0, which is not installed.\nbayesian-optimization 3.0.0 requires scipy<2.0.0,>=1.0.0; python_version < \"3.13\", which is not installed.\nscikit-surprise 1.1.4 requires scipy>=1.6.0, which is not installed.\nfeaturetools 1.31.0 requires scipy>=1.10.0, which is not installed.\nplotly-express 0.4.1 requires scipy>=0.18, which is not installed.\neasyocr 1.7.2 requires scipy, which is not installed.\nimagehash 4.3.1 requires scipy, which is not installed.\nxgboost 2.0.3 requires scipy, which is not installed.\ncatboost 1.2.8 requires scipy, which is not installed.\nfury 0.12.0 requires scipy>=1.0, which is not installed.\ntpot 0.12.1 requires scikit-learn>=0.22.0, which is not installed.\ntpot 0.12.1 requires scipy>=1.3.1, which is not installed.\nshap 0.44.1 requires scikit-learn, which is not installed.\nshap 0.44.1 requires scipy, which is not installed.\nkaggle-environments 1.17.6 requires scipy>=1.11.2, which is not installed.\nkaggle-environments 1.17.6 requires transformers>=4.33.1, which is not installed.\npymc3 3.11.4 requires scipy>=1.2.0, which is not installed.\ndipy 1.11.0 requires scipy>=1.8, which is not installed.\nopen-spiel 1.6 requires scipy>=1.10.1, which is not installed.\npyldavis 3.4.1 requires scikit-learn>=1.0.0, which is not installed.\npyldavis 3.4.1 requires scipy, which is not installed.\nphik 0.12.4 requires scipy>=1.5.2, which is not installed.\ntheano 1.0.5 requires scipy>=0.14, which is not installed.\nnilearn 0.10.4 requires scikit-learn>=1.0.0, which is not installed.\nnilearn 0.10.4 requires scipy>=1.8.0, which is not installed.\neli5 0.13.0 requires scikit-learn>=0.20, which is not installed.\neli5 0.13.0 requires scipy, which is not installed.\nscikit-learn-intelex 2025.6.1 requires scikit-learn>=0.22, which is not installed.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, which is not installed.\ncategory-encoders 2.7.0 requires scipy>=1.0.0, which is not installed.\nlime 0.2.0.1 requires scikit-learn>=0.18, which is not installed.\nlime 0.2.0.1 requires scipy, which is not installed.\ncesium 0.12.4 requires scikit-learn>=0.22.1, which is not installed.\ncesium 0.12.4 requires scipy>=0.16.0, which is not installed.\ntheano-pymc 1.1.2 requires scipy>=0.14, which is not installed.\nhep-ml 0.8.0 requires scikit-learn>=1.0, which is not installed.\nhep-ml 0.8.0 requires scipy>=1.0.0, which is not installed.\nscikit-optimize 0.10.2 requires scikit-learn>=1.0.0, which is not installed.\nscikit-optimize 0.10.2 requires scipy>=1.1.0, which is not installed.\nmne 1.9.0 requires scipy>=1.9, which is not installed.\njax 0.5.2 requires scipy>=1.11.1, which is not installed.\ntsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", which is not installed.\nsklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\nsklearn-pandas 2.2.0 requires scipy>=1.5.1, which is not installed.\nmatplotlib-venn 1.1.2 requires scipy, which is not installed.\ndatascience 0.17.6 requires scipy, which is not installed.\nlibrosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\nlibrosa 0.11.0 requires scipy>=1.6.0, which is not installed.\nscikit-image 0.25.2 requires scipy>=1.11.4, which is not installed.\nmissingno 0.5.2 requires scipy, which is not installed.\narviz 0.21.0 requires scipy>=1.9.0, which is not installed.\numap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\numap-learn 0.5.7 requires scipy>=1.3.1, which is not installed.\nhyperopt 0.2.7 requires scipy, which is not installed.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\nimbalanced-learn 0.13.0 requires scipy<2,>=1.10.1, which is not installed.\nalbumentations 2.0.8 requires scipy>=1.10.0, which is not installed.\npymc 5.23.0 requires scipy>=1.4.1, which is not installed.\nclarabel 0.11.1 requires scipy, which is not installed.\nscs 3.2.7.post2 requires scipy, which is not installed.\nxarray-einstats 0.9.1 requires scipy>=1.11, which is not installed.\nfastai 2.7.19 requires scikit-learn, which is not installed.\nfastai 2.7.19 requires scipy, which is not installed.\nmizani 0.13.5 requires scipy>=1.8.0, which is not installed.\npeft 0.15.2 requires transformers, which is not installed.\ncvxpy 1.6.6 requires scipy>=1.11.0, which is not installed.\nlightgbm 4.5.0 requires scipy, which is not installed.\nstatsmodels 0.14.4 requires scipy!=1.9.2,>=1.8, which is not installed.\nplotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\npynndescent 0.5.13 requires scikit-learn>=0.18, which is not installed.\npynndescent 0.5.13 requires scipy>=1.0, which is not installed.\npytensor 2.31.4 requires scipy<2,>=1, which is not installed.\nhdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\nhdbscan 0.8.40 requires scipy>=1.0, which is not installed.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\njaxlib 0.5.1 requires scipy>=1.11.1, which is not installed.\nstumpy 1.13.0 requires scipy>=1.10, which is not installed.\nyellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\nyellowbrick 1.5 requires scipy>=1.0.0, which is not installed.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\nmlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\nosqp 1.0.4 requires scipy>=0.13.2, which is not installed.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.4 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.4\nCollecting scipy==1.11.4\n  Downloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from scipy==1.11.4) (1.26.4)\nDownloading scipy-1.11.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m301.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scipy\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nwoodwork 0.31.0 requires scikit-learn>=1.1.0, which is not installed.\nboruta 0.4.3 requires scikit-learn>=0.17.1, which is not installed.\nbayesian-optimization 3.0.0 requires scikit-learn<2.0.0,>=1.0.0, which is not installed.\ntpot 0.12.1 requires scikit-learn>=0.22.0, which is not installed.\nshap 0.44.1 requires scikit-learn, which is not installed.\nkaggle-environments 1.17.6 requires transformers>=4.33.1, which is not installed.\npyldavis 3.4.1 requires scikit-learn>=1.0.0, which is not installed.\nscikit-plot 0.3.7 requires scikit-learn>=0.18, which is not installed.\nnilearn 0.10.4 requires scikit-learn>=1.0.0, which is not installed.\neli5 0.13.0 requires scikit-learn>=0.20, which is not installed.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, which is not installed.\nlime 0.2.0.1 requires scikit-learn>=0.18, which is not installed.\ncesium 0.12.4 requires scikit-learn>=0.22.1, which is not installed.\nhep-ml 0.8.0 requires scikit-learn>=1.0, which is not installed.\nscikit-optimize 0.10.2 requires scikit-learn>=1.0.0, which is not installed.\ntsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\nsklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\nlibrosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\numap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\nfastai 2.7.19 requires scikit-learn, which is not installed.\npynndescent 0.5.13 requires scikit-learn>=0.18, which is not installed.\nhdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\nyellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scipy-1.11.4\nCollecting scikit-learn==1.3.2\n  Downloading scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: numpy<2.0,>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.26.4)\nRequirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.3.2) (3.6.0)\nDownloading scikit_learn-1.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m185.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed scikit-learn-1.3.2\nCollecting transformers==4.37.2\n  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (2.32.4)\nCollecting tokenizers<0.19,>=0.14 (from transformers==4.37.2)\n  Downloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.37.2) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (2025.5.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.2) (1.1.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.2) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.37.2) (2025.6.15)\nDownloading transformers-4.37.2-py3-none-any.whl (8.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m126.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m276.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.2\n    Uninstalling tokenizers-0.21.2:\n      Successfully uninstalled tokenizers-0.21.2\nSuccessfully installed tokenizers-0.15.2 transformers-4.37.2\nCollecting sentence-transformers==2.2.2\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (4.37.2)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (4.67.1)\nRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (2.6.0+cu124)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.21.0+cu124)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.26.4)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.3.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (1.11.4)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (3.9.1)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.2.0)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers==2.2.2) (0.33.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.32.4)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (1.1.5)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.6.0->sentence-transformers==2.2.2)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.6.0->sentence-transformers==2.2.2) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.6.0->sentence-transformers==2.2.2) (1.3.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (2024.11.6)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers==2.2.2) (0.5.3)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->sentence-transformers==2.2.2) (8.2.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->sentence-transformers==2.2.2) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers==2.2.2) (3.6.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->sentence-transformers==2.2.2) (11.2.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.6.0->sentence-transformers==2.2.2) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers==2.2.2) (2025.6.15)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m168.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m151.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m168.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m332.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m174.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m172.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m172.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m172.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m171.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m171.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=137f0bdee99eb2d2fac3aa0cceafd8665c51443363f8fd016b45e03cf0604d0c\n  Stored in directory: /tmp/pip-ephem-wheel-cache-_jay5ekh/wheels/ff/27/bf/ffba8b318b02d7f691a57084ee154e26ed24d012b0c7805881\nSuccessfully built sentence-transformers\nInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sentence-transformers-2.2.2\nCollecting huggingface_hub==0.23.2\n  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.23.2) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.23.2) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.23.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.23.2) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.23.2) (2.32.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.23.2) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.23.2) (4.14.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.23.2) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.23.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.23.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.23.2) (2025.6.15)\nDownloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.7/401.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: huggingface_hub\n  Attempting uninstall: huggingface_hub\n    Found existing installation: huggingface-hub 0.33.1\n    Uninstalling huggingface-hub-0.33.1:\n      Successfully uninstalled huggingface-hub-0.33.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\ndatasets 3.6.0 requires huggingface-hub>=0.24.0, but you have huggingface-hub 0.23.2 which is incompatible.\ndiffusers 0.34.0 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.23.2 which is incompatible.\ngradio 5.31.0 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.23.2 which is incompatible.\npeft 0.15.2 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.23.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed huggingface_hub-0.23.2\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\nos._exit(0)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-08-09T16:53:03.925Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"修正评估模块后版本\n但仍未解决问题","metadata":{}},{"cell_type":"code","source":"import json\nimport collections\nimport re\nimport string\nimport faiss\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\n# ------------------ SQuAD Evaluation Functions ------------------\n\ndef normalize_answer(s):\n    if s is None:\n        return \"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    return 2 * precision * recall / (precision + recall)\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                qid = qa['id']\n                gold_answers = [a['text'] for a in qa.get('answers', []) if normalize_answer(a.get('text', ''))]\n                if not gold_answers:\n                    gold_answers = ['']  # no answer questions\n                a_pred = preds.get(qid, \"\")\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\ndef make_eval_dict(exact_scores, f1_scores):\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        ('exact', 100.0 * sum(exact_scores.values()) / total if total else 0),\n        ('f1', 100.0 * sum(f1_scores.values()) / total if total else 0),\n        ('total', total),\n    ])\n\n# ------------------ RAG 主体流程 ------------------\n\ndef load_squad_dataset(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data_json = json.load(f)\n    return data_json['data']\n\ndef extract_contexts(dataset):\n    contexts = []\n    ctx_to_idx = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            ctx = p['context']\n            if ctx not in ctx_to_idx:\n                ctx_to_idx[ctx] = len(contexts)\n                contexts.append(ctx)\n    return contexts, ctx_to_idx\n\ndef build_faiss_index(texts, embedder, batch_size=64):\n    embs = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n        batch_emb = embedder.encode(batch, convert_to_numpy=True, normalize_embeddings=False, show_progress_bar=False)\n        batch_emb = np.asarray(batch_emb, dtype=np.float32)\n        embs.append(batch_emb)\n    embeddings = np.vstack(embs)\n    faiss.normalize_L2(embeddings)\n    dim = embeddings.shape[1]\n    index = faiss.IndexFlatIP(dim)\n    index.add(embeddings)\n    return index\n\ndef retrieve_top_k(question, index, embedder, contexts, k=15):\n    q_emb = embedder.encode([question], convert_to_numpy=True, show_progress_bar=False)\n    q_emb = np.asarray(q_emb, dtype=np.float32)\n    faiss.normalize_L2(q_emb)\n    D, I = index.search(q_emb, k)\n    hits = []\n    for idx in I[0]:\n        if 0 <= idx < len(contexts):\n            hits.append(contexts[idx])\n    return hits\n\ndef chunk_and_truncate_contexts(contexts, max_chars=3000):\n    out = []\n    for c in contexts:\n        if len(c) <= max_chars:\n            out.append(c)\n        else:\n            out.append(c[:max_chars] + \" ...\")\n    return out\n\ndef generate_answer(question, contexts, tokenizer, model, max_new_tokens=128):\n    context_text = \"\\n\".join(chunk_and_truncate_contexts(contexts))\n    prompt = f\"Question: {question}\\nContext:\\n{context_text}\\nAnswer:\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    device = next(model.parameters()).device\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n    \n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        num_beams=4,\n        no_repeat_ngram_size=3,\n        early_stopping=True\n    )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):]\n    return answer.strip()\n\ndef main():\n    train_path = '/kaggle/input/squad-train/train-v2.0.json'\n    dev_path = '/kaggle/input/squad-dev/dev-v2.0.json'\n\n    sentence_transformer_name = 'all-mpnet-base-v2'\n    model_name = \"google-t5/t5-base\"\n    batch_size_embed = 64\n\n    print(\"加载数据集...\")\n    train_data = load_squad_dataset(train_path)\n    dev_data = load_squad_dataset(dev_path)\n\n    print(\"提取并去重训练集上下文...\")\n    train_contexts, _ = extract_contexts(train_data)\n    print(f\"Unique training contexts: {len(train_contexts)}\")\n\n    print(\"加载句向量模型...\")\n    embedder = SentenceTransformer(sentence_transformer_name)\n\n    print(\"建立FAISS索引...\")\n    index = build_faiss_index(train_contexts, embedder, batch_size=batch_size_embed)\n\n    print(f\"加载生成模型和tokenizer: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n    model.eval()\n\n    preds = {}\n\n    print(\"开始对验证集生成回答...\")\n    count = 0\n    max_test = 20\n    for article in dev_data:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                if count >= max_test:\n                    break\n                qid = qa['id']\n                question = qa['question']\n                retrieved = retrieve_top_k(question, index, embedder, train_contexts, k=5)\n                answer = generate_answer(question, retrieved, tokenizer, model)\n                preds[qid] = answer\n                print(f\"[{count}] QID: {qid} 预测完成\")\n                count += 1\n        if count >= max_test:\n            break\n\n    print(\"计算评测指标...\")\n    exact_scores, f1_scores = get_raw_scores(dev_data, preds)\n    eval_results = make_eval_dict(exact_scores, f1_scores)\n\n    print(\"评测结果：\")\n    for k, v in eval_results.items():\n        print(f\"{k}: {v:.2f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T01:43:24.113776Z","iopub.execute_input":"2025-08-10T01:43:24.114476Z","iopub.status.idle":"2025-08-10T01:49:41.058545Z","shell.execute_reply.started":"2025-08-10T01:43:24.114451Z","shell.execute_reply":"2025-08-10T01:49:41.057732Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"加载数据集...\n提取并去重训练集上下文...\nUnique training contexts: 19029\n加载句向量模型...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0be41b622cc4a659979b3ef281a866a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f16bd9af0c0348a08b95dab5268f5df0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef81179c5b954091beab5e6ee3655b60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9947865dac7248b88085d44309e1f2c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d30f26743d9043ad982d20ceb1cc1506"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cbd8f15bc4f4008b9a1b8903d14dd20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18c04f03c1b34b239cc6ba3f7c1f3ce7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3965cf0d39d4149bb5683cd2ee1bd79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_O1.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"096a2e00e939403aa82d5ce287289a83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_O2.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"655c59f5cbe740c8993340e468b0d8c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_O3.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce1a802e6b884aaa81c23c4012b0e0c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_O4.onnx:   0%|          | 0.00/218M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc5bb1261518455bba2f719982983d3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_qint8_arm64.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8345b45db94740d6942faf97cac0e485"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_qint8_arm64.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57b493ebf52c4fe0a21a7bea77ca77f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_qint8_arm64.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7104b85cad0c4c66ac7cd88ef6c252bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_quint8_avx2.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"871ed554be2c42b084a22a6a74b090c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"openvino_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1516b8fdee849eeb975c11e73ddc905"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"openvino_model.xml: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fac9674e01414322a159d57dc6d54b5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"openvino_model_qint8_quantized.bin:   0%|          | 0.00/110M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c60855f988be48d39d03216dc2e690cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"openvino_model_qint8_quantized.xml: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18bd607748c34189824af5f93e87d4ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3f91b52bfd248a89ab4d8df252177b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9a6a5d1021a4337ade9f997ff149cc4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a473dd4608742988ff7856244e3861c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c0627969f44418bbf731fb42c735816"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2bade2f3c3d4f8cbd2b0076fe3558ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train_script.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8562bcaa1a1c4c7381c0ccd7d687aabb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83be5bac6848402489e9fe2277634dbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70d1a30a7ed7433cb7783ba053a0a4c2"}},"metadata":{}},{"name":"stdout","text":"建立FAISS索引...\n加载生成模型和tokenizer: google-t5/t5-base\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1db3af1bac347428c651e7b6d6107ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93153fe2785341dba3d8d41b01c7e347"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6c168258e504ea397dd523242580eee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddcdbdb7e83a4776bb07ce652653e8bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a814575357f4de881c741eff3ee8cf3"}},"metadata":{}},{"name":"stdout","text":"开始对验证集生成回答...\n[0] QID: 56ddde6b9a695914005b9628 预测完成\n[1] QID: 56ddde6b9a695914005b9629 预测完成\n[2] QID: 56ddde6b9a695914005b962a 预测完成\n[3] QID: 56ddde6b9a695914005b962b 预测完成\n[4] QID: 56ddde6b9a695914005b962c 预测完成\n[5] QID: 5ad39d53604f3c001a3fe8d1 预测完成\n[6] QID: 5ad39d53604f3c001a3fe8d2 预测完成\n[7] QID: 5ad39d53604f3c001a3fe8d3 预测完成\n[8] QID: 5ad39d53604f3c001a3fe8d4 预测完成\n[9] QID: 56dddf4066d3e219004dad5f 预测完成\n[10] QID: 56dddf4066d3e219004dad60 预测完成\n[11] QID: 56dddf4066d3e219004dad61 预测完成\n[12] QID: 5ad3a266604f3c001a3fea27 预测完成\n[13] QID: 5ad3a266604f3c001a3fea28 预测完成\n[14] QID: 5ad3a266604f3c001a3fea29 预测完成\n[15] QID: 5ad3a266604f3c001a3fea2a 预测完成\n[16] QID: 5ad3a266604f3c001a3fea2b 预测完成\n[17] QID: 56dde0379a695914005b9636 预测完成\n[18] QID: 56dde0379a695914005b9637 预测完成\n[19] QID: 5ad3ab70604f3c001a3feb89 预测完成\n计算评测指标...\n评测结果：\nexact: 49.99\nf1: 49.99\ntotal: 11873.00\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## 第一个可运行最小版本\n使用google seq2seq模型，可能效果比opt好一点","metadata":{}},{"cell_type":"code","source":"import json\nimport collections\nimport re\nimport string\nimport faiss\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\n# ------------------ SQuAD Evaluation Functions ------------------\n\ndef normalize_answer(s):\n    if s is None:\n        return \"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    return 2 * precision * recall / (precision + recall)\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                qid = qa['id']\n                gold_answers = [a['text'] for a in qa.get('answers', []) if normalize_answer(a.get('text', ''))]\n                if not gold_answers:\n                    gold_answers = ['']  # no answer questions\n                a_pred = preds.get(qid, \"\")\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\ndef make_eval_dict(exact_scores, f1_scores):\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        ('exact', 100.0 * sum(exact_scores.values()) / total if total else 0),\n        ('f1', 100.0 * sum(f1_scores.values()) / total if total else 0),\n        ('total', total),\n    ])\n\n# ------------------ RAG 主体流程 ------------------\n\ndef load_squad_dataset(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data_json = json.load(f)\n    return data_json['data']\n\ndef extract_contexts(dataset):\n    contexts = []\n    ctx_to_idx = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            ctx = p['context']\n            if ctx not in ctx_to_idx:\n                ctx_to_idx[ctx] = len(contexts)\n                contexts.append(ctx)\n    return contexts, ctx_to_idx\n\ndef build_faiss_index(texts, embedder, batch_size=256):\n    embs = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n        batch_emb = embedder.encode(batch, convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n        batch_emb = np.asarray(batch_emb, dtype=np.float32)\n        embs.append(batch_emb)\n    embeddings = np.vstack(embs)\n    # 因为 embedder.encode 已经 normalize_embeddings=True，这里不再调用 faiss.normalize_L2\n    dim = embeddings.shape[1]\n    index = faiss.IndexFlatIP(dim)\n    index.add(embeddings)\n    return index\n\ndef retrieve_top_k(question, index, embedder, contexts, k=5):\n    q_emb = embedder.encode([question], convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n    q_emb = np.asarray(q_emb, dtype=np.float32)\n    # 同理这里不调用 faiss.normalize_L2\n    D, I = index.search(q_emb, k)\n    hits = []\n    for idx in I[0]:\n        if 0 <= idx < len(contexts):\n            hits.append(contexts[idx])\n    return hits\n\ndef chunk_and_truncate_contexts(contexts, max_chars=3000):\n    out = []\n    for c in contexts:\n        if len(c) <= max_chars:\n            out.append(c)\n        else:\n            out.append(c[:max_chars] + \" ...\")\n    return out\n\ndef generate_answer(question, contexts, tokenizer, model, max_new_tokens=64):\n    context_text = \"\\n\".join(chunk_and_truncate_contexts(contexts))\n    prompt = f\"Question: {question}\\nContext:\\n{context_text}\\nAnswer (if no answer, respond with 'no answer'):\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n    device = next(model.parameters()).device\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        num_beams=4,\n        no_repeat_ngram_size=3,\n        early_stopping=True\n    )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):]\n    answer = answer.strip()\n    if answer.lower() in [\"false\", \"no answer\", \"none\", \"\"]:\n        answer = \"\"\n    return answer\n\n\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    train_path = '/kaggle/input/squad-train/train-v2.0.json'\n    dev_path = '/kaggle/input/squad-dev/dev-v2.0.json'\n\n    sentence_transformer_name = 'all-mpnet-base-v2'\n    model_name = \"google/flan-t5-base\"\n    batch_size_embed = 256\n\n    print(\"加载数据集...\")\n    train_data = load_squad_dataset(train_path)\n    dev_data = load_squad_dataset(dev_path)\n\n    print(\"提取并去重训练集上下文...\")\n    train_contexts, _ = extract_contexts(train_data)\n    print(f\"Unique training contexts: {len(train_contexts)}\")\n\n    print(\"加载句向量模型...\")\n    # 这里修改了device传参\n    embedder = SentenceTransformer(sentence_transformer_name, device=device)\n\n    print(\"建立FAISS索引...\")\n    index = build_faiss_index(train_contexts, embedder, batch_size=batch_size_embed)\n\n    print(f\"加载生成模型和tokenizer: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n    model.eval()\n\n    preds = {}\n    dev_subset = []  # 保存只用的前 20 个 QA\n\n    print(\"开始对验证集生成回答...\")\n    count = 0\n    max_test = 20\n    for article in dev_data:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                if count >= max_test:\n                    break\n                qid = qa['id']\n                question = qa['question']\n                retrieved = retrieve_top_k(question, index, embedder, train_contexts, k=5)\n                answer = generate_answer(question, retrieved, tokenizer, model)\n\n                preds[qid] = answer\n                dev_subset.append({\n                    \"id\": qid,\n                    \"question\": question,\n                    \"answers\": qa[\"answers\"],\n                    \"retrieved\": retrieved,\n                    \"pred\": answer\n                })\n\n                print(count)\n\n                count += 1\n            if count >= max_test:\n                break\n        if count >= max_test:\n            break\n\n    # 构造一个只包含前 20 个的 dev_data 子集\n    filtered_dev_data = []\n    for article in dev_data:\n        new_article = {\"paragraphs\": []}\n        for p in article['paragraphs']:\n            new_p = {\"qas\": [], \"context\": p[\"context\"]}\n            for qa in p['qas']:\n                if qa['id'] in preds:\n                    new_p[\"qas\"].append(qa)\n            if new_p[\"qas\"]:\n                new_article[\"paragraphs\"].append(new_p)\n        if new_article[\"paragraphs\"]:\n            filtered_dev_data.append(new_article)\n\n    print(\"计算评测指标...\")\n    exact_scores, f1_scores = get_raw_scores(filtered_dev_data, preds)\n    eval_results = make_eval_dict(exact_scores, f1_scores)\n\n    print(\"评测结果：\")\n    for k, v in eval_results.items():\n        print(f\"{k}: {v:.2f}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:04:20.945326Z","iopub.execute_input":"2025-08-10T06:04:20.946116Z","iopub.status.idle":"2025-08-10T06:09:14.018273Z","shell.execute_reply.started":"2025-08-10T06:04:20.946091Z","shell.execute_reply":"2025-08-10T06:09:14.017369Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"加载数据集...\n提取并去重训练集上下文...\nUnique training contexts: 19029\n加载句向量模型...\n建立FAISS索引...\n加载生成模型和tokenizer: google/flan-t5-base\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08ee75156ba4422883aa6bbe8c3cb694"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85bf25b25a344699abbe009f3c3e9314"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2345f7a58cd4e5ead3b97e8cc6f195e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a9457e0b25d453595027cf8b8e55a15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bd83af8f6cc48569dacae2e531fa8d0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf8863cad1ac40e0be22668c9ade0252"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8383fd92371a485a9d48e8c82f36989e"}},"metadata":{}},{"name":"stdout","text":"开始对验证集生成回答...\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n计算评测指标...\n评测结果：\nexact: 5.00\nf1: 10.75\ntotal: 20.00\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# faiss加速后版本","metadata":{}},{"cell_type":"code","source":"import json\nimport collections\nimport re\nimport string\nimport faiss\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\n# ------------------ SQuAD Evaluation Functions ------------------\n\ndef normalize_answer(s):\n    if s is None:\n        return \"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    return 2 * precision * recall / (precision + recall)\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                qid = qa['id']\n                gold_answers = [a['text'] for a in qa.get('answers', []) if normalize_answer(a.get('text', ''))]\n                if not gold_answers:\n                    gold_answers = ['']  # no answer questions\n                a_pred = preds.get(qid, \"\")\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\ndef make_eval_dict(exact_scores, f1_scores):\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        ('exact', 100.0 * sum(exact_scores.values()) / total if total else 0),\n        ('f1', 100.0 * sum(f1_scores.values()) / total if total else 0),\n        ('total', total),\n    ])\n\n# ------------------ RAG 主体流程 ------------------\n\ndef load_squad_dataset(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data_json = json.load(f)\n    return data_json['data']\n\ndef extract_contexts(dataset):\n    contexts = []\n    ctx_to_idx = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            ctx = p['context']\n            if ctx not in ctx_to_idx:\n                ctx_to_idx[ctx] = len(contexts)\n                contexts.append(ctx)\n    return contexts, ctx_to_idx\n\ndef build_faiss_index_ivfpq(texts, embedder, batch_size=256, nlist=100, m=16, nbits=8):\n    # 编码所有文本\n    embs = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding batches\"):\n        batch = texts[i:i+batch_size]\n        batch_emb = embedder.encode(batch, convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n        batch_emb = np.asarray(batch_emb, dtype=np.float32)\n        embs.append(batch_emb)\n    embeddings = np.vstack(embs)\n\n    dim = embeddings.shape[1]\n    # IVF + PQ索引\n    quantizer = faiss.IndexFlatIP(dim)  # 用内积量化器\n    index = faiss.IndexIVFPQ(quantizer, dim, nlist, m, nbits)\n\n    # 训练索引，必须先训练\n    print(\"训练IVF-PQ索引...\")\n    index.train(embeddings)\n\n    # 添加向量\n    print(\"添加向量到索引...\")\n    index.add(embeddings)\n\n    return index\n\ndef retrieve_top_k(question, index, embedder, contexts, k=5):\n    q_emb = embedder.encode([question], convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n    q_emb = np.asarray(q_emb, dtype=np.float32)\n    D, I = index.search(q_emb, k)\n    hits = []\n    for idx in I[0]:\n        if 0 <= idx < len(contexts):\n            hits.append(contexts[idx])\n    return hits\n\ndef chunk_and_truncate_contexts(contexts, max_chars=3000):\n    out = []\n    for c in contexts:\n        if len(c) <= max_chars:\n            out.append(c)\n        else:\n            out.append(c[:max_chars] + \" ...\")\n    return out\n\ndef generate_answer(question, contexts, tokenizer, model, max_new_tokens=64):\n    context_text = \"\\n\".join(chunk_and_truncate_contexts(contexts))\n    prompt = f\"Question: {question}\\nContext:\\n{context_text}\\nAnswer (if no answer, respond with 'no answer'):\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n    device = next(model.parameters()).device\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        num_beams=4,\n        no_repeat_ngram_size=3,\n        early_stopping=True\n    )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):]\n    answer = answer.strip()\n    if answer.lower() in [\"false\", \"no answer\", \"none\", \"\"]:\n        answer = \"\"\n    return answer\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    train_path = '/kaggle/input/squad-train/train-v2.0.json'\n    dev_path = '/kaggle/input/squad-dev/dev-v2.0.json'\n\n    sentence_transformer_name = 'all-mpnet-base-v2'\n    model_name = \"google/flan-t5-base\"\n    batch_size_embed = 256\n\n    print(\"加载数据集...\")\n    train_data = load_squad_dataset(train_path)\n    dev_data = load_squad_dataset(dev_path)\n\n    print(\"提取并去重训练集上下文...\")\n    train_contexts, _ = extract_contexts(train_data)\n    print(f\"Unique training contexts: {len(train_contexts)}\")\n\n    print(\"加载句向量模型...\")\n    embedder = SentenceTransformer(sentence_transformer_name, device=device)\n\n    print(\"建立FAISS IVF-PQ索引...\")\n    index = build_faiss_index_ivfpq(train_contexts, embedder, batch_size=batch_size_embed, nlist=100, m=16, nbits=8)\n\n    print(f\"加载生成模型和tokenizer: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n    model.eval()\n\n    preds = {}\n    max_test = 20\n    count = 0\n\n    print(\"开始对验证集生成回答...\")\n    for article in dev_data:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                if count >= max_test:\n                    break\n                qid = qa['id']\n                question = qa['question']\n                retrieved = retrieve_top_k(question, index, embedder, train_contexts, k=5)\n                answer = generate_answer(question, retrieved, tokenizer, model)\n\n                preds[qid] = answer\n\n                print(count)\n                count += 1\n            if count >= max_test:\n                break\n        if count >= max_test:\n            break\n\n    # 构造只包含测试用的子集 dev_data\n    filtered_dev_data = []\n    for article in dev_data:\n        new_article = {\"paragraphs\": []}\n        for p in article['paragraphs']:\n            new_p = {\"qas\": [], \"context\": p[\"context\"]}\n            for qa in p['qas']:\n                if qa['id'] in preds:\n                    new_p[\"qas\"].append(qa)\n            if new_p[\"qas\"]:\n                new_article[\"paragraphs\"].append(new_p)\n        if new_article[\"paragraphs\"]:\n            filtered_dev_data.append(new_article)\n\n    print(\"计算评测指标...\")\n    exact_scores, f1_scores = get_raw_scores(filtered_dev_data, preds)\n    eval_results = make_eval_dict(exact_scores, f1_scores)\n\n    print(\"评测结果：\")\n    for k, v in eval_results.items():\n        print(f\"{k}: {v:.2f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:34:43.486793Z","iopub.execute_input":"2025-08-10T06:34:43.487054Z","iopub.status.idle":"2025-08-10T06:39:50.591956Z","shell.execute_reply.started":"2025-08-10T06:34:43.487032Z","shell.execute_reply":"2025-08-10T06:39:50.590967Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"加载数据集...\n提取并去重训练集上下文...\nUnique training contexts: 19029\n加载句向量模型...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":".gitattributes: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba0f5fba936d46cd8c2f9f7f743b5d76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93ac364439754688a943a11684a9653e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaadb7eb54db4eb4b4509c0727248bd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6692dc268f2747c0ac84a8683336afcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19cf9268e94047ccb83a5face700bca4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a880aacc2a5d474e8dcb11114a398ffe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"465edbe591b4486e90db8e467c8d34b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"447cc7c841e64ee09c74a478af26356c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_O1.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0cf96d481f64066be5839fc1b4c7e57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_O2.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf619de7831d4360a16ece7e768cd279"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_O3.onnx:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4716ac01e7494c8a8258a964478d8f43"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_O4.onnx:   0%|          | 0.00/218M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86e1bcf442cf475e8add73809d90ae38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_qint8_arm64.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dc81a77d57042cd977761a6786838ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_qint8_arm64.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c79b8aeb3a4840d39c8de81dd5cfd2c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_qint8_arm64.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdda7dafdc9d42f8bdd0be2400dd4c6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model_quint8_avx2.onnx:   0%|          | 0.00/110M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d80906265d7b4cfabcb94de501d5a71c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"openvino_model.bin:   0%|          | 0.00/436M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"297def7f4ccb42858e49d9444cbb8c05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"openvino_model.xml: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b28e26cd6e394ca193dd3304b37be31f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"openvino_model_qint8_quantized.bin:   0%|          | 0.00/110M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c77411d4540c4bc9afbdc506c8911838"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"openvino_model_qint8_quantized.xml: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81a087d11ccc47c2a172ee157c8da356"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ad9bc9e1c9540ec91ef5b33cf599cc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aee227c0cc684029b2238997a6cfabd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"046bda6a3f6f4e819679650f5a7f3599"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f585c8daeaa485ea6601004dc9a5a6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2e3d42d534947ebb0ae0aa30880dc8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train_script.py: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"575fc1ebe2c344479d0033bcf1016528"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8875d8ff8c8417a9cf5642911466443"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86b3337e3a9240578c357233b4a48b10"}},"metadata":{}},{"name":"stdout","text":"建立FAISS IVF-PQ索引...\n","output_type":"stream"},{"name":"stderr","text":"Embedding batches: 100%|██████████| 75/75 [04:22<00:00,  3.50s/it]\n","output_type":"stream"},{"name":"stdout","text":"训练IVF-PQ索引...\n添加向量到索引...\n加载生成模型和tokenizer: google/flan-t5-base\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"544a3f26fc4c48b3ba316f29f90235ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6933ffd3a3244e9daf795386282e50f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7de811025c9a4d62adc8fcdf1cd22d39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a28e9b13d9e45738fa336fe585d24fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e51254725c95470a8d083259d6bb8ae9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8a6b2dbaf3b4e3fac1951d611d2fd8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85f4f19643614644b9194e76a5cadca4"}},"metadata":{}},{"name":"stdout","text":"开始对验证集生成回答...\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n计算评测指标...\n评测结果：\nexact: 5.00\nf1: 7.50\ntotal: 20.00\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"## faiss增强-更大QA set","metadata":{}},{"cell_type":"code","source":"import json\nimport collections\nimport re\nimport string\nimport faiss\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\n# ------------------ SQuAD Evaluation Functions ------------------\n\ndef normalize_answer(s):\n    if s is None:\n        return \"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    return 2 * precision * recall / (precision + recall)\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                qid = qa['id']\n                gold_answers = [a['text'] for a in qa.get('answers', []) if normalize_answer(a.get('text', ''))]\n                if not gold_answers:\n                    gold_answers = ['']  # no answer questions\n                a_pred = preds.get(qid, \"\")\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\ndef make_eval_dict(exact_scores, f1_scores):\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        ('exact', 100.0 * sum(exact_scores.values()) / total if total else 0),\n        ('f1', 100.0 * sum(f1_scores.values()) / total if total else 0),\n        ('total', total),\n    ])\n\n# ------------------ RAG 主体流程 ------------------\n\ndef load_squad_dataset(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data_json = json.load(f)\n    return data_json['data']\n\ndef extract_contexts(dataset):\n    contexts = []\n    ctx_to_idx = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            ctx = p['context']\n            if ctx not in ctx_to_idx:\n                ctx_to_idx[ctx] = len(contexts)\n                contexts.append(ctx)\n    return contexts, ctx_to_idx\n\ndef build_faiss_index_ivfpq(texts, embedder, batch_size=256, nlist=100, m=16, nbits=8):\n    # 编码所有文本\n    embs = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding batches\"):\n        batch = texts[i:i+batch_size]\n        batch_emb = embedder.encode(batch, convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n        batch_emb = np.asarray(batch_emb, dtype=np.float32)\n        embs.append(batch_emb)\n    embeddings = np.vstack(embs)\n\n    dim = embeddings.shape[1]\n    # IVF + PQ索引\n    quantizer = faiss.IndexFlatIP(dim)  # 用内积量化器\n    index = faiss.IndexIVFPQ(quantizer, dim, nlist, m, nbits)\n\n    # 训练索引，必须先训练\n    print(\"训练IVF-PQ索引...\")\n    index.train(embeddings)\n\n    # 添加向量\n    print(\"添加向量到索引...\")\n    index.add(embeddings)\n\n    return index\n\ndef retrieve_top_k(question, index, embedder, contexts, k=5):\n    q_emb = embedder.encode([question], convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n    q_emb = np.asarray(q_emb, dtype=np.float32)\n    D, I = index.search(q_emb, k)\n    hits = []\n    for idx in I[0]:\n        if 0 <= idx < len(contexts):\n            hits.append(contexts[idx])\n    return hits\n\ndef chunk_and_truncate_contexts(contexts, max_chars=3000):\n    out = []\n    for c in contexts:\n        if len(c) <= max_chars:\n            out.append(c)\n        else:\n            out.append(c[:max_chars] + \" ...\")\n    return out\n\ndef generate_answer(question, contexts, tokenizer, model, max_new_tokens=64):\n    context_text = \"\\n\".join(chunk_and_truncate_contexts(contexts))\n    prompt = f\"Question: {question}\\nContext:\\n{context_text}\\nAnswer (if no answer, respond with 'no answer'):\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n    device = next(model.parameters()).device\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        num_beams=4,\n        no_repeat_ngram_size=3,\n        early_stopping=True\n    )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):]\n    answer = answer.strip()\n    if answer.lower() in [\"false\", \"no answer\", \"none\", \"\"]:\n        answer = \"\"\n    return answer\n\n\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    train_path = '/kaggle/input/squad-train/train-v2.0.json'\n    dev_path = '/kaggle/input/squad-dev/dev-v2.0.json'\n\n    sentence_transformer_name = 'all-mpnet-base-v2'\n    model_name = \"google/flan-t5-base\"\n    batch_size_embed = 256\n\n    print(\"加载数据集...\")\n    train_data = load_squad_dataset(train_path)\n    dev_data = load_squad_dataset(dev_path)\n\n    print(\"提取并去重训练集上下文...\")\n    train_contexts, _ = extract_contexts(train_data)\n    print(f\"Unique training contexts: {len(train_contexts)}\")\n\n    print(\"加载句向量模型...\")\n    embedder = SentenceTransformer(sentence_transformer_name, device=device)\n\n    print(\"建立FAISS IVF-PQ索引...\")\n    index = build_faiss_index_ivfpq(train_contexts, embedder, batch_size=batch_size_embed, nlist=100, m=16, nbits=8)\n\n    print(f\"加载生成模型和tokenizer: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n    model.eval()\n\n    preds = {}\n    max_test = 500  # 改成跑 500 个 QA\n    count = 0\n\n    print(\"开始对验证集生成回答...\")\n    for article in dev_data:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                if count >= max_test:\n                    break\n                qid = qa['id']\n                question = qa['question']\n                retrieved = retrieve_top_k(question, index, embedder, train_contexts, k=5)\n                answer = generate_answer(question, retrieved, tokenizer, model)\n                preds[qid] = answer\n                count += 1\n                print(count)\n            if count >= max_test:\n                break\n        if count >= max_test:\n            break\n\n    # 构造只包含测试用的子集 dev_data\n    filtered_dev_data = []\n    for article in dev_data:\n        new_article = {\"paragraphs\": []}\n        for p in article['paragraphs']:\n            new_p = {\"qas\": [], \"context\": p[\"context\"]}\n            for qa in p['qas']:\n                if qa['id'] in preds:  # 只保留前 500 个的 QA\n                    new_p[\"qas\"].append(qa)\n            if new_p[\"qas\"]:\n                new_article[\"paragraphs\"].append(new_p)\n        if new_article[\"paragraphs\"]:\n            filtered_dev_data.append(new_article)\n\n    print(\"计算评测指标...\")\n    exact_scores, f1_scores = get_raw_scores(filtered_dev_data, preds)\n    eval_results = make_eval_dict(exact_scores, f1_scores)\n\n    print(\"评测结果：\")\n    for k, v in eval_results.items():\n        print(f\"{k}: {v:.2f}\")\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T06:45:40.564174Z","iopub.execute_input":"2025-08-10T06:45:40.565084Z","iopub.status.idle":"2025-08-10T06:51:52.710129Z","shell.execute_reply.started":"2025-08-10T06:45:40.565058Z","shell.execute_reply":"2025-08-10T06:51:52.709305Z"}},"outputs":[{"name":"stdout","text":"加载数据集...\n提取并去重训练集上下文...\nUnique training contexts: 19029\n加载句向量模型...\n建立FAISS IVF-PQ索引...\n","output_type":"stream"},{"name":"stderr","text":"Embedding batches: 100%|██████████| 75/75 [04:38<00:00,  3.71s/it]\n","output_type":"stream"},{"name":"stdout","text":"训练IVF-PQ索引...\n添加向量到索引...\n加载生成模型和tokenizer: google/flan-t5-base\n开始对验证集生成回答...\n计算评测指标...\n评测结果：\nexact: 2.20\nf1: 3.25\ntotal: 500.00\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# 弃用faiss-并提升GPU利用效率","metadata":{}},{"cell_type":"code","source":"import json\nimport collections\nimport re\nimport string\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\n# ------------------ SQuAD Evaluation Functions ------------------\n\ndef normalize_answer(s):\n    if s is None:\n        return \"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    return 2 * precision * recall / (precision + recall)\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                qid = qa['id']\n                gold_answers = [a['text'] for a in qa.get('answers', []) if normalize_answer(a.get('text', ''))]\n                if not gold_answers:\n                    gold_answers = ['']  # no answer questions\n                a_pred = preds.get(qid, \"\")\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\ndef make_eval_dict(exact_scores, f1_scores):\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        ('exact', 100.0 * sum(exact_scores.values()) / total if total else 0),\n        ('f1', 100.0 * sum(f1_scores.values()) / total if total else 0),\n        ('total', total),\n    ])\n\n# ------------------ RAG 主体流程 ------------------\n\ndef load_squad_dataset(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data_json = json.load(f)\n    return data_json['data']\n\ndef chunk_and_truncate_contexts(contexts, max_chars=3000):\n    out = []\n    for c in contexts:\n        if len(c) <= max_chars:\n            out.append(c)\n        else:\n            out.append(c[:max_chars] + \" ...\")\n    return out\n\ndef build_context_embeddings(contexts, embedder, batch_size=1024):\n    embs = []\n    for i in tqdm(range(0, len(contexts), batch_size), desc=\"Embedding contexts\"):\n        batch = contexts[i:i+batch_size]\n        batch_emb = embedder.encode(batch, convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n        embs.append(batch_emb)\n    return np.vstack(embs).astype(np.float32)\n\ndef retrieve_top_k(question, all_ctx_emb, embedder, contexts, k=5):\n    q_emb = embedder.encode([question], convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n    q_emb = np.asarray(q_emb, dtype=np.float32)\n    # 计算所有上下文与问题的内积相似度\n    sims = np.dot(all_ctx_emb, q_emb.T).squeeze(1)  # shape: (num_contexts,)\n    topk_idx = np.argpartition(-sims, k)[:k]\n    # 按相似度排序\n    topk_idx = topk_idx[np.argsort(-sims[topk_idx])]\n    hits = [contexts[i] for i in topk_idx if i < len(contexts)]\n    return hits\n\ndef generate_answer(question, contexts, tokenizer, model, max_new_tokens=64):\n    context_text = \"\\n\".join(chunk_and_truncate_contexts(contexts))\n    prompt = f\"Question: {question}\\nContext:\\n{context_text}\\nAnswer (if no answer, respond with 'no answer'):\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n    device = next(model.parameters()).device\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        num_beams=4,\n        no_repeat_ngram_size=3,\n        early_stopping=True\n    )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):]\n    answer = answer.strip()\n    if answer.lower() in [\"false\", \"no answer\", \"none\", \"\"]:\n        answer = \"\"\n    return answer\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_file = '/kaggle/input/squad-train/train-v2.0.json'\n    dev_file = '/kaggle/input/squad-dev/dev-v2.0.json'\n\n    sentence_transformer_name = 'all-mpnet-base-v2'\n    model_name = \"google/flan-t5-base\"\n    batch_size_embed = 256\n\n    print(\"加载训练数据...\")\n    train_data = load_squad_dataset(train_file)\n    print(\"加载验证数据...\")\n    dev_data = load_squad_dataset(dev_file)\n\n    # 提取所有训练上下文（遍历每篇文章每个段落）\n    contexts = []\n    for article in train_data:\n        for para in article[\"paragraphs\"]:\n            contexts.append(para[\"context\"])\n    print(f\"训练集上下文数量: {len(contexts)}\")\n\n    print(\"加载句向量模型...\")\n    embedder = SentenceTransformer(sentence_transformer_name, device=device)\n\n    print(\"GPU上计算所有上下文的向量表示...\")\n    all_ctx_emb = build_context_embeddings(contexts, embedder, batch_size=batch_size_embed)\n\n    print(f\"加载生成模型和tokenizer: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n    model.eval()\n\n    preds = {}\n    max_test = 500\n    count = 0\n\n    print(f\"开始对验证集前{max_test}个样本生成回答...\")\n    for article in dev_data:\n        for para in article[\"paragraphs\"]:\n            for qa in para[\"qas\"]:\n                if count >= max_test:\n                    break\n                qid = qa['id']\n                question = qa['question']\n\n                retrieved = retrieve_top_k(question, all_ctx_emb, embedder, contexts, k=5)\n                answer = generate_answer(question, retrieved, tokenizer, model)\n\n                preds[qid] = answer\n\n                print(f\"{count}: QID={qid}, Answer='{answer[:30]}...'\")\n                count += 1\n            if count >= max_test:\n                break\n        if count >= max_test:\n            break\n\n    # 构造只包含测试用的dev_data子集\n    filtered_dev_data = []\n    for article in dev_data:\n        new_article = {\"paragraphs\": []}\n        for para in article[\"paragraphs\"]:\n            new_para = {\"qas\": [], \"context\": para[\"context\"]}\n            for qa in para[\"qas\"]:\n                if qa['id'] in preds:\n                    new_para[\"qas\"].append(qa)\n            if new_para[\"qas\"]:\n                new_article[\"paragraphs\"].append(new_para)\n        if new_article[\"paragraphs\"]:\n            filtered_dev_data.append(new_article)\n\n    print(\"计算评测指标...\")\n    exact_scores, f1_scores = get_raw_scores(filtered_dev_data, preds)\n    eval_results = make_eval_dict(exact_scores, f1_scores)\n\n    print(\"评测结果：\")\n    for k, v in eval_results.items():\n        print(f\"{k}: {v:.2f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T07:00:46.016068Z","iopub.execute_input":"2025-08-10T07:00:46.016390Z","iopub.status.idle":"2025-08-10T07:06:57.701998Z","shell.execute_reply.started":"2025-08-10T07:00:46.016367Z","shell.execute_reply":"2025-08-10T07:06:57.701241Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"加载训练数据...\n加载验证数据...\n训练集上下文数量: 19035\n加载句向量模型...\nGPU上计算所有上下文的向量表示...\n","output_type":"stream"},{"name":"stderr","text":"Embedding contexts: 100%|██████████| 75/75 [04:38<00:00,  3.72s/it]\n","output_type":"stream"},{"name":"stdout","text":"加载生成模型和tokenizer: google/flan-t5-base\n开始对验证集前500个样本生成回答...\n0: QID=56ddde6b9a695914005b9628, Answer='France...'\n1: QID=56ddde6b9a695914005b9629, Answer='11th and 12th centuries...'\n2: QID=56ddde6b9a695914005b962a, Answer='Scandinavian...'\n3: QID=56ddde6b9a695914005b962b, Answer='King Christian II...'\n4: QID=56ddde6b9a695914005b962c, Answer='1066...'\n5: QID=5ad39d53604f3c001a3fe8d1, Answer='the Dukes of Normandy...'\n6: QID=5ad39d53604f3c001a3fe8d2, Answer='Europe...'\n7: QID=5ad39d53604f3c001a3fe8d3, Answer='Lords Proprietors...'\n8: QID=5ad39d53604f3c001a3fe8d4, Answer='9th century...'\n9: QID=56dddf4066d3e219004dad5f, Answer='Arthur Wellesley took his titl...'\n10: QID=56dddf4066d3e219004dad60, Answer='Robert Guiscard...'\n11: QID=56dddf4066d3e219004dad61, Answer='Christian...'\n12: QID=5ad3a266604f3c001a3fea27, Answer='royal prestige rose to new hei...'\n13: QID=5ad3a266604f3c001a3fea28, Answer='Graeco-Roman...'\n14: QID=5ad3a266604f3c001a3fea29, Answer='Romance languages...'\n15: QID=5ad3a266604f3c001a3fea2a, Answer='William the Conqueror...'\n16: QID=5ad3a266604f3c001a3fea2b, Answer='Normandy...'\n17: QID=56dde0379a695914005b9636, Answer='\"the land of the Galicians\"...'\n18: QID=56dde0379a695914005b9637, Answer='mid 7th century...'\n19: QID=5ad3ab70604f3c001a3feb89, Answer='Norman architecture...'\n20: QID=5ad3ab70604f3c001a3feb8a, Answer='at least 1652...'\n21: QID=56dde0ba66d3e219004dad75, Answer='1066...'\n22: QID=56dde0ba66d3e219004dad76, Answer='Moreau...'\n23: QID=56dde0ba66d3e219004dad77, Answer='Ludowingian...'\n24: QID=5ad3ad61604f3c001a3fec0d, Answer='late 18th century...'\n25: QID=5ad3ad61604f3c001a3fec0e, Answer='...'\n26: QID=5ad3ad61604f3c001a3fec0f, Answer='Louis...'\n27: QID=5ad3ad61604f3c001a3fec10, Answer='every possible attack point...'\n28: QID=56dde1d966d3e219004dad8d, Answer='Norwegian sealer...'\n29: QID=5ad3ae14604f3c001a3fec39, Answer='June 6, 1944...'\n30: QID=5ad3ae14604f3c001a3fec3a, Answer='Magyar...'\n31: QID=56dde27d9a695914005b9651, Answer='Christian...'\n32: QID=56dde27d9a695914005b9652, Answer='southern France...'\n33: QID=5ad3af11604f3c001a3fec63, Answer='Viking influence...'\n34: QID=5ad3af11604f3c001a3fec64, Answer='Old English...'\n35: QID=5ad3af11604f3c001a3fec65, Answer='Old Catalan...'\n36: QID=56dde2fa66d3e219004dad9b, Answer='guano...'\n37: QID=5ad3c626604f3c001a3ff011, Answer='France...'\n38: QID=5ad3c626604f3c001a3ff012, Answer='guano...'\n39: QID=5ad3c626604f3c001a3ff013, Answer='Byzantine...'\n40: QID=56de0f6a4396321400ee257f, Answer='Magyar...'\n41: QID=5ad3dbc6604f3c001a3ff3e9, Answer='Theodoric the Great...'\n42: QID=5ad3dbc6604f3c001a3ff3ea, Answer='Magyar...'\n43: QID=5ad3dbc6604f3c001a3ff3eb, Answer='Henry II...'\n44: QID=5ad3dbc6604f3c001a3ff3ec, Answer='The First Samnite War...'\n45: QID=56de0ffd4396321400ee258d, Answer='913–959)...'\n46: QID=56de0ffd4396321400ee258e, Answer='1853...'\n47: QID=56de0ffd4396321400ee258f, Answer='the Germans...'\n48: QID=5ad3de8b604f3c001a3ff467, Answer='Ottoman Empire...'\n49: QID=5ad3de8b604f3c001a3ff468, Answer='1896...'\n50: QID=5ad3de8b604f3c001a3ff469, Answer='Turkish Cypriots...'\n51: QID=5ad3de8b604f3c001a3ff46a, Answer='Battle of Manzikert...'\n52: QID=56de10b44396321400ee2593, Answer='Château Gaillard...'\n53: QID=56de10b44396321400ee2594, Answer='France...'\n54: QID=56de10b44396321400ee2595, Answer='Seljuks of Rum...'\n55: QID=5ad3e96b604f3c001a3ff689, Answer='Turk...'\n56: QID=5ad3e96b604f3c001a3ff68a, Answer='Russians...'\n57: QID=5ad3e96b604f3c001a3ff68b, Answer='Eisenhower...'\n58: QID=5ad3e96b604f3c001a3ff68c, Answer='unanswerable...'\n59: QID=56de11154396321400ee25aa, Answer='15 ancestors...'\n60: QID=5ad3ea79604f3c001a3ff6e9, Answer='across northern Europe...'\n61: QID=5ad3ea79604f3c001a3ff6ea, Answer='Dukes of Normandy...'\n62: QID=5ad3ea79604f3c001a3ff6eb, Answer='voyage of exploration by Chris...'\n63: QID=56de148dcffd8e1900b4b5bc, Answer='Agathocles of Syracuse...'\n64: QID=56de148dcffd8e1900b4b5bd, Answer='1066...'\n65: QID=56de148dcffd8e1900b4b5be, Answer='10,000...'\n66: QID=5ad3ed26604f3c001a3ff799, Answer='the Ottomans...'\n67: QID=5ad3ed26604f3c001a3ff79a, Answer='Pope John XXIII...'\n68: QID=5ad3ed26604f3c001a3ff79b, Answer='conquest of Islamic Sicily...'\n69: QID=5ad3ed26604f3c001a3ff79c, Answer='5000...'\n70: QID=56de15104396321400ee25b7, Answer='Cherson...'\n71: QID=56de15104396321400ee25b8, Answer='Geoffrey...'\n72: QID=56de15104396321400ee25b9, Answer='Papireto...'\n73: QID=5ad3ee2d604f3c001a3ff7e1, Answer='Ireland...'\n74: QID=5ad3ee2d604f3c001a3ff7e2, Answer='France...'\n75: QID=5ad3ee2d604f3c001a3ff7e3, Answer='Alexander IV...'\n76: QID=56de1563cffd8e1900b4b5c2, Answer='1066...'\n77: QID=56de1563cffd8e1900b4b5c3, Answer='NAS Bermuda...'\n78: QID=56de1563cffd8e1900b4b5c4, Answer='Aventine...'\n79: QID=5ad3f028604f3c001a3ff823, Answer='seafaring raiders...'\n80: QID=5ad3f028604f3c001a3ff824, Answer='Richard...'\n81: QID=5ad3f028604f3c001a3ff825, Answer='Fortress of Louisbourg...'\n82: QID=56de15dbcffd8e1900b4b5c8, Answer='Sheila...'\n83: QID=56de15dbcffd8e1900b4b5c9, Answer='Edward Oxford...'\n84: QID=56de15dbcffd8e1900b4b5ca, Answer='Anjou...'\n85: QID=56de15dbcffd8e1900b4b5cb, Answer='Perdiccas...'\n86: QID=5ad3f187604f3c001a3ff86f, Answer='Princess Wencheng...'\n87: QID=5ad3f187604f3c001a3ff870, Answer='1195...'\n88: QID=5ad3f187604f3c001a3ff871, Answer='Spain...'\n89: QID=56de1645cffd8e1900b4b5d0, Answer='Wallis Simpson...'\n90: QID=56de1645cffd8e1900b4b5d1, Answer='early 1194...'\n91: QID=56de1645cffd8e1900b4b5d2, Answer='Reginald...'\n92: QID=5ad3f350604f3c001a3ff8ef, Answer='1052...'\n93: QID=5ad3f350604f3c001a3ff8f0, Answer='modern military structures...'\n94: QID=5ad3f350604f3c001a3ff8f1, Answer='Richard...'\n95: QID=56de16ca4396321400ee25c5, Answer='in the abbey...'\n96: QID=56de16ca4396321400ee25c6, Answer='John...'\n97: QID=56de16ca4396321400ee25c7, Answer='18 August–15 September 1951...'\n98: QID=56de16ca4396321400ee25c8, Answer='the nobility...'\n99: QID=5ad3f4b1604f3c001a3ff951, Answer='1066...'\n100: QID=5ad3f4b1604f3c001a3ff952, Answer='Battle of Lechfeld...'\n101: QID=5ad3f4b1604f3c001a3ff953, Answer='Francois IX...'\n102: QID=5ad3f4b1604f3c001a3ff954, Answer='European power...'\n103: QID=56de1728cffd8e1900b4b5d7, Answer='Middle English...'\n104: QID=5ad3f5b0604f3c001a3ff9ab, Answer='English...'\n105: QID=5ad3f5b0604f3c001a3ff9ac, Answer='Old French...'\n106: QID=5ad3f5b0604f3c001a3ff9ad, Answer='Dante Alighieri...'\n107: QID=56de179dcffd8e1900b4b5da, Answer='1066...'\n108: QID=56de179dcffd8e1900b4b5db, Answer='Ireland...'\n109: QID=56de179dcffd8e1900b4b5dc, Answer='Gaelic...'\n110: QID=5ad3f6f5604f3c001a3ffa09, Answer='Ireland...'\n111: QID=5ad3f6f5604f3c001a3ffa0a, Answer='Boston politics...'\n112: QID=5ad3f6f5604f3c001a3ffa0b, Answer='Château Gaillard...'\n113: QID=56de17f9cffd8e1900b4b5e0, Answer='Richard...'\n114: QID=56de17f9cffd8e1900b4b5e1, Answer='Peter Townsend...'\n115: QID=56de17f9cffd8e1900b4b5e2, Answer='1209...'\n116: QID=56de17f9cffd8e1900b4b5e3, Answer='52 personnel and citizens...'\n117: QID=5ad3f7ac604f3c001a3ffa3b, Answer='Sheila...'\n118: QID=5ad3f7ac604f3c001a3ffa3c, Answer='Vikings...'\n119: QID=5ad3f7ac604f3c001a3ffa3d, Answer='William...'\n120: QID=56de3cd0cffd8e1900b4b6be, Answer='Roxana...'\n121: QID=56de3cd0cffd8e1900b4b6bf, Answer='Spanish...'\n122: QID=5ad3f8d2604f3c001a3ffa8d, Answer='Alexander II of Scotland...'\n123: QID=5ad3f8d2604f3c001a3ffa8e, Answer='masonic lodges...'\n124: QID=56de3d594396321400ee26ca, Answer='King George III...'\n125: QID=56de3d594396321400ee26cb, Answer='France...'\n126: QID=56de3d594396321400ee26cc, Answer='William Cookworthy...'\n127: QID=5ad3fb01604f3c001a3ffb35, Answer='John...'\n128: QID=5ad3fb01604f3c001a3ffb36, Answer='King George III...'\n129: QID=56de3dbacffd8e1900b4b6d2, Answer='England...'\n130: QID=5ad3fb6e604f3c001a3ffb5f, Answer='a cross-channel empire...'\n131: QID=5ad3fb6e604f3c001a3ffb60, Answer='North Wales...'\n132: QID=56de3e414396321400ee26d8, Answer='1789...'\n133: QID=56de3e414396321400ee26d9, Answer='Philip II of Spain...'\n134: QID=5ad3fc41604f3c001a3ffb8f, Answer='Sicily...'\n135: QID=5ad3fc41604f3c001a3ffb90, Answer='the Crusades...'\n136: QID=5ad3fc41604f3c001a3ffb91, Answer='Charles Martel...'\n137: QID=5ad3fc41604f3c001a3ffb92, Answer='Eighty Years' War...'\n138: QID=56de3ebc4396321400ee26e6, Answer='1099...'\n139: QID=56de3ebc4396321400ee26e7, Answer='Mithradates VI...'\n140: QID=56de3ebc4396321400ee26e8, Answer='Gallic Wars...'\n141: QID=5ad4017a604f3c001a3ffd1f, Answer='202-195...'\n142: QID=5ad4017a604f3c001a3ffd20, Answer='Prince Albert...'\n143: QID=56de3efccffd8e1900b4b6fe, Answer='over three centuries...'\n144: QID=5ad401f2604f3c001a3ffd41, Answer='Norman conquest of Islamic Sic...'\n145: QID=5ad401f2604f3c001a3ffd42, Answer='King rpád...'\n146: QID=56de3f784396321400ee26fa, Answer='death of his mother...'\n147: QID=56de3f784396321400ee26fb, Answer='Isabel of Gloucester...'\n148: QID=56de3f784396321400ee26fc, Answer='1216...'\n149: QID=56de3f784396321400ee26fd, Answer='Alexander the Great...'\n150: QID=5ad40280604f3c001a3ffd57, Answer='Holy Roman Empire...'\n151: QID=5ad40280604f3c001a3ffd58, Answer='1855...'\n152: QID=5ad40280604f3c001a3ffd59, Answer='Ptolemaic Egypt...'\n153: QID=56de40da4396321400ee2708, Answer='Lou Ferrigno...'\n154: QID=56de40da4396321400ee2709, Answer='wood...'\n155: QID=56de40da4396321400ee270a, Answer='Lysimachus...'\n156: QID=5ad404a6604f3c001a3ffde1, Answer='tzi the Iceman...'\n157: QID=5ad404a6604f3c001a3ffde2, Answer='King Constantine...'\n158: QID=5ad404a6604f3c001a3ffde3, Answer='the Queen and Albert...'\n159: QID=56de49564396321400ee277a, Answer='Australia...'\n160: QID=5ad40419604f3c001a3ffdb7, Answer='the Catalan rulers...'\n161: QID=5ad40419604f3c001a3ffdb8, Answer='Andaman and Nicobar Islands...'\n162: QID=56de49a8cffd8e1900b4b7a7, Answer='Father San Vitores...'\n163: QID=56de49a8cffd8e1900b4b7a8, Answer='Universal Partnerships...'\n164: QID=56de49a8cffd8e1900b4b7a9, Answer='Universal Pictures...'\n165: QID=5ad403c1604f3c001a3ffd97, Answer='Angevin...'\n166: QID=5ad403c1604f3c001a3ffd98, Answer='the United Kingdom...'\n167: QID=56de4a474396321400ee2786, Answer='Channel Islands...'\n168: QID=56de4a474396321400ee2787, Answer='Xeer...'\n169: QID=5ad40358604f3c001a3ffd7d, Answer='Canon law...'\n170: QID=5ad40358604f3c001a3ffd7e, Answer='Restatement of Torts...'\n171: QID=5ad40358604f3c001a3ffd7f, Answer='Canon law...'\n172: QID=56de4a89cffd8e1900b4b7bd, Answer='Romanesque...'\n173: QID=56de4a89cffd8e1900b4b7be, Answer='semi-circular...'\n174: QID=5ad402ce604f3c001a3ffd67, Answer='pointed arch...'\n175: QID=56de4b074396321400ee2793, Answer='Gothic...'\n176: QID=56de4b074396321400ee2794, Answer='First Romanesque...'\n177: QID=56de4b074396321400ee2795, Answer='the Tomb...'\n178: QID=5ad400b0604f3c001a3ffcdf, Answer='Old English...'\n179: QID=5ad400b0604f3c001a3ffce0, Answer='Renaissance architecture...'\n180: QID=5ad400b0604f3c001a3ffce1, Answer='Romanesque architecture...'\n181: QID=56de4b5c4396321400ee2799, Answer='mid-16th century...'\n182: QID=56de4b5c4396321400ee279a, Answer='Protestant reformers...'\n183: QID=5ad3ffd7604f3c001a3ffca7, Answer='mosaic...'\n184: QID=5ad3ffd7604f3c001a3ffca8, Answer='King Henry VIII...'\n185: QID=5ad3ffd7604f3c001a3ffca9, Answer='Eastern and Western Europe...'\n186: QID=5ad3ffd7604f3c001a3ffcaa, Answer='the Dutch Republic...'\n187: QID=56de4bb84396321400ee27a2, Answer='during the 1550s...'\n188: QID=5ad3ff1b604f3c001a3ffc73, Answer='French and Indian War...'\n189: QID=5ad3ff1b604f3c001a3ffc74, Answer='the Bourbon restoration...'\n190: QID=56de4c324396321400ee27ab, Answer='silk...'\n191: QID=56de4c324396321400ee27ac, Answer='Cappella Palatina of Roger II...'\n192: QID=56de4c324396321400ee27ad, Answer='Justinian...'\n193: QID=5ad3fe91604f3c001a3ffc47, Answer='Cappella Palatina...'\n194: QID=5ad3fe91604f3c001a3ffc48, Answer='the Byzantine Emperor...'\n195: QID=56de51244396321400ee27ef, Answer='figurative monumental sculptur...'\n196: QID=5ad3fe0d604f3c001a3ffc1b, Answer='ivories...'\n197: QID=5ad3fe0d604f3c001a3ffc1c, Answer='Gothic style...'\n198: QID=5ad3fe0d604f3c001a3ffc1d, Answer='monasteries...'\n199: QID=56de51c64396321400ee27f7, Answer='11th...'\n200: QID=56de51c64396321400ee27f8, Answer='Edward's...'\n201: QID=5ad3fd68604f3c001a3ffbe7, Answer='a cross-channel empire...'\n202: QID=5ad3fd68604f3c001a3ffbe8, Answer='the Bonus March encampment...'\n203: QID=56de52614396321400ee27fb, Answer='France...'\n204: QID=56de52614396321400ee27fc, Answer='Cistercian...'\n205: QID=56de52614396321400ee27fd, Answer='Pope Pius XI...'\n206: QID=56de52614396321400ee27fe, Answer='cenobitism...'\n207: QID=5ad3fccf604f3c001a3ffbb5, Answer='Austrians...'\n208: QID=56e16182e3433e1400422e28, Answer='mathematics...'\n209: QID=56e16182e3433e1400422e29, Answer='cyclic...'\n210: QID=56e16182e3433e1400422e2a, Answer='logical...'\n211: QID=5ad5316b5b96ef001a10ab72, Answer='Turing-complete...'\n212: QID=5ad5316b5b96ef001a10ab73, Answer='algebra...'\n213: QID=5ad5316b5b96ef001a10ab74, Answer='Entscheidungsproblem...'\n214: QID=5ad5316b5b96ef001a10ab75, Answer='Entscheidungsproblem...'\n215: QID=5ad5316b5b96ef001a10ab76, Answer='The slide rule...'\n216: QID=56e16839cd28a01900c67887, Answer='cyclic...'\n217: QID=56e16839cd28a01900c67888, Answer='a computer program...'\n218: QID=56e16839cd28a01900c67889, Answer='knowledge...'\n219: QID=56e16839cd28a01900c6788a, Answer='watts...'\n220: QID=56e16839cd28a01900c6788b, Answer='contain errors...'\n221: QID=5ad532575b96ef001a10ab7c, Answer='The Church–Turing thesis...'\n222: QID=5ad532575b96ef001a10ab7d, Answer='decompression...'\n223: QID=5ad532575b96ef001a10ab7e, Answer='mathematical 'theory'...'\n224: QID=5ad532575b96ef001a10ab7f, Answer='candela...'\n225: QID=5ad532575b96ef001a10ab80, Answer='sexagesimal...'\n226: QID=56e17644e3433e1400422f40, Answer='computability and computationa...'\n227: QID=56e17644e3433e1400422f41, Answer='computational complexity...'\n228: QID=56e17644e3433e1400422f42, Answer='computational complexity...'\n229: QID=56e17644e3433e1400422f43, Answer='computational complexity...'\n230: QID=5ad5344b5b96ef001a10ab86, Answer='computability...'\n231: QID=5ad5344b5b96ef001a10ab87, Answer='calculators)...'\n232: QID=5ad5344b5b96ef001a10ab88, Answer='boundary value analysis...'\n233: QID=5ad5344b5b96ef001a10ab89, Answer='Black-box testing...'\n234: QID=5ad5344b5b96ef001a10ab8a, Answer='finite groups...'\n235: QID=56e17a7ccd28a01900c679a1, Answer='bits...'\n236: QID=56e17a7ccd28a01900c679a2, Answer='White-box testing...'\n237: QID=56e17a7ccd28a01900c679a3, Answer='abstract...'\n238: QID=56e17a7ccd28a01900c679a4, Answer='Dosimeter...'\n239: QID=56e17a7ccd28a01900c679a5, Answer='assembly language...'\n240: QID=5ad5364c5b96ef001a10ab90, Answer='a set...'\n241: QID=5ad5364c5b96ef001a10ab91, Answer='data symbol...'\n242: QID=5ad5364c5b96ef001a10ab92, Answer='problem...'\n243: QID=5ad5364c5b96ef001a10ab93, Answer='123...'\n244: QID=5ad5364c5b96ef001a10ab94, Answer='the ordinary algebraic concept...'\n245: QID=56e17e6ee3433e1400422f7f, Answer='13...'\n246: QID=56e17e6ee3433e1400422f80, Answer='the overwintering areas of the...'\n247: QID=56e17e6ee3433e1400422f81, Answer='nature and limits of computati...'\n248: QID=5ad537a15b96ef001a10ab9a, Answer='22 mi...'\n249: QID=5ad537a15b96ef001a10ab9b, Answer='non-functional dimensions...'\n250: QID=5ad537a15b96ef001a10ab9c, Answer='the existence of facts such as...'\n251: QID=5ad537a15b96ef001a10ab9d, Answer='mathematicians...'\n252: QID=56e181d9e3433e1400422fa0, Answer='character...'\n253: QID=56e181d9e3433e1400422fa1, Answer='lowercase...'\n254: QID=56e181d9e3433e1400422fa2, Answer='mnemonic...'\n255: QID=56e181d9e3433e1400422fa3, Answer='decimal...'\n256: QID=56e181d9e3433e1400422fa4, Answer='ASCII...'\n257: QID=5ad5391e5b96ef001a10aba2, Answer='flag of Greece...'\n258: QID=5ad5391e5b96ef001a10aba3, Answer='The Georgian alphabet...'\n259: QID=5ad5391e5b96ef001a10aba4, Answer='zahir...'\n260: QID=5ad5391e5b96ef001a10aba5, Answer='negative numbers...'\n261: QID=5ad5391e5b96ef001a10aba6, Answer='ASCII encodes 128 specified ch...'\n262: QID=56e190bce3433e1400422fc8, Answer='computational functions of the...'\n263: QID=56e190bce3433e1400422fc9, Answer='formal...'\n264: QID=56e190bce3433e1400422fca, Answer='boolean truth values...'\n265: QID=56e190bce3433e1400422fcb, Answer='back-translation...'\n266: QID=56e190bce3433e1400422fcc, Answer='ACK...'\n267: QID=5ad53b9d5b96ef001a10abc8, Answer='computability...'\n268: QID=5ad53b9d5b96ef001a10abc9, Answer='calculators...'\n269: QID=5ad53b9d5b96ef001a10abca, Answer='lect...'\n270: QID=5ad53b9d5b96ef001a10abcb, Answer='the standard deviation of the ...'\n271: QID=5ad53b9d5b96ef001a10abcc, Answer='STALL...'\n272: QID=56e19557e3433e1400422fee, Answer='Fourier...'\n273: QID=56e19557e3433e1400422ff0, Answer='set G...'\n274: QID=56e19557e3433e1400422ff1, Answer='UTF-32...'\n275: QID=5ad53d705b96ef001a10abd2, Answer='timelines...'\n276: QID=5ad53d705b96ef001a10abd3, Answer='underlying set...'\n277: QID=5ad53d705b96ef001a10abd4, Answer='reverse-translation...'\n278: QID=5ad53d705b96ef001a10abd5, Answer='mutual intelligibility...'\n279: QID=56e19724cd28a01900c679f6, Answer='failure testing...'\n280: QID=56e19724cd28a01900c679f7, Answer='each f-number factor...'\n281: QID=56e19724cd28a01900c679f8, Answer='game...'\n282: QID=56e19724cd28a01900c679f9, Answer='aesthetic considerations...'\n283: QID=56e19724cd28a01900c679fa, Answer='simple...'\n284: QID=5ad53e615b96ef001a10abda, Answer='lossless data compression...'\n285: QID=5ad53e615b96ef001a10abdb, Answer='arithmetic operations...'\n286: QID=5ad53e615b96ef001a10abdc, Answer='Destructive testing...'\n287: QID=5ad53e615b96ef001a10abdd, Answer='bugs...'\n288: QID=5ad53e615b96ef001a10abde, Answer='complex...'\n289: QID=56e1a0dccd28a01900c67a2e, Answer='re-running previous sets of te...'\n290: QID=56e1a0dccd28a01900c67a2f, Answer='modulo 12...'\n291: QID=5ad53f815b96ef001a10abe4, Answer='the doctrine of stare decisis...'\n292: QID=5ad53f815b96ef001a10abe5, Answer='addition...'\n293: QID=5ad53f815b96ef001a10abe6, Answer='ALU...'\n294: QID=56e1a38de3433e140042305c, Answer='gigaflops...'\n295: QID=56e1a38de3433e140042305d, Answer='lighting...'\n296: QID=56e1a38de3433e140042305e, Answer='The number (as on a clock dial...'\n297: QID=56e1a38de3433e140042305f, Answer='candela...'\n298: QID=56e1a38de3433e1400423060, Answer='biological complexity...'\n299: QID=5ad541ad5b96ef001a10abea, Answer='a nontrivial group...'\n300: QID=5ad541ad5b96ef001a10abeb, Answer='time...'\n301: QID=5ad541ad5b96ef001a10abec, Answer='You write down the problem. Yo...'\n302: QID=5ad541ad5b96ef001a10abed, Answer='lossy data compression schemes...'\n303: QID=5ad541ad5b96ef001a10abee, Answer='time is what clocks measure...'\n304: QID=56e1a564cd28a01900c67a48, Answer='Gödel...'\n305: QID=56e1a564cd28a01900c67a49, Answer='processor...'\n306: QID=56e1a564cd28a01900c67a4a, Answer='the second...'\n307: QID=56e1a564cd28a01900c67a4b, Answer='decibels...'\n308: QID=56e1a564cd28a01900c67a4c, Answer='The Feynman Algorithm...'\n309: QID=5ad542db5b96ef001a10abf4, Answer='one says that observing a cert...'\n310: QID=5ad542db5b96ef001a10abf5, Answer='Popper's...'\n311: QID=5ad542db5b96ef001a10abf6, Answer='cyclical...'\n312: QID=5ad542db5b96ef001a10abf7, Answer='minuscule...'\n313: QID=5ad542db5b96ef001a10abf8, Answer='arithmetic...'\n314: QID=56e1aba0e3433e1400423094, Answer='Turing machine...'\n315: QID=56e1aba0e3433e1400423095, Answer='stored on tape...'\n316: QID=56e1aba0e3433e1400423097, Answer='relational model...'\n317: QID=56e1aba0e3433e1400423098, Answer='instructions...'\n318: QID=5ad543c05b96ef001a10abfe, Answer='supercomputer...'\n319: QID=5ad543c05b96ef001a10abff, Answer='stamper...'\n320: QID=5ad543c05b96ef001a10ac00, Answer='Any device which processes inf...'\n321: QID=5ad543c05b96ef001a10ac01, Answer='computational neuroscience...'\n322: QID=56e1aff7cd28a01900c67a68, Answer='Turing-complete...'\n323: QID=56e1aff7cd28a01900c67a69, Answer='inputs and outputs...'\n324: QID=56e1aff7cd28a01900c67a6a, Answer='plasmids...'\n325: QID=56e1aff7cd28a01900c67a6b, Answer='universal...'\n326: QID=56e1aff7cd28a01900c67a6c, Answer='random number generators...'\n327: QID=5ad546c75b96ef001a10ac0e, Answer='federal agencies...'\n328: QID=5ad546c75b96ef001a10ac0f, Answer='IBM Research...'\n329: QID=5ad546c75b96ef001a10ac10, Answer='possible shortcuts...'\n330: QID=5ad546c75b96ef001a10ac11, Answer='Superscalar...'\n331: QID=5ad546c75b96ef001a10ac12, Answer='it is difficult to explain why...'\n332: QID=56e1b00ce3433e140042309e, Answer='computability...'\n333: QID=56e1b00ce3433e140042309f, Answer='memory capacity and operating ...'\n334: QID=56e1b00ce3433e14004230a1, Answer='supercomputer, cellular automa...'\n335: QID=5ad545545b96ef001a10ac06, Answer='calculators...'\n336: QID=5ad545545b96ef001a10ac07, Answer='memory capacity and operating ...'\n337: QID=5ad545545b96ef001a10ac08, Answer='SCIMs...'\n338: QID=5ad545545b96ef001a10ac09, Answer='time...'\n339: QID=56e1b169cd28a01900c67a72, Answer='transistors...'\n340: QID=56e1b169cd28a01900c67a73, Answer='number of states...'\n341: QID=56e1b169cd28a01900c67a74, Answer='nitrogen and carbon proportion...'\n342: QID=56e1b169cd28a01900c67a75, Answer='able to perform the same compu...'\n343: QID=5ad547945b96ef001a10ac18, Answer='netbook, supercomputer, cellul...'\n344: QID=5ad547945b96ef001a10ac19, Answer='ASA scale...'\n345: QID=5ad547945b96ef001a10ac1a, Answer='nitrogen and carbon proportion...'\n346: QID=5ad547945b96ef001a10ac1b, Answer='Apple's System 7...'\n347: QID=56e1b355e3433e14004230b0, Answer='non-deterministic systems...'\n348: QID=56e1b355e3433e14004230b1, Answer='compression...'\n349: QID=56e1b355e3433e14004230b2, Answer='programs...'\n350: QID=56e1b355e3433e14004230b3, Answer='time...'\n351: QID=5ad5489b5b96ef001a10ac2a, Answer='genomics...'\n352: QID=5ad5489b5b96ef001a10ac2b, Answer='calculators...'\n353: QID=5ad5489b5b96ef001a10ac2c, Answer='computational complexity...'\n354: QID=5ad5489b5b96ef001a10ac2d, Answer='single neurons...'\n355: QID=5ad5489b5b96ef001a10ac2e, Answer='storage capacity...'\n356: QID=56e1b62ecd28a01900c67aa3, Answer='time...'\n357: QID=56e1b62ecd28a01900c67aa4, Answer='computational complexity...'\n358: QID=56e1b62ecd28a01900c67aa5, Answer='sequence of events...'\n359: QID=56e1b62ecd28a01900c67aa6, Answer='time...'\n360: QID=5ad54a375b96ef001a10ac48, Answer='time is what clocks measure...'\n361: QID=5ad54a375b96ef001a10ac49, Answer='finite memory stores...'\n362: QID=5ad54a375b96ef001a10ac4a, Answer='sequence of events...'\n363: QID=5ad54a375b96ef001a10ac4b, Answer='time...'\n364: QID=5ad54a375b96ef001a10ac4c, Answer='slide rules...'\n365: QID=56e1b754cd28a01900c67abc, Answer='mental framework...'\n366: QID=56e1b754cd28a01900c67abd, Answer='computational...'\n367: QID=56e1b754cd28a01900c67abe, Answer='A broad operational definition...'\n368: QID=56e1b754cd28a01900c67abf, Answer='dimensions...'\n369: QID=56e1b754cd28a01900c67ac0, Answer='time series analysis...'\n370: QID=5ad54b035b96ef001a10ac52, Answer='a vacuum...'\n371: QID=5ad54b035b96ef001a10ac53, Answer='Computer algebra systems can b...'\n372: QID=5ad54b035b96ef001a10ac54, Answer='there is no generally accepted...'\n373: QID=5ad54b035b96ef001a10ac55, Answer='time vs. energy...'\n374: QID=5ad54b035b96ef001a10ac56, Answer='quantifiability...'\n375: QID=56e1b8f3e3433e14004230e6, Answer='dative case...'\n376: QID=56e1b8f3e3433e14004230e7, Answer='value...'\n377: QID=56e1b8f3e3433e14004230e8, Answer='volume testing...'\n378: QID=56e1b8f3e3433e14004230e9, Answer='bisexual and lesbian identific...'\n379: QID=5ad54c2f5b96ef001a10ac5c, Answer='vocative and dative...'\n380: QID=5ad54c2f5b96ef001a10ac5d, Answer='spatial measurements are used ...'\n381: QID=5ad54c2f5b96ef001a10ac5e, Answer='time vs. energy...'\n382: QID=5ad54c2f5b96ef001a10ac5f, Answer='numeric...'\n383: QID=56e1ba41cd28a01900c67ae0, Answer='merge sort algorithm...'\n384: QID=56e1ba41cd28a01900c67ae1, Answer='arithmetic operation...'\n385: QID=56e1ba41cd28a01900c67ae2, Answer='the minimum case...'\n386: QID=5ad54d625b96ef001a10ac64, Answer='the first and second halves of...'\n387: QID=5ad54d625b96ef001a10ac65, Answer='depaneling...'\n388: QID=5ad54d625b96ef001a10ac66, Answer='minimum...'\n389: QID=5ad54d625b96ef001a10ac67, Answer='zero samples...'\n390: QID=5ad54d625b96ef001a10ac68, Answer='the identity and the rotations...'\n391: QID=56e1bc3ae3433e1400423104, Answer='business...'\n392: QID=56e1bc3ae3433e1400423105, Answer='neural computation...'\n393: QID=56e1bc3ae3433e1400423106, Answer='circularity of definition...'\n394: QID=56e1bc3ae3433e1400423107, Answer='n...'\n395: QID=56e1bc3ae3433e1400423108, Answer='infinitely parallel...'\n396: QID=5ad54e7c5b96ef001a10ac76, Answer='latency...'\n397: QID=5ad54e7c5b96ef001a10ac77, Answer='zero samples...'\n398: QID=5ad54e7c5b96ef001a10ac78, Answer='philosophy of art...'\n399: QID=5ad54e7c5b96ef001a10ac79, Answer='temporal machines...'\n400: QID=5ad54e7c5b96ef001a10ac7a, Answer='any computer that is capable o...'\n401: QID=56e1bd4acd28a01900c67afc, Answer='ceiling...'\n402: QID=56e1bd4acd28a01900c67afd, Answer='information stored in memory...'\n403: QID=56e1bd4acd28a01900c67afe, Answer='ligatures...'\n404: QID=56e1bd4acd28a01900c67aff, Answer='objects\"...'\n405: QID=5ad54f775b96ef001a10ac88, Answer='arithmetic operations...'\n406: QID=5ad54f775b96ef001a10ac89, Answer='the identity of indiscernibles...'\n407: QID=5ad54f775b96ef001a10ac8a, Answer='many different definitions are...'\n408: QID=5ad54f775b96ef001a10ac8b, Answer='...'\n409: QID=56e1c0f6cd28a01900c67b2c, Answer='a mathematical notion...'\n410: QID=56e1c0f6cd28a01900c67b2d, Answer='Class I...'\n411: QID=56e1c0f6cd28a01900c67b2e, Answer='finite groups...'\n412: QID=5ad5501f5b96ef001a10ac90, Answer='must semantics, and temporal m...'\n413: QID=5ad5501f5b96ef001a10ac91, Answer='the Von Neumann approach...'\n414: QID=5ad5501f5b96ef001a10ac92, Answer='Class I...'\n415: QID=5ad5501f5b96ef001a10ac93, Answer='the assumed computation model...'\n416: QID=56e1c2eee3433e1400423134, Answer='continuous magnitudes...'\n417: QID=56e1c2eee3433e1400423135, Answer='finite...'\n418: QID=56e1c2eee3433e1400423136, Answer='supercomputer...'\n419: QID=56e1c2eee3433e1400423137, Answer='The B-series...'\n420: QID=56e1c2eee3433e1400423138, Answer='Class I...'\n421: QID=5ad55ee35b96ef001a10ace4, Answer='the assumed computation model...'\n422: QID=5ad55ee35b96ef001a10ace5, Answer='the assumed computation model...'\n423: QID=5ad55ee35b96ef001a10ace6, Answer='Entscheidungsproblem...'\n424: QID=5ad55ee35b96ef001a10ace7, Answer='code point...'\n425: QID=5ad55ee35b96ef001a10ace8, Answer='B-series...'\n426: QID=56e1c3e1e3433e1400423148, Answer='time and energy...'\n427: QID=56e1c3e1e3433e1400423149, Answer='circularity of definition...'\n428: QID=56e1c3e1e3433e140042314a, Answer='quanta...'\n429: QID=5ad55fe75b96ef001a10ad0c, Answer='time...'\n430: QID=5ad55fe75b96ef001a10ad0d, Answer='computing hardware...'\n431: QID=5ad55fe75b96ef001a10ad0e, Answer='time is what clocks measure...'\n432: QID=5ad55fe75b96ef001a10ad0f, Answer='Kármán line...'\n433: QID=56e1c4fce3433e140042314e, Answer='Class I...'\n434: QID=56e1c4fce3433e140042314f, Answer='alternating current...'\n435: QID=56e1c4fce3433e1400423150, Answer='Universal...'\n436: QID=56e1c4fce3433e1400423151, Answer='ten...'\n437: QID=56e1c4fce3433e1400423152, Answer='abstract formalization...'\n438: QID=5ad560b85b96ef001a10ad1e, Answer='Class I...'\n439: QID=5ad560b85b96ef001a10ad1f, Answer='IBM...'\n440: QID=5ad560b85b96ef001a10ad20, Answer='IBM...'\n441: QID=5ad560b85b96ef001a10ad21, Answer='Class III...'\n442: QID=5ad560b85b96ef001a10ad22, Answer='OS/360...'\n443: QID=56e1c720e3433e140042316a, Answer='time vs energy...'\n444: QID=56e1c720e3433e140042316b, Answer='s]...'\n445: QID=56e1c720e3433e140042316c, Answer='non-relativistic classical mec...'\n446: QID=56e1c720e3433e140042316d, Answer='efficiency...'\n447: QID=56e1c720e3433e140042316e, Answer='The Church–Turing thesis...'\n448: QID=5ad561c85b96ef001a10ad3c, Answer='the number of states...'\n449: QID=5ad561c85b96ef001a10ad3d, Answer='a proper class...'\n450: QID=5ad561c85b96ef001a10ad3e, Answer='daylight saving time...'\n451: QID=5ad561c85b96ef001a10ad3f, Answer='the axioms...'\n452: QID=5ad561c85b96ef001a10ad40, Answer='biological fitness...'\n453: QID=56e1c7e2cd28a01900c67b74, Answer='theorem...'\n454: QID=56e1c7e2cd28a01900c67b75, Answer='time...'\n455: QID=56e1c7e2cd28a01900c67b76, Answer='importance...'\n456: QID=5ad562525b96ef001a10ad50, Answer='the complexity of the assumed ...'\n457: QID=5ad562525b96ef001a10ad51, Answer='Philosophy of space and time...'\n458: QID=5ad562525b96ef001a10ad52, Answer='time...'\n459: QID=5ad562525b96ef001a10ad53, Answer='matter...'\n460: QID=56e1c9bfe3433e1400423192, Answer='theorem...'\n461: QID=56e1c9bfe3433e1400423193, Answer='lossy...'\n462: QID=56e1c9bfe3433e1400423194, Answer='reduces bits...'\n463: QID=56e1c9bfe3433e1400423195, Answer='lossy or lossless...'\n464: QID=56e1c9bfe3433e1400423196, Answer='lossy...'\n465: QID=5ad5632f5b96ef001a10ad6c, Answer='the axioms...'\n466: QID=5ad5632f5b96ef001a10ad6d, Answer='empirical laws and explanation...'\n467: QID=5ad5632f5b96ef001a10ad6e, Answer='double indirect method...'\n468: QID=5ad5632f5b96ef001a10ad6f, Answer='error elimination...'\n469: QID=5ad5632f5b96ef001a10ad70, Answer='lossy or lossless...'\n470: QID=56e1cbe2cd28a01900c67bac, Answer='lossy audio compression...'\n471: QID=56e1cbe2cd28a01900c67bad, Answer='modulus...'\n472: QID=56e1cbe2cd28a01900c67bae, Answer='s...'\n473: QID=56e1cbe2cd28a01900c67baf, Answer='modulus...'\n474: QID=56e1cbe2cd28a01900c67bb0, Answer='the more complex operations...'\n475: QID=5ad5648b5b96ef001a10ad94, Answer='lossless...'\n476: QID=5ad5648b5b96ef001a10ad95, Answer='encoding information using few...'\n477: QID=5ad5648b5b96ef001a10ad96, Answer='modular addition...'\n478: QID=5ad5648b5b96ef001a10ad97, Answer='modulus...'\n479: QID=5ad5648b5b96ef001a10ad98, Answer='division...'\n480: QID=56e1ce08e3433e14004231a4, Answer='computational complexity...'\n481: QID=56e1ce08e3433e14004231a5, Answer='one party wishes to create an ...'\n482: QID=56e1ce08e3433e14004231a6, Answer='arithmetic...'\n483: QID=56e1ce08e3433e14004231a8, Answer='unanswerable...'\n484: QID=5ad565575b96ef001a10adb2, Answer='the degree of compression...'\n485: QID=5ad565575b96ef001a10adb3, Answer='lossless compression...'\n486: QID=5ad565575b96ef001a10adb4, Answer='computational complexity...'\n487: QID=5ad565575b96ef001a10adb5, Answer='hypotheses...'\n488: QID=56e1d9fee3433e14004231cb, Answer='finite simple groups...'\n489: QID=56e1d9fee3433e14004231cc, Answer='Class III...'\n490: QID=56e1d9fee3433e14004231cd, Answer='that Principia Mathematica cou...'\n491: QID=56e1d9fee3433e14004231ce, Answer='full energy...'\n492: QID=5ad566375b96ef001a10adce, Answer='hard for C...'\n493: QID=5ad566375b96ef001a10adcf, Answer='computational complexity...'\n494: QID=5ad566375b96ef001a10add0, Answer='Class I...'\n495: QID=5ad566375b96ef001a10add1, Answer='the number of states is not kn...'\n496: QID=56e1dc62cd28a01900c67bca, Answer='Class I...'\n497: QID=56e1dc62cd28a01900c67bcb, Answer='prime number is necessarily cy...'\n498: QID=56e1dc62cd28a01900c67bcc, Answer='Class I...'\n499: QID=56e1dc62cd28a01900c67bcd, Answer='A nontrivial group is called s...'\n计算评测指标...\n评测结果：\nexact: 2.40\nf1: 4.05\ntotal: 500.00\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## 检索阶段优化，增大k与更换效果更好的句向量模型","metadata":{}},{"cell_type":"code","source":"import json\nimport collections\nimport re\nimport string\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\n# ------------------ SQuAD Evaluation Functions ------------------\n\ndef normalize_answer(s):\n    if s is None:\n        return \"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    return 2 * precision * recall / (precision + recall)\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                qid = qa['id']\n                gold_answers = [a['text'] for a in qa.get('answers', []) if normalize_answer(a.get('text', ''))]\n                if not gold_answers:\n                    gold_answers = ['']  # no answer questions\n                a_pred = preds.get(qid, \"\")\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\ndef make_eval_dict(exact_scores, f1_scores):\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        ('exact', 100.0 * sum(exact_scores.values()) / total if total else 0),\n        ('f1', 100.0 * sum(f1_scores.values()) / total if total else 0),\n        ('total', total),\n    ])\n\n# ------------------ RAG 主体流程 ------------------\n\ndef load_squad_dataset(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data_json = json.load(f)\n    return data_json['data']\n\ndef chunk_and_truncate_contexts(contexts, max_chars=3000):\n    out = []\n    for c in contexts:\n        if len(c) <= max_chars:\n            out.append(c)\n        else:\n            out.append(c[:max_chars] + \" ...\")\n    return out\n\ndef build_context_embeddings(contexts, embedder, batch_size=512):\n    embs = []\n    for i in tqdm(range(0, len(contexts), batch_size), desc=\"Embedding contexts\"):\n        batch = contexts[i:i+batch_size]\n        batch_emb = embedder.encode(batch, convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n        batch_emb = np.asarray(batch_emb, dtype=np.float32)\n        embs.append(batch_emb)\n    embedder.to('cpu')  # 回收GPU显存\n    return np.vstack(embs)\n\n\ndef retrieve_top_k(question, all_ctx_emb, embedder, contexts, k=20):\n    q_emb = embedder.encode([question], convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n    q_emb = np.asarray(q_emb, dtype=np.float32)\n    # 计算所有上下文与问题的内积相似度\n    sims = np.dot(all_ctx_emb, q_emb.T).squeeze(1)  # shape: (num_contexts,)\n    topk_idx = np.argpartition(-sims, k)[:k]\n    # 按相似度排序\n    topk_idx = topk_idx[np.argsort(-sims[topk_idx])]\n    hits = [contexts[i] for i in topk_idx if i < len(contexts)]\n    return hits\n\ndef generate_answer(question, contexts, tokenizer, model, max_new_tokens=64):\n    context_text = \"\\n\".join(chunk_and_truncate_contexts(contexts))\n    prompt = f\"Question: {question}\\nContext:\\n{context_text}\\nAnswer (if no answer, respond with 'no answer'):\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n    device = next(model.parameters()).device\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        num_beams=4,\n        no_repeat_ngram_size=3,\n        early_stopping=True\n    )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):]\n    answer = answer.strip()\n    if answer.lower() in [\"false\", \"no answer\", \"none\", \"\"]:\n        answer = \"\"\n    return answer\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_file = '/kaggle/input/squad-train/train-v2.0.json'\n    dev_file = '/kaggle/input/squad-dev/dev-v2.0.json'\n\n    sentence_transformer_name = 'sentence-transformers/all-mpnet-base-v2'\n    model_name = \"google/flan-t5-base\"\n    batch_size_embed = 256\n\n    print(\"加载训练数据...\")\n    train_data = load_squad_dataset(train_file)\n    print(\"加载验证数据...\")\n    dev_data = load_squad_dataset(dev_file)\n\n    # 提取所有训练上下文（遍历每篇文章每个段落）\n    contexts = []\n    for article in train_data:\n        for para in article[\"paragraphs\"]:\n            contexts.append(para[\"context\"])\n    print(f\"训练集上下文数量: {len(contexts)}\")\n\n    print(\"加载句向量模型...\")\n    embedder = SentenceTransformer(sentence_transformer_name, device=device)\n\n    print(\"GPU上计算所有上下文的向量表示...\")\n    all_ctx_emb = build_context_embeddings(contexts, embedder, batch_size=batch_size_embed)\n\n    print(f\"加载生成模型和tokenizer: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n    model.eval()\n\n    preds = {}\n    max_test = 500\n    count = 0\n\n    print(f\"开始对验证集前{max_test}个样本生成回答...\")\n    for article in dev_data:\n        for para in article[\"paragraphs\"]:\n            for qa in para[\"qas\"]:\n                if count >= max_test:\n                    break\n                qid = qa['id']\n                question = qa['question']\n\n                retrieved = retrieve_top_k(question, all_ctx_emb, embedder, contexts, k=5)\n                answer = generate_answer(question, retrieved, tokenizer, model)\n\n                preds[qid] = answer\n\n                print(count)\n                count += 1\n            if count >= max_test:\n                break\n        if count >= max_test:\n            break\n\n    # 构造只包含测试用的dev_data子集\n    filtered_dev_data = []\n    for article in dev_data:\n        new_article = {\"paragraphs\": []}\n        for para in article[\"paragraphs\"]:\n            new_para = {\"qas\": [], \"context\": para[\"context\"]}\n            for qa in para[\"qas\"]:\n                if qa['id'] in preds:\n                    new_para[\"qas\"].append(qa)\n            if new_para[\"qas\"]:\n                new_article[\"paragraphs\"].append(new_para)\n        if new_article[\"paragraphs\"]:\n            filtered_dev_data.append(new_article)\n\n    print(\"计算评测指标...\")\n    exact_scores, f1_scores = get_raw_scores(filtered_dev_data, preds)\n    eval_results = make_eval_dict(exact_scores, f1_scores)\n\n    print(\"评测结果：\")\n    for k, v in eval_results.items():\n        print(f\"{k}: {v:.2f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T07:21:19.256371Z","iopub.execute_input":"2025-08-10T07:21:19.257096Z","iopub.status.idle":"2025-08-10T07:27:31.173440Z","shell.execute_reply.started":"2025-08-10T07:21:19.257071Z","shell.execute_reply":"2025-08-10T07:27:31.172685Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"加载训练数据...\n加载验证数据...\n训练集上下文数量: 19035\n加载句向量模型...\nGPU上计算所有上下文的向量表示...\n","output_type":"stream"},{"name":"stderr","text":"Embedding contexts: 100%|██████████| 75/75 [04:39<00:00,  3.72s/it]\n","output_type":"stream"},{"name":"stdout","text":"加载生成模型和tokenizer: google/flan-t5-base\n开始对验证集前500个样本生成回答...\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n计算评测指标...\n评测结果：\nexact: 2.40\nf1: 4.05\ntotal: 500.00\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# 更换google生成模型","metadata":{}},{"cell_type":"code","source":"import json\nimport collections\nimport re\nimport string\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\n# ------------------ SQuAD Evaluation Functions ------------------\n\ndef normalize_answer(s):\n    if s is None:\n        return \"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    return 2 * precision * recall / (precision + recall)\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                qid = qa['id']\n                gold_answers = [a['text'] for a in qa.get('answers', []) if normalize_answer(a.get('text', ''))]\n                if not gold_answers:\n                    gold_answers = ['']  # no answer questions\n                a_pred = preds.get(qid, \"\")\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\ndef make_eval_dict(exact_scores, f1_scores):\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        ('exact', 100.0 * sum(exact_scores.values()) / total if total else 0),\n        ('f1', 100.0 * sum(f1_scores.values()) / total if total else 0),\n        ('total', total),\n    ])\n\n# ------------------ RAG 主体流程 ------------------\n\ndef load_squad_dataset(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data_json = json.load(f)\n    return data_json['data']\n\ndef chunk_and_truncate_contexts(contexts, max_chars=3000):\n    out = []\n    for c in contexts:\n        if len(c) <= max_chars:\n            out.append(c)\n        else:\n            out.append(c[:max_chars] + \" ...\")\n    return out\n\ndef build_context_embeddings(contexts, embedder, batch_size=512):\n    embs = []\n    for i in tqdm(range(0, len(contexts), batch_size), desc=\"Embedding contexts\"):\n        batch = contexts[i:i+batch_size]\n        batch_emb = embedder.encode(batch, convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n        batch_emb = np.asarray(batch_emb, dtype=np.float32)\n        embs.append(batch_emb)\n    embedder.to('cpu')  # 回收GPU显存\n    return np.vstack(embs)\n\n\ndef retrieve_top_k(question, all_ctx_emb, embedder, contexts, k=20):\n    q_emb = embedder.encode([question], convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n    q_emb = np.asarray(q_emb, dtype=np.float32)\n    # 计算所有上下文与问题的内积相似度\n    sims = np.dot(all_ctx_emb, q_emb.T).squeeze(1)  # shape: (num_contexts,)\n    topk_idx = np.argpartition(-sims, k)[:k]\n    # 按相似度排序\n    topk_idx = topk_idx[np.argsort(-sims[topk_idx])]\n    hits = [contexts[i] for i in topk_idx if i < len(contexts)]\n    return hits\n\ndef generate_answer(question, contexts, tokenizer, model, max_new_tokens=64):\n    context_text = \"\\n\".join(chunk_and_truncate_contexts(contexts))\n    prompt = f\"Question: {question}\\nContext:\\n{context_text}\\nAnswer (if no answer, respond with 'no answer'):\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n    device = next(model.parameters()).device\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        num_beams=4,\n        no_repeat_ngram_size=3,\n        early_stopping=True\n    )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):]\n    answer = answer.strip()\n    if answer.lower() in [\"false\", \"no answer\", \"none\", \"\"]:\n        answer = \"\"\n    return answer\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_file = '/kaggle/input/squad-train/train-v2.0.json'\n    dev_file = '/kaggle/input/squad-dev/dev-v2.0.json'\n\n    sentence_transformer_name = 'sentence-transformers/all-mpnet-base-v2'\n    model_name = \"google/flan-t5-xl\"\n    batch_size_embed = 256\n\n    print(\"加载训练数据...\")\n    train_data = load_squad_dataset(train_file)\n    print(\"加载验证数据...\")\n    dev_data = load_squad_dataset(dev_file)\n\n    # 提取所有训练上下文（遍历每篇文章每个段落）\n    contexts = []\n    for article in train_data:\n        for para in article[\"paragraphs\"]:\n            contexts.append(para[\"context\"])\n    print(f\"训练集上下文数量: {len(contexts)}\")\n\n    print(\"加载句向量模型...\")\n    embedder = SentenceTransformer(sentence_transformer_name, device=device)\n\n    print(\"GPU上计算所有上下文的向量表示...\")\n    all_ctx_emb = build_context_embeddings(contexts, embedder, batch_size=batch_size_embed)\n\n    print(f\"加载生成模型和tokenizer: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n    model.eval()\n\n    preds = {}\n    max_test = 500\n    count = 0\n\n    print(f\"开始对验证集前{max_test}个样本生成回答...\")\n    for article in dev_data:\n        for para in article[\"paragraphs\"]:\n            for qa in para[\"qas\"]:\n                if count >= max_test:\n                    break\n                qid = qa['id']\n                question = qa['question']\n\n                retrieved = retrieve_top_k(question, all_ctx_emb, embedder, contexts, k=5)\n                answer = generate_answer(question, retrieved, tokenizer, model)\n\n                preds[qid] = answer\n\n                print(count)\n                count += 1\n            if count >= max_test:\n                break\n        if count >= max_test:\n            break\n\n    # 构造只包含测试用的dev_data子集\n    filtered_dev_data = []\n    for article in dev_data:\n        new_article = {\"paragraphs\": []}\n        for para in article[\"paragraphs\"]:\n            new_para = {\"qas\": [], \"context\": para[\"context\"]}\n            for qa in para[\"qas\"]:\n                if qa['id'] in preds:\n                    new_para[\"qas\"].append(qa)\n            if new_para[\"qas\"]:\n                new_article[\"paragraphs\"].append(new_para)\n        if new_article[\"paragraphs\"]:\n            filtered_dev_data.append(new_article)\n\n    print(\"计算评测指标...\")\n    exact_scores, f1_scores = get_raw_scores(filtered_dev_data, preds)\n    eval_results = make_eval_dict(exact_scores, f1_scores)\n\n    print(\"评测结果：\")\n    for k, v in eval_results.items():\n        print(f\"{k}: {v:.2f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T07:32:05.120155Z","iopub.execute_input":"2025-08-10T07:32:05.120488Z","iopub.status.idle":"2025-08-10T07:49:34.272580Z","shell.execute_reply.started":"2025-08-10T07:32:05.120465Z","shell.execute_reply":"2025-08-10T07:49:34.271870Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"加载训练数据...\n加载验证数据...\n训练集上下文数量: 19035\n加载句向量模型...\nGPU上计算所有上下文的向量表示...\n","output_type":"stream"},{"name":"stderr","text":"Embedding contexts: 100%|██████████| 75/75 [04:39<00:00,  3.72s/it]\n","output_type":"stream"},{"name":"stdout","text":"加载生成模型和tokenizer: google/flan-t5-xl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bed4fd4af96547778c2be5947ab6575a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37fecadbc2f542d98094bbf954855d4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff650a649ce6417a913f3ffc68421a7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"410bc8fed03f4140b154f49a7d36ce81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9db1854fa0ea4b5ebb6432bf0e5bdda5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"976c962923e544c4a586890fe8a51cca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af01895dc1994bd19a668757ad7e1a62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1386c88c2824bc0bd75f2629f039825"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95cbaf3a35d6403e94a8fb73dff87217"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f831f70d36184c3d9e748c8c40b847ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af866954263f4808b2a04c1901487266"}},"metadata":{}},{"name":"stdout","text":"开始对验证集前500个样本生成回答...\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n计算评测指标...\n评测结果：\nexact: 2.80\nf1: 5.54\ntotal: 500.00\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## 使用hugging face多卡并行并尝试调节生成模型端参数\n多卡失败，尝试单卡调参","metadata":{}},{"cell_type":"code","source":"import json\nimport collections\nimport re\nimport string\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\nfrom accelerate import Accelerator\n\n# ------------------ SQuAD 评测函数 ------------------\n\ndef normalize_answer(s):\n    if s is None:\n        return \"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    return 2 * precision * recall / (precision + recall)\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                qid = qa['id']\n                gold_answers = [a['text'] for a in qa.get('answers', []) if normalize_answer(a.get('text', ''))]\n                if not gold_answers:\n                    gold_answers = ['']  # no answer questions\n                a_pred = preds.get(qid, \"\")\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\ndef make_eval_dict(exact_scores, f1_scores):\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        ('exact', 100.0 * sum(exact_scores.values()) / total if total else 0),\n        ('f1', 100.0 * sum(f1_scores.values()) / total if total else 0),\n        ('total', total),\n    ])\n\n# ------------------ 数据加载与上下文处理 ------------------\n\ndef load_squad_dataset(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data_json = json.load(f)\n    return data_json['data']\n\ndef chunk_and_truncate_contexts(contexts, max_chars=3000):\n    out = []\n    for c in contexts:\n        if len(c) <= max_chars:\n            out.append(c)\n        else:\n            out.append(c[:max_chars] + \" ...\")\n    return out\n\ndef build_context_embeddings(contexts, embedder, batch_size=256):\n    embs = []\n    for i in tqdm(range(0, len(contexts), batch_size), desc=\"Embedding contexts\"):\n        batch = contexts[i:i+batch_size]\n        batch_emb = embedder.encode(batch, convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n        batch_emb = np.asarray(batch_emb, dtype=np.float32)\n        embs.append(batch_emb)\n    return np.vstack(embs)\n\ndef retrieve_top_k(question, all_ctx_emb, embedder, contexts, k=20):\n    q_emb = embedder.encode([question], convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n    q_emb = np.asarray(q_emb, dtype=np.float32)\n    sims = np.dot(all_ctx_emb, q_emb.T).squeeze(1)  # shape: (num_contexts,)\n    topk_idx = np.argpartition(-sims, k)[:k]\n    topk_idx = topk_idx[np.argsort(-sims[topk_idx])]\n    hits = [contexts[i] for i in topk_idx if i < len(contexts)]\n    return hits\n\n# ------------------ 生成答案 ------------------\n\ndef get_first_device_from_device_map(device_map):\n    # 先找所有设备索引（无论单个数字还是列表）\n    devices = []\n    for v in device_map.values():\n        if isinstance(v, list):\n            devices.extend(v)\n        else:\n            devices.append(v)\n    # 取最小设备号（通常是第一个GPU）\n    first_device_idx = min(devices)\n    return torch.device(f'cuda:{first_device_idx}')\n\ndef generate_answer(question, contexts, tokenizer, model, accelerator, gen_params):\n    context_text = \"\\n\".join(chunk_and_truncate_contexts(contexts))\n    prompt = f\"Question: {question}\\nContext:\\n{context_text}\\nAnswer (if no answer, respond with 'no answer'):\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n\n    if hasattr(model, 'device_map') and model.device_map:\n        device_for_input = get_first_device_from_device_map(model.device_map)\n    else:\n        device_for_input = accelerator.device\n\n    inputs = {k: v.to(device_for_input) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        with accelerator.autocast():\n            outputs = model.generate(\n                **inputs,\n                max_new_tokens=gen_params.get(\"max_new_tokens\", 80),\n                do_sample=gen_params.get(\"do_sample\", False),\n                num_beams=gen_params.get(\"num_beams\", 6),\n                no_repeat_ngram_size=gen_params.get(\"no_repeat_ngram_size\", 3),\n                early_stopping=gen_params.get(\"early_stopping\", True),\n                temperature=gen_params.get(\"temperature\", 1.0),\n                top_k=gen_params.get(\"top_k\", None),\n                top_p=gen_params.get(\"top_p\", None),\n            )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):]\n    answer = answer.strip()\n    if answer.lower() in [\"false\", \"no answer\", \"none\", \"\"]:\n        answer = \"\"\n    return answer\n\n\n\n\n# ------------------ 主流程 ------------------\n\ndef main():\n    accelerator = Accelerator(mixed_precision='fp16')\n\n    train_file = '/kaggle/input/squad-train/train-v2.0.json'\n    dev_file = '/kaggle/input/squad-dev/dev-v2.0.json'\n\n    sentence_transformer_name = 'sentence-transformers/all-mpnet-base-v2'\n    model_name = \"google/flan-t5-xl\"\n    batch_size_embed = 256\n\n    print(\"加载训练数据...\")\n    train_data = load_squad_dataset(train_file)\n    print(\"加载验证数据...\")\n    dev_data = load_squad_dataset(dev_file)\n\n    contexts = []\n    for article in train_data:\n        for para in article[\"paragraphs\"]:\n            contexts.append(para[\"context\"])\n    print(f\"训练集上下文数量: {len(contexts)}\")\n\n    print(\"加载句向量模型（GPU）...\")\n    embedder = SentenceTransformer(sentence_transformer_name, device='cuda')\n\n    print(\"计算所有上下文向量（GPU）...\")\n    all_ctx_emb = build_context_embeddings(contexts, embedder, batch_size=batch_size_embed)\n\n    print(f\"加载生成模型和tokenizer: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n    n_gpus = torch.cuda.device_count()\n    print(f\"检测到 {n_gpus} 张GPU\")\n    if n_gpus >= 2:\n        print(\"启用模型并行，将模型拆分到多张GPU\")\n        model.parallelize()\n    else:\n        print(\"单卡模式，将模型加载到CUDA设备\")\n        model = model.to('cuda')\n\n    model, tokenizer = accelerator.prepare(model, tokenizer)\n    model.eval()\n\n    preds = {}\n    max_test = 500\n    count = 0\n\n    gen_params = {\n        \"max_new_tokens\": 80,\n        \"num_beams\": 6,\n        \"do_sample\": False,\n        \"no_repeat_ngram_size\": 3,\n        \"early_stopping\": True,\n        \"temperature\": 1.0,\n        \"top_k\": None,\n        \"top_p\": None,\n    }\n\n    print(f\"开始对验证集前{max_test}个样本生成回答...\")\n    for article in dev_data:\n        for para in article[\"paragraphs\"]:\n            for qa in para[\"qas\"]:\n                if count >= max_test:\n                    break\n                qid = qa['id']\n                question = qa['question']\n\n                retrieved = retrieve_top_k(question, all_ctx_emb, embedder, contexts, k=20)\n                answer = generate_answer(question, retrieved, tokenizer, model, accelerator, gen_params)\n\n                preds[qid] = answer\n\n                if count % 20 == 0:\n                    print(f\"已处理 {count} 个问题\")\n                count += 1\n            if count >= max_test:\n                break\n        if count >= max_test:\n            break\n\n    filtered_dev_data = []\n    for article in dev_data:\n        new_article = {\"paragraphs\": []}\n        for para in article['paragraphs']:\n            new_para = {\"qas\": [], \"context\": para[\"context\"]}\n            for qa in para[\"qas\"]:\n                if qa['id'] in preds:\n                    new_para[\"qas\"].append(qa)\n            if new_para[\"qas\"]:\n                new_article[\"paragraphs\"].append(new_para)\n        if new_article[\"paragraphs\"]:\n            filtered_dev_data.append(new_article)\n\n    print(\"计算评测指标...\")\n    exact_scores, f1_scores = get_raw_scores(filtered_dev_data, preds)\n    eval_results = make_eval_dict(exact_scores, f1_scores)\n\n    print(\"评测结果：\")\n    for k, v in eval_results.items():\n        print(f\"{k}: {v:.2f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 单卡调参","metadata":{}},{"cell_type":"code","source":"import json\nimport collections\nimport re\nimport string\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\n# ------------------ SQuAD 评测函数 ------------------\n\ndef normalize_answer(s):\n    if s is None:\n        return \"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    return 2 * precision * recall / (precision + recall)\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                qid = qa['id']\n                gold_answers = [a['text'] for a in qa.get('answers', []) if normalize_answer(a.get('text', ''))]\n                if not gold_answers:\n                    gold_answers = ['']  # no answer questions\n                a_pred = preds.get(qid, \"\")\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\ndef make_eval_dict(exact_scores, f1_scores):\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        ('exact', 100.0 * sum(exact_scores.values()) / total if total else 0),\n        ('f1', 100.0 * sum(f1_scores.values()) / total if total else 0),\n        ('total', total),\n    ])\n\n# ------------------ 数据加载与上下文处理 ------------------\n\ndef load_squad_dataset(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data_json = json.load(f)\n    return data_json['data']\n\ndef chunk_and_truncate_contexts(contexts, max_chars=3000):\n    out = []\n    for c in contexts:\n        if len(c) <= max_chars:\n            out.append(c)\n        else:\n            out.append(c[:max_chars] + \" ...\")\n    return out\n\ndef build_context_embeddings(contexts, embedder, batch_size=128):\n    embs = []\n    for i in tqdm(range(0, len(contexts), batch_size), desc=\"Embedding contexts\"):\n        batch = contexts[i:i+batch_size]\n        batch_emb = embedder.encode(batch, convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n        batch_emb = np.asarray(batch_emb, dtype=np.float32)\n        embs.append(batch_emb)\n    return np.vstack(embs)\n\ndef retrieve_top_k(question, all_ctx_emb, embedder, contexts, k=10):\n    q_emb = embedder.encode([question], convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n    q_emb = np.asarray(q_emb, dtype=np.float32)\n    sims = np.dot(all_ctx_emb, q_emb.T).squeeze(1)  # shape: (num_contexts,)\n    topk_idx = np.argpartition(-sims, k)[:k]\n    topk_idx = topk_idx[np.argsort(-sims[topk_idx])]\n    hits = [contexts[i] for i in topk_idx if i < len(contexts)]\n    return hits\n\n# ------------------ 生成答案 ------------------\n\ndef generate_answer(question, contexts, tokenizer, model, gen_params):\n    context_text = \"\\n\".join(chunk_and_truncate_contexts(contexts))\n    prompt = f\"Question: {question}\\nContext:\\n{context_text}\\nAnswer (if no answer, respond with 'no answer'):\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to('cuda')\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=gen_params.get(\"max_new_tokens\", 40),\n            do_sample=gen_params.get(\"do_sample\", False),\n            num_beams=gen_params.get(\"num_beams\", 2),\n            no_repeat_ngram_size=gen_params.get(\"no_repeat_ngram_size\", 3),\n            early_stopping=gen_params.get(\"early_stopping\", True),\n        )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):]\n    answer = answer.strip()\n    if answer.lower() in [\"false\", \"no answer\", \"none\", \"\"]:\n        answer = \"\"\n    return answer\n\n# ------------------ 主流程 ------------------\n\ndef main():\n    train_file = '/kaggle/input/squad-train/train-v2.0.json'\n    dev_file = '/kaggle/input/squad-dev/dev-v2.0.json'\n\n    sentence_transformer_name = 'sentence-transformers/all-mpnet-base-v2'\n    model_name = \"google/flan-t5-xl\"\n\n    batch_size_embed = 128\n    max_test = 500\n\n    print(\"加载训练数据...\")\n    train_data = load_squad_dataset(train_file)\n    print(\"加载验证数据...\")\n    dev_data = load_squad_dataset(dev_file)\n\n    contexts = []\n    for article in train_data:\n        for para in article[\"paragraphs\"]:\n            contexts.append(para[\"context\"])\n    print(f\"训练集上下文数量: {len(contexts)}\")\n\n    print(\"加载句向量模型（GPU）...\")\n    embedder = SentenceTransformer(sentence_transformer_name, device='cuda')\n\n    print(\"计算所有上下文向量（GPU）...\")\n    all_ctx_emb = build_context_embeddings(contexts, embedder, batch_size=batch_size_embed)\n\n    print(f\"加载生成模型和tokenizer: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')\n\n    model.eval()\n    preds = {}\n\n    gen_params = {\n        \"max_new_tokens\": 40,\n        \"num_beams\": 6,\n        \"do_sample\": False,\n        \"no_repeat_ngram_size\": 3,\n        \"early_stopping\": True,\n    }\n\n    print(f\"开始对验证集前{max_test}个样本生成回答...\")\n    count = 0\n    for article in dev_data:\n        for para in article[\"paragraphs\"]:\n            for qa in para[\"qas\"]:\n                if count >= max_test:\n                    break\n                qid = qa['id']\n                question = qa['question']\n\n                retrieved = retrieve_top_k(question, all_ctx_emb, embedder, contexts, k=10)\n                answer = generate_answer(question, retrieved, tokenizer, model, gen_params)\n\n                preds[qid] = answer\n\n                if count % 10 == 0:\n                    print(f\"已处理 {count} 个问题\")\n                count += 1\n            if count >= max_test:\n                break\n        if count >= max_test:\n            break\n\n    filtered_dev_data = []\n    for article in dev_data:\n        new_article = {\"paragraphs\": []}\n        for para in article['paragraphs']:\n            new_para = {\"qas\": [], \"context\": para[\"context\"]}\n            for qa in para[\"qas\"]:\n                if qa['id'] in preds:\n                    new_para[\"qas\"].append(qa)\n            if new_para[\"qas\"]:\n                new_article[\"paragraphs\"].append(new_para)\n        if new_article[\"paragraphs\"]:\n            filtered_dev_data.append(new_article)\n\n    print(\"计算评测指标...\")\n    exact_scores, f1_scores = get_raw_scores(filtered_dev_data, preds)\n    eval_results = make_eval_dict(exact_scores, f1_scores)\n\n    print(\"评测结果：\")\n    for k, v in eval_results.items():\n        print(f\"{k}: {v:.2f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T08:49:03.364854Z","iopub.execute_input":"2025-08-10T08:49:03.365135Z","iopub.status.idle":"2025-08-10T09:05:48.856339Z","shell.execute_reply.started":"2025-08-10T08:49:03.365111Z","shell.execute_reply":"2025-08-10T09:05:48.855576Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"加载训练数据...\n加载验证数据...\n训练集上下文数量: 19035\n加载句向量模型（GPU）...\n计算所有上下文向量（GPU）...\n","output_type":"stream"},{"name":"stderr","text":"Embedding contexts: 100%|██████████| 149/149 [04:59<00:00,  2.01s/it]\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"加载生成模型和tokenizer: google/flan-t5-xl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"383477aa2eca43c5a0ad7200e0c5b989"}},"metadata":{}},{"name":"stdout","text":"开始对验证集前500个样本生成回答...\n已处理 0 个问题\n已处理 10 个问题\n已处理 20 个问题\n已处理 30 个问题\n已处理 40 个问题\n已处理 50 个问题\n已处理 60 个问题\n已处理 70 个问题\n已处理 80 个问题\n已处理 90 个问题\n已处理 100 个问题\n已处理 110 个问题\n已处理 120 个问题\n已处理 130 个问题\n已处理 140 个问题\n已处理 150 个问题\n已处理 160 个问题\n已处理 170 个问题\n已处理 180 个问题\n已处理 190 个问题\n已处理 200 个问题\n已处理 210 个问题\n已处理 220 个问题\n已处理 230 个问题\n已处理 240 个问题\n已处理 250 个问题\n已处理 260 个问题\n已处理 270 个问题\n已处理 280 个问题\n已处理 290 个问题\n已处理 300 个问题\n已处理 310 个问题\n已处理 320 个问题\n已处理 330 个问题\n已处理 340 个问题\n已处理 350 个问题\n已处理 360 个问题\n已处理 370 个问题\n已处理 380 个问题\n已处理 390 个问题\n已处理 400 个问题\n已处理 410 个问题\n已处理 420 个问题\n已处理 430 个问题\n已处理 440 个问题\n已处理 450 个问题\n已处理 460 个问题\n已处理 470 个问题\n已处理 480 个问题\n已处理 490 个问题\n计算评测指标...\n评测结果：\nexact: 1.60\nf1: 4.41\ntotal: 500.00\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"第一次调参效果变差，\n考虑max_new_tokens 太短了\n40个token可能不足以生成完整答案，尤其复杂问题。缩短生成长度容易导致答案不完整。\n\n上下文检索k增加，带入无关或噪声\n检索更多上下文，理论上有更多信息，但也可能带入不相关内容，干扰模型生成。\n\nnum_beams 增大未必提升\n增大beam宽度能探索更多可能，但如果输入上下文信息有噪声，beam越大可能越倾向于产生受干扰的答案。\n\n模型对上下文的长度敏感\n如果输入prompt超长，模型截断导致有用信息丢失，也会影响结果。","metadata":{}},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"markdown","source":"## 第二次调参","metadata":{}},{"cell_type":"markdown","source":"k = 8,num beams = 5 max token = 80,第二次调参","metadata":{}},{"cell_type":"code","source":"import json\nimport collections\nimport re\nimport string\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\n# ------------------ SQuAD 评测函数 ------------------\n\ndef normalize_answer(s):\n    if s is None:\n        return \"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    return 2 * precision * recall / (precision + recall)\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                qid = qa['id']\n                gold_answers = [a['text'] for a in qa.get('answers', []) if normalize_answer(a.get('text', ''))]\n                if not gold_answers:\n                    gold_answers = ['']  # no answer questions\n                a_pred = preds.get(qid, \"\")\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\ndef make_eval_dict(exact_scores, f1_scores):\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        ('exact', 100.0 * sum(exact_scores.values()) / total if total else 0),\n        ('f1', 100.0 * sum(f1_scores.values()) / total if total else 0),\n        ('total', total),\n    ])\n\n# ------------------ 数据加载与上下文处理 ------------------\n\ndef load_squad_dataset(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data_json = json.load(f)\n    return data_json['data']\n\ndef chunk_and_truncate_contexts(contexts, max_chars=3000):\n    out = []\n    for c in contexts:\n        if len(c) <= max_chars:\n            out.append(c)\n        else:\n            out.append(c[:max_chars] + \" ...\")\n    return out\n\ndef build_context_embeddings(contexts, embedder, batch_size=128):\n    embs = []\n    for i in tqdm(range(0, len(contexts), batch_size), desc=\"Embedding contexts\"):\n        batch = contexts[i:i+batch_size]\n        batch_emb = embedder.encode(batch, convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n        batch_emb = np.asarray(batch_emb, dtype=np.float32)\n        embs.append(batch_emb)\n    return np.vstack(embs)\n\ndef retrieve_top_k(question, all_ctx_emb, embedder, contexts, k=8):\n    q_emb = embedder.encode([question], convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n    q_emb = np.asarray(q_emb, dtype=np.float32)\n    sims = np.dot(all_ctx_emb, q_emb.T).squeeze(1)  # shape: (num_contexts,)\n    topk_idx = np.argpartition(-sims, k)[:k]\n    topk_idx = topk_idx[np.argsort(-sims[topk_idx])]\n    hits = [contexts[i] for i in topk_idx if i < len(contexts)]\n    return hits\n\n# ------------------ 生成答案 ------------------\n\ndef generate_answer(question, contexts, tokenizer, model, gen_params):\n    context_text = \"\\n\".join(chunk_and_truncate_contexts(contexts))\n    prompt = f\"Question: {question}\\nContext:\\n{context_text}\\nAnswer (if no answer, respond with 'no answer'):\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to('cuda')\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=gen_params.get(\"max_new_tokens\", 40),\n            do_sample=gen_params.get(\"do_sample\", False),\n            num_beams=gen_params.get(\"num_beams\", 2),\n            no_repeat_ngram_size=gen_params.get(\"no_repeat_ngram_size\", 3),\n            early_stopping=gen_params.get(\"early_stopping\", True),\n        )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):]\n    answer = answer.strip()\n    if answer.lower() in [\"false\", \"no answer\", \"none\", \"\"]:\n        answer = \"\"\n    return answer\n\n# ------------------ 主流程 ------------------\n\ndef main():\n    train_file = '/kaggle/input/squad-train/train-v2.0.json'\n    dev_file = '/kaggle/input/squad-dev/dev-v2.0.json'\n\n    sentence_transformer_name = 'sentence-transformers/all-mpnet-base-v2'\n    model_name = \"google/flan-t5-xl\"\n\n    batch_size_embed = 128\n    max_test = 500\n\n    print(\"加载训练数据...\")\n    train_data = load_squad_dataset(train_file)\n    print(\"加载验证数据...\")\n    dev_data = load_squad_dataset(dev_file)\n\n    contexts = []\n    for article in train_data:\n        for para in article[\"paragraphs\"]:\n            contexts.append(para[\"context\"])\n    print(f\"训练集上下文数量: {len(contexts)}\")\n\n    print(\"加载句向量模型（GPU）...\")\n    embedder = SentenceTransformer(sentence_transformer_name, device='cuda')\n\n    print(\"计算所有上下文向量（GPU）...\")\n    all_ctx_emb = build_context_embeddings(contexts, embedder, batch_size=batch_size_embed)\n\n    print(f\"加载生成模型和tokenizer: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('cuda')\n\n    model.eval()\n    preds = {}\n\n    gen_params = {\n        \"max_new_tokens\": 80,\n        \"num_beams\": 5,\n        \"do_sample\": False,\n        \"no_repeat_ngram_size\": 3,\n        \"early_stopping\": True,\n    }\n\n    print(f\"开始对验证集前{max_test}个样本生成回答...\")\n    count = 0\n    for article in dev_data:\n        for para in article[\"paragraphs\"]:\n            for qa in para[\"qas\"]:\n                if count >= max_test:\n                    break\n                qid = qa['id']\n                question = qa['question']\n\n                retrieved = retrieve_top_k(question, all_ctx_emb, embedder, contexts, k=10)\n                answer = generate_answer(question, retrieved, tokenizer, model, gen_params)\n\n                preds[qid] = answer\n\n                if count % 10 == 0:\n                    print(f\"已处理 {count} 个问题\")\n                count += 1\n            if count >= max_test:\n                break\n        if count >= max_test:\n            break\n\n    filtered_dev_data = []\n    for article in dev_data:\n        new_article = {\"paragraphs\": []}\n        for para in article['paragraphs']:\n            new_para = {\"qas\": [], \"context\": para[\"context\"]}\n            for qa in para[\"qas\"]:\n                if qa['id'] in preds:\n                    new_para[\"qas\"].append(qa)\n            if new_para[\"qas\"]:\n                new_article[\"paragraphs\"].append(new_para)\n        if new_article[\"paragraphs\"]:\n            filtered_dev_data.append(new_article)\n\n    print(\"计算评测指标...\")\n    exact_scores, f1_scores = get_raw_scores(filtered_dev_data, preds)\n    eval_results = make_eval_dict(exact_scores, f1_scores)\n\n    print(\"评测结果：\")\n    for k, v in eval_results.items():\n        print(f\"{k}: {v:.2f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-08-10T09:28:44.501137Z","shell.execute_reply.started":"2025-08-10T09:12:43.249639Z","shell.execute_reply":"2025-08-10T09:28:44.500218Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"加载训练数据...\n加载验证数据...\n训练集上下文数量: 19035\n加载句向量模型（GPU）...\n计算所有上下文向量（GPU）...\n","output_type":"stream"},{"name":"stderr","text":"Embedding contexts: 100%|██████████| 149/149 [05:00<00:00,  2.02s/it]\n/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"加载生成模型和tokenizer: google/flan-t5-xl\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"339a1059f8674db19970e1fbedef9ff2"}},"metadata":{}},{"name":"stdout","text":"开始对验证集前500个样本生成回答...\n已处理 0 个问题\n已处理 10 个问题\n已处理 20 个问题\n已处理 30 个问题\n已处理 40 个问题\n已处理 50 个问题\n已处理 60 个问题\n已处理 70 个问题\n已处理 80 个问题\n已处理 90 个问题\n已处理 100 个问题\n已处理 110 个问题\n已处理 120 个问题\n已处理 130 个问题\n已处理 140 个问题\n已处理 150 个问题\n已处理 160 个问题\n已处理 170 个问题\n已处理 180 个问题\n已处理 190 个问题\n已处理 200 个问题\n已处理 210 个问题\n已处理 220 个问题\n已处理 230 个问题\n已处理 240 个问题\n已处理 250 个问题\n已处理 260 个问题\n已处理 270 个问题\n已处理 280 个问题\n已处理 290 个问题\n已处理 300 个问题\n已处理 310 个问题\n已处理 320 个问题\n已处理 330 个问题\n已处理 340 个问题\n已处理 350 个问题\n已处理 360 个问题\n已处理 370 个问题\n已处理 380 个问题\n已处理 390 个问题\n已处理 400 个问题\n已处理 410 个问题\n已处理 420 个问题\n已处理 430 个问题\n已处理 440 个问题\n已处理 450 个问题\n已处理 460 个问题\n已处理 470 个问题\n已处理 480 个问题\n已处理 490 个问题\n计算评测指标...\n评测结果：\nexact: 1.40\nf1: 4.08\ntotal: 500.00\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"xl加all mpnet base v2应该是最佳配置，在有限资源下","metadata":{}},{"cell_type":"code","source":"!pip install rank_bm25\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T05:51:51.328591Z","iopub.execute_input":"2025-08-11T05:51:51.328994Z","iopub.status.idle":"2025-08-11T05:51:54.842676Z","shell.execute_reply.started":"2025-08-11T05:51:51.328959Z","shell.execute_reply":"2025-08-11T05:51:54.841964Z"}},"outputs":[{"name":"stdout","text":"Collecting rank_bm25\n  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rank_bm25) (1.26.4)\nDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\nInstalling collected packages: rank_bm25\nSuccessfully installed rank_bm25-0.2.2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"##  BM25经典稀疏检索算法","metadata":{}},{"cell_type":"code","source":"import json\nimport collections\nimport re\nimport string\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\nfrom rank_bm25 import BM25Okapi\n\n# ----------- 评测函数（与你原来相同） ---------------\n\ndef normalize_answer(s):\n    if s is None:\n        return \"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    return 2 * precision * recall / (precision + recall)\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                qid = qa['id']\n                gold_answers = [a['text'] for a in qa.get('answers', []) if normalize_answer(a.get('text', ''))]\n                if not gold_answers:\n                    gold_answers = ['']  # no answer questions\n                a_pred = preds.get(qid, \"\")\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\ndef make_eval_dict(exact_scores, f1_scores):\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        ('exact', 100.0 * sum(exact_scores.values()) / total if total else 0),\n        ('f1', 100.0 * sum(f1_scores.values()) / total if total else 0),\n        ('total', total),\n    ])\n\n# ----------- 工具函数 -----------\n\ndef load_squad_dataset(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data_json = json.load(f)\n    return data_json['data']\n\ndef tokenize_for_bm25(text):\n    # 简单英文分词，BM25用\n    return normalize_answer(text).split()\n\ndef preprocess_context(context, max_chars=1000):\n    # 截断过长文本，防止显存爆炸\n    return context if len(context) <= max_chars else context[:max_chars] + \" ...\"\n\n# ----------- BM25构建 -----------\n\ndef build_bm25_index(contexts):\n    tokenized_corpus = [tokenize_for_bm25(c) for c in contexts]\n    bm25 = BM25Okapi(tokenized_corpus)\n    return bm25, tokenized_corpus\n\n# ----------- 向量检索 -----------\n\ndef embed_texts(texts, embedder, batch_size=16, device='cuda'):\n    all_embs = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n        batch_emb = embedder.encode(batch, convert_to_numpy=True, device=device, normalize_embeddings=True, show_progress_bar=False)\n        all_embs.append(batch_emb)\n    return np.vstack(all_embs)\n\ndef retrieve_top_k(question, bm25, tokenized_corpus, contexts, embedder, vector_topk=10, bm25_topk=50, device='cuda'):\n    # 1. BM25检索前bm25_topk条粗排\n    q_tokens = tokenize_for_bm25(question)\n    bm25_scores = bm25.get_scores(q_tokens)\n    bm25_top_indices = np.argpartition(-bm25_scores, bm25_topk)[:bm25_topk]\n    bm25_top_indices = bm25_top_indices[np.argsort(-bm25_scores[bm25_top_indices])]\n\n    candidate_contexts = [contexts[i] for i in bm25_top_indices]\n    candidate_tokenized = [tokenized_corpus[i] for i in bm25_top_indices]\n\n    # 2. 向量编码 & 余弦相似度重新排序\n    # 先对候选文本做长度截断\n    candidate_contexts_trunc = [preprocess_context(c, max_chars=1000) for c in candidate_contexts]\n\n    candidate_embs = embed_texts(candidate_contexts_trunc, embedder, batch_size=16, device=device)\n    q_emb = embedder.encode([question], convert_to_numpy=True, device=device, normalize_embeddings=True)[0]\n\n    sims = np.dot(candidate_embs, q_emb)\n    topk_idx = np.argpartition(-sims, vector_topk)[:vector_topk]\n    topk_idx = topk_idx[np.argsort(-sims[topk_idx])]\n\n    # 返回最终topk文本\n    results = [candidate_contexts_trunc[i] for i in topk_idx]\n    return results\n\n# ----------- 生成回答 -----------\n\ndef generate_answer(question, contexts, tokenizer, model, max_new_tokens=64):\n    context_text = \"\\n\".join(contexts)\n    prompt = f\"Question: {question}\\nContext:\\n{context_text}\\nAnswer (if no answer, respond with 'no answer'):\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n    device = next(model.parameters()).device\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        num_beams=4,\n        no_repeat_ngram_size=3,\n        early_stopping=True\n    )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):]\n    answer = answer.strip()\n    if answer.lower() in [\"false\", \"no answer\", \"none\", \"\"]:\n        answer = \"\"\n    return answer\n\n# ----------- 主函数 -----------\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_file = '/kaggle/input/squad-train/train-v2.0.json'\n    dev_file = '/kaggle/input/squad-dev/dev-v2.0.json'\n\n    sentence_transformer_name = 'sentence-transformers/all-mpnet-base-v2'\n    model_name = \"google/flan-t5-xl\"\n\n    print(\"加载训练数据...\")\n    train_data = load_squad_dataset(train_file)\n    print(\"加载验证数据...\")\n    dev_data = load_squad_dataset(dev_file)\n\n    # 提取所有训练上下文\n    contexts = []\n    for article in train_data:\n        for para in article[\"paragraphs\"]:\n            contexts.append(para[\"context\"])\n    print(f\"训练集上下文数量: {len(contexts)}\")\n\n    print(\"加载句向量模型...\")\n    embedder = SentenceTransformer(sentence_transformer_name, device=device)\n\n    print(\"构建BM25索引...\")\n    bm25, tokenized_corpus = build_bm25_index(contexts)\n\n    print(f\"加载生成模型和tokenizer: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n    model.eval()\n\n    preds = {}\n    max_test = 500\n    count = 0\n\n    print(f\"开始对验证集前{max_test}个样本生成回答...\")\n\n    for article in dev_data:\n        for para in article[\"paragraphs\"]:\n            for qa in para[\"qas\"]:\n                if count >= max_test:\n                    break\n                qid = qa['id']\n                question = qa['question']\n\n                retrieved = retrieve_top_k(question, bm25, tokenized_corpus, contexts, embedder,\n                                         vector_topk=5, bm25_topk=50, device=device)\n                answer = generate_answer(question, retrieved, tokenizer, model)\n\n                preds[qid] = answer\n\n                print(f\"已处理样本数: {count+1}\")\n                count += 1\n            if count >= max_test:\n                break\n        if count >= max_test:\n            break\n\n    # 构造只包含测试用的dev_data子集\n    filtered_dev_data = []\n    for article in dev_data:\n        new_article = {\"paragraphs\": []}\n        for para in article[\"paragraphs\"]:\n            new_para = {\"qas\": [], \"context\": para[\"context\"]}\n            for qa in para[\"qas\"]:\n                if qa['id'] in preds:\n                    new_para[\"qas\"].append(qa)\n            if new_para[\"qas\"]:\n                new_article[\"paragraphs\"].append(new_para)\n        if new_article[\"paragraphs\"]:\n            filtered_dev_data.append(new_article)\n\n    print(\"计算评测指标...\")\n    exact_scores, f1_scores = get_raw_scores(filtered_dev_data, preds)\n    eval_results = make_eval_dict(exact_scores, f1_scores)\n\n    print(\"评测结果：\")\n    for k, v in eval_results.items():\n        print(f\"{k}: {v:.2f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T16:17:07.991297Z","iopub.execute_input":"2025-08-10T16:17:07.991792Z","iopub.status.idle":"2025-08-10T16:34:01.290255Z","shell.execute_reply.started":"2025-08-10T16:17:07.991765Z","shell.execute_reply":"2025-08-10T16:34:01.289358Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"加载训练数据...\n加载验证数据...\n训练集上下文数量: 19035\n加载句向量模型...\n构建BM25索引...\n加载生成模型和tokenizer: google/flan-t5-xl\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c883ba9b395849be9cd16d1563b4776a"}},"metadata":{}},{"name":"stdout","text":"开始对验证集前500个样本生成回答...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c909f959d40a4c4dad9bc7fc53dd2118"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6a9b21a830b4f1fa007a3fd96074878"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04822db7f36d4b14bdafea458da67fe0"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f23f437022c444bca3faed0cd71864ea"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddcb864b717e4a12bd2246be17279462"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 5\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdba82efc9b04615aca4c3879c8ec007"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 6\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88a596d1d45c4509878198504b0e149e"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 7\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dfa2ee282364f6c9a3bf87d679409d4"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 8\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2c9eb1dd598433a98d19a4942820939"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 9\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70301ed7ff6a4d14bd7498ff149bf322"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 10\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f42e5509d1a416bae595f35fc011eea"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 11\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad994ac9340c481f9ebcbee9660e6405"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 12\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3dc4ba06e154d4a93a092fb005ef27b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 13\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5db10d4717194e0e9753c2853228e827"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 14\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7685d8c0b4c343aa8d9ccca925e8df0d"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 15\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"576a1757702642c99c4f0d3c4ebc06fe"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 16\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cfc70bf6099494b95756b189bbd97a3"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 17\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e64d001f4ed34b84afdd68af9c9f52a7"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 18\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a94281a07f7e4b14afb1bca858213b9c"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 19\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fced8837dd1144a3affb29cbcf86289f"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 20\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2240bd167796436eba8c2c50a29068bc"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 21\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a806ba1f44904ebdab713adc6c3c4429"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 22\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecf7797f4c5f46f78cbfca7fce5a20da"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 23\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb3a0202941f47988118367fd738462b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 24\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c198ee9bcc9421c82ccb47b0d1560c4"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 25\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"032bb03005054169a5528507cbe2e8ea"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 26\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5b931bac9924530a3fd5298ee4e36fb"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 27\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b8d1fe2416342c9838c76fad6861904"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 28\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9597d1988762434cab991c7b9cd884ac"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 29\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2e909310aa34cfd960428c4a6669750"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 30\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceaba0e96fae44cfbc85e2389aca09b3"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 31\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"008f1bf8e2114e94bd13c570e711a451"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 32\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea44ea1cee224fb097efebcb8e232415"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 33\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab89e879e5e741cba788bed39ac20120"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 34\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9102e2167a54790994c45f3cfb715ec"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 35\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d2331326d654f49aa49f27ddd51032f"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 36\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25720890d9bb49f096e4d85952603fd6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 37\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1628b06fa9a14375a01889f34da666bd"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 38\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7efddf4e736480d8972acd4ca8d3c3c"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 39\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20b9720781c4422cb85d278e02cbc8ff"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 40\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dd193a6db66484b85a33688cf42e4f1"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 41\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93f4acc79c754e0985e1cd0453868d5a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 42\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81ea736c293b40c0a0b133931df3bb3d"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 43\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfc9a51ac01c44aaac20943090a110d6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 44\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbd3dcdf19c84fbaa73aab01bcf5db7f"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 45\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d925db14dd0476db994f82268105c34"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 46\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b84f1b62ff104988b8e8b95d34e012fa"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 47\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e4b78466a2c47dc8142dc4d2e22d56b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 48\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b6e368a9d5e46d9b4cb11f72be833d5"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 49\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"697b6ea2bda34243a76e63f3afd179c5"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 50\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5738fdffba54e7dbafc7e03f5eac14b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 51\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6267aacafda94bed843a0b4a00ecdc1a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 52\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d99db91e4a364d169866f1d9062c965b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 53\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5745811666fe435097a05afcafc2a883"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 54\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"478684d018624ba3a473df80e7474980"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 55\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce511edd45834856b8198e84144af971"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 56\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f49c9e7578244e6b92fa6d65e5b9a0f"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 57\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7810f875078438883d49f6c3b13356c"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 58\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7028d5475653460590cb57d6c610baaf"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 59\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"719ddfd7fe4c4e35b639eb8988b90076"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 60\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fd803ffc0ef4408ae98e8f3ef182a66"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 61\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b956b14a4ec34366a2341a4b5a8e39ac"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 62\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c236bb1827d645719c9a11f2349bddf2"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 63\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc5df71c2d8644d08ea3b8cd32d9332e"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 64\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ddf43443eaf4cdd9841f9bc11dc1b36"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 65\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a4188ae196c46c2a983ff743e8c91f9"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 66\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eabe125e66654126af451b444d4864cc"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 67\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5c92addc3d643f288dbdb8c0e8c7a21"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 68\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ccb2338a99141158860e34f3f71a230"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 69\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0dfe4e5b4094bba8e4c6f796bc37f71"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 70\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f0dc2960c5649b0aa5ab458a81ae649"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 71\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abb90e9623aa48faa44aef1d6894e110"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 72\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f30739dd72d94abca71a87510ccf3ca1"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 73\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f4c5e3009cb41f5bd58a7fad60ee678"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 74\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57ec7b22164c48c78069f76b282ba104"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 75\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c661e9a2aae41c5ad735e743aea897f"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 76\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1bed0a35ed946dfb45d4175db30f07a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 77\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dec954fdf1cf42b480acde4e630b0709"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 78\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28235fa6ddf14b318533dda4976fb562"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 79\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a7e21ee237f4147943e0abd2c074de2"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 80\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80a28c8555be4591892e408cbb9ef30e"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 81\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1177be83f56640bc8ec276c38ea012fe"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 82\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66b10c6fdf684dd49b23f9e151179320"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 83\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e735e13d37142c0b74827bce0d976a1"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 84\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccd3ccb7c6204003993c8f0ce7497ce7"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 85\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b3f7597a2724961a31c6f7b67bfc1c6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 86\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c871cce15054395b8c290f2ea2b9c50"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 87\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aced9ddb267546e997a3dfb026d0286f"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 88\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf15881ace8d4df6a03d1a9d2b6180c2"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 89\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b5488ed3dc243cf9d62f3eb07de215b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 90\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcc1c5603cbf499ba07c293433ebc252"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 91\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23e99567061e4031ba869aad361e68da"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 92\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5710d95cd5c341ee867bb803aa4bc7c4"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 93\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38761e4225584d0692986d152d00f7ef"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 94\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"457344f1ce9044ddac647828983de166"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 95\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e21a15a8101045d88bc3ef49f0436cf0"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 96\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"705d02d7f1e14d8dbc6789e0e78ba891"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 97\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8363c9bfdf2a489ab0d40d7c65e8f567"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 98\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8a1178b173f469995078c2a1b8775e0"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 99\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38d2efc8070a4b05bdef6046618103fd"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 100\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38c259b0568d4802a32f0fd388fed4a2"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 101\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33cbf7d0688248e09b0ca3730930e845"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 102\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6d6674751684cb5a8e5b2cbac2bbf59"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 103\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb20b15999e14d00a1c0392940a03492"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 104\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edd326cd66494897a7fa8be385fd5009"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 105\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"289dd3de2d9b4ed988dc9473b093449f"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 106\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b90c1effb04f49be817a6ef2f9467d7d"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 107\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21e777031ebe403f964817f7d6351ce5"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 108\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"568a7cad94d143ff85acd07136d33d2e"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 109\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8840dbd52a845b484c1077d9854affa"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 110\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a891c8de2e3e4207bfc02e27d8d6a57f"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 111\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f83b4035d79541bab4293289a7fbd1be"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 112\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee3e61ff17bd43be84070eeed04341ad"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 113\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16303b0d279b40bc967c8e7dff12cbeb"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 114\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14ac0621cfd746549bdd72c2a1c89084"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 115\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebf146edb95c4c0baeb849c2c578304d"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 116\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d4eb54bea0e4022b941ac6dac392728"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 117\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e344654518747afbc96127a0a1b0876"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 118\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"378be97bbe9f410394ff755e92ea7cd9"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 119\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78738b316a474c68af27ca03771afe64"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 120\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a67215344cd43df91993d28c2f8e4ed"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 121\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c456331e397f4be3bed450ee70588fcd"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 122\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bd97ed2d33647258359ec4d3cc96b63"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 123\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b90c5521d8a04af19abb6a73ed500687"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 124\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08391af945194b159b9e222609e1f8ec"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 125\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"159a912930144384a98214f15e4415a8"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 126\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48c7bd41cf2f499e95f8a35bee165d70"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 127\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"392796d567764fe2baac811610b71ee6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 128\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a4e6e7c326744c8a51ecc7f510399a4"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 129\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e24304bd5e6423c8b18fca6cafebeee"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 130\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbd6564f843d4502b8ece01cef985b2a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 131\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abe105c9b4af4cd4967275fb6e279d1f"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 132\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"405dd264645449dcac80709e4a34bb12"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 133\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69ccad9c003346d992efb3d107c34fd9"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 134\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64e46b38997c4c4bb48465ce42b8cefb"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 135\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d02624ea54a44bcfaf1465b2f5dacb9c"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 136\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cd7eb1111854d16bec15ea738419aaf"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 137\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3eb3aa281d448bda5044a68947a4a05"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 138\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1dfd2648cf864607979205a5b95a7f8b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 139\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4c937f9eaab4268973d42d273582409"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 140\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e87086a40da042149c7fe6102fc7396a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 141\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b93aadc20e742e8b0811e1e0516e8c2"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 142\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ca36d8d68be46a4902493b87ed172ef"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 143\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9e39e3bda2d43a8a2415139c9eedc8f"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 144\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56ceced93eaf4d7e9ef0f60a55a9785e"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 145\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b897d8227454293af3e359e51395cb8"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 146\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f69c50e3f1444f518d91d74aefcbe447"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 147\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae12525895844233b2f09ad5652742c3"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 148\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2bb5783a2e745efaf0cdd9589b22515"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 149\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a393a661f78f483b89f2a795c82205cf"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 150\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09358319c5d5455d95aeb913555389a5"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 151\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16f3282fe49a403e8eb9d5d61d8335d6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 152\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1850eb38ca0c49b38a4712db7ba6005d"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 153\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b090bb0bad504cd29230cb77d8e60c5e"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 154\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b19a06908f44e29bc19b79bbc9e33fa"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 155\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d689338c39304893956c0e40f57da411"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 156\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a35be7595e2d418f9eecb527a114f492"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 157\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5e243b454d14b47bf10dff4c29cc7eb"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 158\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cefd918ad1d544098f318bfb567d3ea4"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 159\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39c9d70d8719416eadd9a9ec721c6d33"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 160\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9064cae3d744f5192f70622cf23153b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 161\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4e33c708efc469bba88666aaf4d00b4"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 162\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d5cf8c45d3440e788d48ac0a6daa3b5"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 163\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b030e2110cd463f8e7539cba6e559ef"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 164\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82d210bbc3b44da889b20c96f9e114da"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 165\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a09017bf0804166a418edcc58f9c7a6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 166\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c65622dc5504f0a878664d71665a4ac"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 167\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"634ce424e7ef45a89ce74ea542134329"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 168\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ab6c0527ea44a08ada2fb60ea494df1"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 169\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b447cd964114effb179299b87739b70"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 170\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e2670a498e24ce893fdeab765d6e0d7"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 171\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ddaef373fef4b128c738375edc4dd08"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 172\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"393707e19e4848b0b3515391b86edf14"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 173\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"496da495b174481f931ebc55c170a041"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 174\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fa5d6e2edbe473980546f7252121dc4"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 175\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c201fa4249c46e39e054a48a78e8ff2"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 176\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96f8975578a746aa91e0d68bd1e4ede0"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 177\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1798a755d92840bcb3b0e1eb60a597b7"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 178\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29b57eb5a91744708bfc5c41fdce8972"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 179\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9532c0a296844250a363dd2ad08d943e"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 180\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3eaf7ec98baf4e2cbfffe2d4ca852e07"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 181\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a43855fd75943ff9df6765325373f86"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 182\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2bbc4a695b044de8a6c18b94126bba7"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 183\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7cc2d247a434266aa65a5f6832d3b03"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 184\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba64ee83e355401481f2d4b5bfb3354d"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 185\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc36452b9b2a46f385b22a86cd405516"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 186\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abaf28eb09014496b2d8d8346ac1d1b6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 187\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f14b8926943b496780fd9936fc18f3d2"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 188\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51694bec1d074e63bb664d7c5284cd5d"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 189\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"431a6f755d3d42fca469872306b0a281"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 190\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3de9a5fe81f44ce9f1e6af33fb43f90"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 191\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ad271d83aea477f9aa658a286661865"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 192\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"074efa09871c41bf958a9126f8c98be0"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 193\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"059d4ba4eb1141c4ac8bde4d091523f9"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 194\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b828344ebf4510abf5463b77a382f4"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 195\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"333302a67b4b46d28ca2d8e221f47323"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 196\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7d6a87db92c40f0a089346569d0e4a9"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 197\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d95be1782354ebf8024ec6c2fe96b09"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 198\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eebb3c199d4f4b658042a2e2186b1d7a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 199\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29a9f106f8094004817722a85add0958"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 200\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2f8b985f5aa430ea0cfef8e76434be7"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 201\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a16cfc640a654416aaed094095890ed7"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 202\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff6b53b983774540bbf236843a973523"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 203\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fd5110a9e114b7d8acc0c95a6e1c84c"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 204\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6210cec522974277a7d9f32579cc5485"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 205\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c39a8f4f77484dfaa136ff0c76b42af1"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 206\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"101a25a9e60649399591686ed3a55f50"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 207\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc06b5ef5a8b40c29fca6046938b109d"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 208\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd2c9b046933482980269cf73aa60771"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 209\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6291e17a50104077b84eac489d84c59e"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 210\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3868cb2be2464de59a4272906b4c4e82"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 211\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca1bf3ce406d45f49ecf5f18aa96483b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 212\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2b924727ac2453e9f3988a24ae8a7a0"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 213\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf9d9793e4a94062820e5578766b3705"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 214\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6fef9e07e254664bb0bdef9dc0c5005"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 215\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88be26ee7a0c4d809ca132be3fb2fb65"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 216\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32976e4d87cb42e0aaff4901efe9be33"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 217\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e46f4b368304fda831ac395f270fd41"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 218\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2f37fc6a6e540b3afe7b09251c24ac3"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 219\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98c4cf73081a4fc4b4984c13ef6dc522"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 220\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86d3eaa2e2014effbd65060121b914b6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 221\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28f6843b155a406684d8b7db74bd6adc"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 222\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3360bb0115c74a7c8471fe4cf56fdf3e"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 223\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d98e54b36fcc498a97b7cb4e460133f1"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 224\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"835353fe64584c5685461ad2c10e1598"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 225\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"150bfa110d7a40da900d3f698d9f4f36"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 226\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c47580894428486599f6a80ce78c4579"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 227\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf3e68a96760424889887d84613b72ba"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 228\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29c51fe8863744b092fe7fd717f57a5f"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 229\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58324a12771341a58b1c8ec597300437"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 230\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"395968d3e01b445595f341bd967e3f8a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 231\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b489fede2a744b95968fcfe619856be1"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 232\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2deee19fb987476eb28c9fee7819f355"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 233\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a50a5db4ff544589190ecb63390d3d6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 234\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2379bcc78e8b476dba3a9aa4e5a93766"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 235\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3e57512da3f4fa38a7958f7dd2e2f62"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 236\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7c51ab885944756ba9756664a4875de"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 237\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eefa95d33142405a9e0e42d718515da0"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 238\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ed8b0dd2bbf42088106823d17a30462"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 239\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cb1b0ee8c144a06bbe984c8d8bcfb12"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 240\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d82156d2d7de4e3bad17925fd27fc3a2"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 241\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e3ed98f8c1640c9b6ce780427458738"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 242\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42eb58e9bcc941c49d50062c4278f95a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 243\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d205d6736055412baa7354bddcc34e73"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 244\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b88995b663ed48c98c43ba9f0921d0bb"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 245\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93338e87a5cc463c8a37c1956c806572"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 246\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a47d60f4c28b44cfacdbed419b6aa629"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 247\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cefa245e544c4d9aba7b32571a28b3fe"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 248\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fad7e5c77a5f4070a931b114d3d0152d"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 249\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"475fee4b221f4392a967e9aa7688cf35"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 250\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9ddb8881eef4231b55b9bd8ea71d875"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 251\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2ec9870d7b644d89680b74390c344d3"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 252\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2ef1893d14742e7b4eea124afd744de"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 253\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ffa4b8b8b284e2794d59346277e3f69"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 254\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3eda30296a944a98fb4e5cd9bd862fc"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 255\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d40bb6b924f4420fbf934aecf445f036"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 256\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c0d18674d3a4d0db5c3b74ec0b1b2f6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 257\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b87f63086ce4578af8a62722503432f"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 258\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e8ef0326e164ba2ac346f0ad3c9cc7a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 259\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2079aeda202a41b7b637568bfc47efcf"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 260\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bab52696010340e29cce63d614a02815"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 261\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d400b5c0bd0144dfa472e49a4c30837d"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 262\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d213ea0f99814b90b6497d04a638f1dd"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 263\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17a11588a5eb4b43ae9013e08283c9aa"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 264\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67dd74f9823c47d88b616ef1dd165a13"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 265\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c057c63d8d2e41de9ffd7e2b32e8c3cf"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 266\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2563fc7ceb3c49ed8e70032efced5e00"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 267\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3981ae71f2704c1dac719bad0fad8209"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 268\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a07bd37be54b4ac2bfb393ae8bcac793"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 269\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc34b430a9ad4cd887b419093f7e67d6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 270\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c97483783314fcd8b48b257f9649929"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 271\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73c7c37225064d3ebb69e7854efb51d2"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 272\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"446481c80cce4be4bb304131a06b446b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 273\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5029d152f67b4a538cb51c81fc1a1e31"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 274\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d01a4fc3916b4af4a34c39ca68a1dc0a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 275\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3117b9f860cc47d18293c998efc83fd2"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 276\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b38ef00b05ea481bbd39b856b0c1075d"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 277\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"011c6c72a56940159960a038a3cad92a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 278\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6dbf572f1e840d5aa4c0779d029e7d1"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 279\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8f7cfe04f0c4f408acbbf9160a805af"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 280\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6901a14a6a644fffb7eb051084f29155"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 281\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4c4c01929ff4d6fbb0874119100674d"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 282\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eead05c93d9d41d7abd8c74c6b00433c"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 283\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3bdf088cbe3408bb51ce49352a4b003"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 284\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d554925a69b417a8679d4468eea0311"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 285\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"757bfc950cf9474193d1353b1ecb0e28"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 286\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ec96df4a4e5430cae259959fabbc587"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 287\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eab18debd7a24b01ab5518b336af2845"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 288\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1b5f0afe3594ac7adb846dae5d250e2"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 289\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed19c8cfb41f42e6aafa7b21486f39ed"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 290\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74480400c5e5433aafff9af1e4ed8a6d"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 291\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ee685e24ee347deb46f040067a8a6f4"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 292\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69862f99951147a49465e00afe7ea78c"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 293\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebdae7c7cb194bbe9a4e87ccd673fa44"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 294\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b24ddb3b02344e98194956ab003fa68"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 295\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e2e1d9236104018a6546ed700321b98"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 296\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f40c2876c6dc4397b00aa281db56c8a9"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 297\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"756867cac06343bd82b9044017660def"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 298\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"304b4061fa224d04901decd55d419045"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 299\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"debdb1e221ed455eaab6c1457d46c7ca"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 300\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02e514e726fd4de0aa3ffed71439d242"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 301\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb95c92e649248fb9995e5b3f0678f9f"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 302\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ce6698077994afbaa915fac8ceecf65"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 303\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91d6737dd7734d9b837718d3a7d32cb1"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 304\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dc093202e464a958f567ccb2fcba0d1"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 305\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"499541c318f945638e64e23a233e7cbc"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 306\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"687e25891c0a4a8781fa62106b664e05"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 307\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c6bcd4f5f6446ad92eaaa5dab2e5ce6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 308\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5c1c8f0afda44d4a329afeb69c63ba2"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 309\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c2179487cad49feb26e1dbf16deb1e4"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 310\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b47f06f4a3a401c949c926441f736bc"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 311\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cad9b85849734135a62e9c8b815a9d17"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 312\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"580793b706ec4fa086c9fafa129bcd1b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 313\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50462219d37044bca1016df03c9c4541"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 314\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5994412127142529f7aba7413f13028"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 315\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a45846ec6bfe43228e1895252c91c39a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 316\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b7b506e8d4e41b893fec94afc1209e6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 317\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9322ff74bc9c4661bfb39e84f8c2aa82"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 318\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ef7eab7f1934df58fce900857668c4a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 319\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a2623f280864e48979126a8273b45fe"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 320\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"daa29f5c59fe41f88f31d8651e5bd9ad"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 321\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df0e16e4b5014ae09521e939a217097a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 322\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61dc6f69d8b14beaa5ad2e790d3acd43"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 323\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66651d94bc0a4874a324914ec8b0f87d"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 324\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfee17e1f92541b6b2f3c1bd881622d6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 325\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07ece077f5aa42089ac62a662159bfba"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 326\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"920db177c3734207897d6484dc7d22e0"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 327\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0be963f7a9d6408891e6e72326861bc5"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 328\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56f0026d77dc4ee8be82e7b74df98a89"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 329\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7725eeb597d9468cb561f190a1da85d6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 330\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0070161547a746c39c598f8494c28b83"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 331\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c451c402daaa4c93907161492be6b9f5"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 332\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7b92c7eabce4fcaaa4895a1786058fc"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 333\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d182d64ba3684821849bbe0a22773f36"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 334\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85e8adc1257843e0b41ae71ea13c6799"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 335\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af7b274ec7ae4a3885fb5682fbbe2d9e"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 336\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"197a998a411c4a118038cde2850241be"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 337\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6984ee6674a84cf4a1316292497d76f4"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 338\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01dcaf6660994fbda94f8a24d43e11c9"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 339\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66ddc196361b4bf48f81541c6d3d7173"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 340\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ce31cd87f874e2eaa12e0d282734e48"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 341\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c503d5d23794453a2f5c1e020176475"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 342\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39fb301dc5d448e8924e97fab9c66ce7"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 343\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebc7afaec2ee4d3eb811c4ee684a5fac"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 344\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"403b513966db4ec793f2adcea4688b76"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 345\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6c6da9849224fe2ae1ecc1ca61c49d8"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 346\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"476540f5dde54bc69a03610bb538f323"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 347\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46aa966a2cfd43ac989d3090d41754ad"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 348\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c63b4d1255a4927aaf018a9a4bdfd31"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 349\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d465639d5224c9096933f84f6d08642"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 350\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"edcd8074dc464e08b4ac20ff001f6c1a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 351\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f1d2b8b151c426c96fdad1683a7f6d0"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 352\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"496e9f0271b44dac942b8dbc2bc48044"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 353\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"222ebd2bc5494029a9833dacd1e02917"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 354\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fe2168e67724fd28af0a665f5bf132e"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 355\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75d77cc872da4f958fdd6e6fb0d359cb"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 356\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bec2969d6bf4995a4b1c0a348aa2512"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 357\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42aebdf4fe6a4debb1e849d0ed3c6215"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 358\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cf84d5d706247f48f46d246c615d214"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 359\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81d40b29338c4a8b8ff0d5d7d8bae161"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 360\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c272db3a4780430197a9353073f3ace0"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 361\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2affb53240d436ea75c052942d7e31a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 362\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42cde65c7e474c948d89505294d491e7"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 363\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a803a09b3edf4b6bb0b148542be2a2eb"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 364\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f74063dadf714ad981e1e20253ee9da0"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 365\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"173f56c5d4ed455e9a7f46714465abe5"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 366\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9b522d0797e441292628b1f8eacd912"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 367\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d411476dc57547e6a0fc39b806c7273b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 368\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52a22804439f40b1a9e1cff8934ab783"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 369\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef82659cd7a143298353d20be72bd3f0"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 370\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d104fff0d2574fdeb5afc2f7e96a482f"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 371\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed74ba3eba274e74b1fbb03ce3796ac1"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 372\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32e4310c7ba44b158d6053a42bb016f3"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 373\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8fa26ec82f44f90b6ab2fe392622024"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 374\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"092da21f646b47108984e74b8ab0b83a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 375\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"386f2eb4418b4a25a8579552fef9f1b5"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 376\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03515960a5784ddbbbe034c65df0e8f5"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 377\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9e8129d6442416e9c7fab3606161aeb"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 378\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ecbef51774da4c488a72e60a63e93c72"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 379\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"967fb63d2c8641c8a08815c288738bf6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 380\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b512913305e48da8c9d4c1b7418e446"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 381\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc3e925e48e64220b9eca61dcd6657c4"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 382\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7b9998246874f01939a735264a19f32"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 383\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"501023f14db1412db3ed760633db26f6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 384\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66c9230d99574d3796ffa128688a71f5"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 385\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5d0e9511a1a4c95a0f12b9bc2da569c"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 386\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5b36811386f4000a4785561d3d60556"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 387\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd25dd96f0774fafad3df707eafa0681"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 388\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"629882038ad34268939d0cafab09649a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 389\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eb4bcaaf8094249ae3bbf45ff443d6c"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 390\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b9fc6652e5d40eda039e972c0dd5a10"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 391\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97517df1ae4a48e782df2e195a5d5963"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 392\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4dcf4bdee3b4e3bb4e103899ea29b07"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 393\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"431ca1abffd34e2fb7450b5d609d6483"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 394\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f42a5fcc4e94d1d98966611ebe9aa2e"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 395\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5acc7e949ba34ff483e91852e8129186"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 396\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"789b93773fea4e9aaa9bcbe9e69ec545"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 397\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68e23ebfe5da456b90a6deee6f45c5e6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 398\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d40ff8b0ab24cedbfb43d7bb478e165"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 399\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"423f212213a94947a0714459a7befba2"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 400\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a40e60049274d3189bc3887bec3588d"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 401\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e8e61cb7bff41d4a906ab09b60082f0"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 402\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb9e3198934e4e9393158f6549a5c83b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 403\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c1b9cda19b94536b9c6a3fcdba04bc6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 404\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8f14db407c54219a1fe313dc0f6fe2b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 405\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fdeca8530d64544bcfcd528d6c37654"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 406\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62e1dd7e32b84dc194f061a5b88142bf"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 407\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"839d65e5d12d4e518626e9163c9a72fa"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 408\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec8b64a4cc574a138789f718eef2ae7b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 409\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"128fdcc10ee04ef7bab1bcf7aba1ff93"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 410\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"417b6e157cd944b585f5868b90c76f71"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 411\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4da219636514f6b8d1b4bc3a0e707d3"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 412\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4edb5fa64404adca1e444d151284ff2"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 413\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2a36732c3e84b5fb2e67369f3ee10eb"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 414\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1335cb15aab9496f947c80a0e059695c"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 415\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb5cbc47380745cd92dacb929e37a7d0"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 416\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc2e06fabfff409f9e4a70e86e436b81"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 417\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cdb5a78275ab46a9a8aa56e30191fc46"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 418\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d246a9de544331833559699fa96f58"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 419\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97f44041c7554245a52e04150317bfa8"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 420\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"543c1fd290d74793bfcb6f2c1e0f2be8"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 421\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36db0453b65545c0919f8f2954611294"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 422\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eae5be7c26b45fc99f34c1722984098"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 423\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56b55ed535fd44ec846e6dd722b71c6b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 424\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eb81ff4fd860424c8feae31adb45f638"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 425\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d2a978e6a2924700aecfe51789f3bdb8"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 426\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"158ce8a2072948f3bb9fab9e525ab51d"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 427\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d35eed971f64ed789992236cc9710c9"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 428\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c15b9023c0bd49b4bcb20e2d7d8a8688"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 429\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b93b8e13910a47fdba87f072cf9fe777"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 430\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"727a1726d8534232a05626f1dfe128df"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 431\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ff15129436d49ff9654bd50e3c808ea"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 432\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a231c637e0e41568e162fcda2127131"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 433\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80187d1af4844ec58e27ed5a1a07c2c4"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 434\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4de7f66d6c448a4a88ff9578d2222ec"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 435\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9546bd636f6042f2a74d144c425891dc"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 436\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"612610c07ee64136a7bf7a1fe6b9d4db"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 437\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e57b602bab14050bc09a21dd938d458"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 438\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55784f1d5f614e8f9cc1cc14ddb78afb"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 439\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b6e257d023a4a8f920e1d95686dea19"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 440\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28a2f9fda2784d74bae00735a667f980"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 441\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76558af76df249ceb6c7000a6bda46d2"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 442\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1667059bf6ad46348aa01e91faf65c92"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 443\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"338023ef059a48b6a83127648d1243c3"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 444\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b6a261bb1fe4ad6bd9164355a22f9a6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 445\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bcd2cb3358204dcbba1e3b3e6eb6741e"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 446\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed506732a0ff49219db5d392d6c26651"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 447\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72e6283db9524c2e8b51c2e00f5ffae5"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 448\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4a5fd6713274cd592d7fd63abd80841"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 449\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceeb3a0d491c4b1e87d9736ca39ceb71"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 450\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7436e35cb7745ccb060591ec477f843"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 451\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf2926248bac4f1f9bef2a4db29b1124"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 452\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a29dbfd6ad944b89e4db6f1724ef980"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 453\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe70b6d7de9149629b0ac2f9e51d63a7"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 454\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d3799364d7f4e7288c8f72e8338a394"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 455\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f16bd18726214a9f9e2ce2026bcb87b8"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 456\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f457bb4dec274d22ac812dc7d07cc6aa"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 457\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3eb85b5341804ababc90476fb822127d"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 458\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e2349a30237b4b458155e09bc4668042"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 459\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb1ba095c2074b2594253ca6d21d11f9"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 460\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96ffa62dba55425592bff477b648001a"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 461\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8547c5edd7e0423a9c026be062894727"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 462\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b55c474c6fb4738940aefe152964f5b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 463\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8bd3f1100c340fc8f8e311a0af9e548"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 464\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab7e73cc72e34a77aaa2d5b686d68a3c"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 465\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1223ccdd3de4406c81839c4513239476"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 466\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83fd328b88f94a80bf1c95d338b86a5f"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 467\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea0e9acc9738454baa6bff3860610094"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 468\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1da06d8c80f43f8a2098962429772ea"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 469\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a529f7803414f539bb53817ab19b14b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 470\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcbdcc9b7dc04022b914a264b4030aff"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 471\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75b73620837049bebb5684b6e9c4e292"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 472\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"827cd5fab9eb40e1b83402596a2a6bb7"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 473\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9d2425315fd4e118d962544883d9642"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 474\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2b8866b1a624092b89694fb232ac345"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 475\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b370f4d6141a469a86fb542b7d6aa673"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 476\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a5fb1461ad944e39d451d0bb08e0327"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 477\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"067d4486e04642829fac363b7c956a72"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 478\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db4691bed08d4dda9c4d2d97fb94423c"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 479\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8ec52508a8f4aa7a32a926524e71330"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 480\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34263f99bf43452a94a83898a65bd4f8"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 481\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b28f73ba3cbf410cae7121c79c9ad96f"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 482\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b505da64d9114e0fa8bc74e417852f9b"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 483\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30ea36b555614d4c8c4314bc2cf0fdc9"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 484\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad53a439dbe5439680aea9b061296e27"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 485\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"667503bbbd0545cd8619dcaddd9f2a91"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 486\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4493d45ddb34e9894857b302602a4e9"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 487\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a4b4633436c4afcb90c177f625f3a05"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 488\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e4f32477fc140cfa79c6e97479914cd"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 489\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49198b987bbf484fbe21fd893ceb8694"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 490\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a032366b18a4911921deda205503663"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 491\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bae2792a0bd4b24a67982b56fe778b6"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 492\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5ddb6319a8a4cf78058aed4f25046f4"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 493\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d218591a305441cad5d09d735d8c11e"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 494\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27e9d92a34d4421b8402f28ab2b768ac"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 495\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8fb4defb9a141f186b25397db9ad3da"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 496\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6c93d4f53ec4c379b189d89f5d0cc49"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 497\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64a5500b3feb4ea192941f2fcbb0ecea"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 498\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b36ec6b1c5742d6bc1ae5b44c9bcb6c"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 499\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"393252cf67654d3eb716ed85a61b6194"}},"metadata":{}},{"name":"stdout","text":"已处理样本数: 500\n计算评测指标...\n评测结果：\nexact: 2.20\nf1: 4.89\ntotal: 500.00\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"优化promt设计","metadata":{}},{"cell_type":"code","source":"import json\nimport collections\nimport re\nimport string\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\n# ------------------ SQuAD Evaluation Functions ------------------\n\ndef normalize_answer(s):\n    if s is None:\n        return \"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    return 2 * precision * recall / (precision + recall)\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                qid = qa['id']\n                gold_answers = [a['text'] for a in qa.get('answers', []) if normalize_answer(a.get('text', ''))]\n                if not gold_answers:\n                    gold_answers = ['']  # no answer questions\n                a_pred = preds.get(qid, \"\")\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\ndef make_eval_dict(exact_scores, f1_scores):\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        ('exact', 100.0 * sum(exact_scores.values()) / total if total else 0),\n        ('f1', 100.0 * sum(f1_scores.values()) / total if total else 0),\n        ('total', total),\n    ])\n\n# ------------------ RAG 主体流程 ------------------\n\ndef load_squad_dataset(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data_json = json.load(f)\n    return data_json['data']\n\ndef chunk_and_truncate_contexts(contexts, max_chars=3000):\n    out = []\n    for c in contexts:\n        if len(c) <= max_chars:\n            out.append(c)\n        else:\n            out.append(c[:max_chars] + \" ...\")\n    return out\n\ndef build_context_embeddings(contexts, embedder, batch_size=512):\n    embs = []\n    for i in tqdm(range(0, len(contexts), batch_size), desc=\"Embedding contexts\"):\n        batch = contexts[i:i+batch_size]\n        batch_emb = embedder.encode(batch, convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n        batch_emb = np.asarray(batch_emb, dtype=np.float32)\n        embs.append(batch_emb)\n    embedder.to('cpu')  # 回收GPU显存\n    return np.vstack(embs)\n\ndef retrieve_top_k(question, all_ctx_emb, embedder, contexts, k=20):\n    q_emb = embedder.encode([question], convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n    q_emb = np.asarray(q_emb, dtype=np.float32)\n    sims = np.dot(all_ctx_emb, q_emb.T).squeeze(1)\n    topk_idx = np.argpartition(-sims, k)[:k]\n    topk_idx = topk_idx[np.argsort(-sims[topk_idx])]\n    hits = [contexts[i] for i in topk_idx if i < len(contexts)]\n    return hits\n\ndef generate_answer_with_prompt(question, contexts, tokenizer, model, prompt_template, max_new_tokens=64):\n    context_text = \"\\n\".join(chunk_and_truncate_contexts(contexts))\n    prompt = prompt_template.format(question=question, context=context_text)\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n    device = next(model.parameters()).device\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        num_beams=4,\n        no_repeat_ngram_size=3,\n        early_stopping=True\n    )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):]\n    answer = answer.strip()\n    if answer.lower() in [\"false\", \"no answer\", \"none\", \"\"]:\n        answer = \"\"\n    return answer\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_file = '/kaggle/input/squad-train/train-v2.0.json'\n    dev_file = '/kaggle/input/squad-dev/dev-v2.0.json'\n\n    sentence_transformer_name = 'sentence-transformers/all-mpnet-base-v2'\n    model_name = \"google/flan-t5-xl\"\n    batch_size_embed = 256\n    max_test = 500\n\n    print(\"加载训练数据...\")\n    train_data = load_squad_dataset(train_file)\n    print(\"加载验证数据...\")\n    dev_data = load_squad_dataset(dev_file)\n\n    contexts = []\n    for article in train_data:\n        for para in article[\"paragraphs\"]:\n            contexts.append(para[\"context\"])\n    print(f\"训练集上下文数量: {len(contexts)}\")\n\n    print(\"加载句向量模型...\")\n    embedder = SentenceTransformer(sentence_transformer_name, device=device)\n\n    print(\"GPU上计算所有上下文的向量表示...\")\n    all_ctx_emb = build_context_embeddings(contexts, embedder, batch_size=batch_size_embed)\n\n    print(f\"加载生成模型和tokenizer: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n    model.eval()\n\n    # 定义多个Prompt模板，方便自动搜索最优\n    prompt_templates = [\n        \"Question: {question}\\nContext:\\n{context}\\nAnswer briefly. If no answer, say 'no answer'.\",\n        \"Please answer the question based on the context below. If unknown, respond with 'no answer'.\\nQuestion: {question}\\nContext:\\n{context}\\nAnswer:\",\n        \"Given the context, answer the question concisely. If not answerable, reply 'no answer'.\\nQ: {question}\\nContext:\\n{context}\\nA:\",\n        # 可以加更多模板\n    ]\n\n    best_prompt = None\n    best_score = -1\n\n    print(f\"开始在验证集前{max_test}个样本上自动搜索最优Prompt...\")\n    for prompt_template in prompt_templates:\n        preds = {}\n        count = 0\n        for article in dev_data:\n            for para in article[\"paragraphs\"]:\n                for qa in para[\"qas\"]:\n                    if count >= max_test:\n                        break\n                    qid = qa['id']\n                    question = qa['question']\n\n                    retrieved = retrieve_top_k(question, all_ctx_emb, embedder, contexts, k=5)\n                    answer = generate_answer_with_prompt(question, retrieved, tokenizer, model, prompt_template)\n\n                    preds[qid] = answer\n                    count += 1\n                if count >= max_test:\n                    break\n            if count >= max_test:\n                break\n\n        # 过滤dev_data只保留测试样本对应的qa\n        filtered_dev_data = []\n        for article in dev_data:\n            new_article = {\"paragraphs\": []}\n            for para in article[\"paragraphs\"]:\n                new_para = {\"qas\": [], \"context\": para[\"context\"]}\n                for qa in para[\"qas\"]:\n                    if qa['id'] in preds:\n                        new_para[\"qas\"].append(qa)\n                if new_para[\"qas\"]:\n                    new_article[\"paragraphs\"].append(new_para)\n            if new_article[\"paragraphs\"]:\n                filtered_dev_data.append(new_article)\n\n        exact_scores, f1_scores = get_raw_scores(filtered_dev_data, preds)\n        eval_results = make_eval_dict(exact_scores, f1_scores)\n\n        print(f\"Prompt模板: {prompt_template[:40]}... => Exact: {eval_results['exact']:.2f}, F1: {eval_results['f1']:.2f}\")\n\n        if eval_results['f1'] > best_score:\n            best_score = eval_results['f1']\n            best_prompt = prompt_template\n\n    print(f\"最优Prompt是:\\n{best_prompt}\")\n\n    # 使用最优Prompt生成最终结果\n    print(f\"使用最优Prompt生成验证集回答...\")\n    final_preds = {}\n    count = 0\n    for article in dev_data:\n        for para in article[\"paragraphs\"]:\n            for qa in para[\"qas\"]:\n                if count >= max_test:\n                    break\n                qid = qa['id']\n                question = qa['question']\n\n                retrieved = retrieve_top_k(question, all_ctx_emb, embedder, contexts, k=5)\n                answer = generate_answer_with_prompt(question, retrieved, tokenizer, model, best_prompt)\n\n                final_preds[qid] = answer\n                count += 1\n            if count >= max_test:\n                break\n        if count >= max_test:\n            break\n\n    filtered_dev_data = []\n    for article in dev_data:\n        new_article = {\"paragraphs\": []}\n        for para in article[\"paragraphs\"]:\n            new_para = {\"qas\": [], \"context\": para[\"context\"]}\n            for qa in para[\"qas\"]:\n                if qa['id'] in final_preds:\n                    new_para[\"qas\"].append(qa)\n            if new_para[\"qas\"]:\n                new_article[\"paragraphs\"].append(new_para)\n        if new_article[\"paragraphs\"]:\n            filtered_dev_data.append(new_article)\n\n    exact_scores, f1_scores = get_raw_scores(filtered_dev_data, final_preds)\n    eval_results = make_eval_dict(exact_scores, f1_scores)\n\n    print(\"最终评测结果：\")\n    for k, v in eval_results.items():\n        print(f\"{k}: {v:.2f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"500个qa 耗时约27h，不可接受，故作罢。","metadata":{}},{"cell_type":"markdown","source":"# 调节top k","metadata":{}},{"cell_type":"code","source":"import json\nimport collections\nimport re\nimport string\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\n\n# ------------------ SQuAD Evaluation Functions ------------------\n\ndef normalize_answer(s):\n    if s is None:\n        return \"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    return 2 * precision * recall / (precision + recall)\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                qid = qa['id']\n                gold_answers = [a['text'] for a in qa.get('answers', []) if normalize_answer(a.get('text', ''))]\n                if not gold_answers:\n                    gold_answers = ['']  # no answer questions\n                a_pred = preds.get(qid, \"\")\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\ndef make_eval_dict(exact_scores, f1_scores):\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        ('exact', 100.0 * sum(exact_scores.values()) / total if total else 0),\n        ('f1', 100.0 * sum(f1_scores.values()) / total if total else 0),\n        ('total', total),\n    ])\n\n# ------------------ RAG 主体流程 ------------------\n\ndef load_squad_dataset(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data_json = json.load(f)\n    return data_json['data']\n\ndef chunk_and_truncate_contexts(contexts, max_chars=3000):\n    out = []\n    for c in contexts:\n        if len(c) <= max_chars:\n            out.append(c)\n        else:\n            out.append(c[:max_chars] + \" ...\")\n    return out\n\ndef build_context_embeddings(contexts, embedder, batch_size=256):\n    embs = []\n    for i in tqdm(range(0, len(contexts), batch_size), desc=\"Embedding contexts\"):\n        batch = contexts[i:i+batch_size]\n        batch_emb = embedder.encode(batch, convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n        batch_emb = np.asarray(batch_emb, dtype=np.float32)\n        embs.append(batch_emb)\n    embedder.to('cpu')\n    return np.vstack(embs)\n\ndef retrieve_top_k(question, all_ctx_emb, embedder, contexts, k=5, sim_threshold=None):\n    q_emb = embedder.encode([question], convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n    q_emb = np.asarray(q_emb, dtype=np.float32)\n    sims = np.dot(all_ctx_emb, q_emb.T).squeeze(1)\n    \n    if sim_threshold is not None:\n        # 过滤相似度低的\n        candidate_idxs = np.where(sims >= sim_threshold)[0]\n        if len(candidate_idxs) == 0:\n            # 没有高于阈值的，降级为无过滤\n            candidate_idxs = np.arange(len(sims))\n        sims_filtered = sims[candidate_idxs]\n        topk_idx = candidate_idxs[np.argpartition(-sims_filtered, min(k,len(sims_filtered)-1))[:k]]\n        topk_idx = topk_idx[np.argsort(-sims[topk_idx])]\n    else:\n        topk_idx = np.argpartition(-sims, k)[:k]\n        topk_idx = topk_idx[np.argsort(-sims[topk_idx])]\n\n    hits = [contexts[i] for i in topk_idx if i < len(contexts)]\n    return hits\n\ndef generate_answer(question, contexts, tokenizer, model, max_new_tokens=64):\n    context_text = \"\\n\".join(chunk_and_truncate_contexts(contexts))\n    prompt = f\"Question: {question}\\nContext:\\n{context_text}\\nAnswer (if no answer, respond with 'no answer'):\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n    device = next(model.parameters()).device\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        do_sample=False,\n        num_beams=4,\n        no_repeat_ngram_size=3,\n        early_stopping=True\n    )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):]\n    answer = answer.strip()\n    if answer.lower() in [\"false\", \"no answer\", \"none\", \"\"]:\n        answer = \"\"\n    return answer\n\ndef evaluate_for_k(k, sim_threshold, dev_data, all_ctx_emb, embedder, contexts, tokenizer, model, max_test=500):\n    preds = {}\n    count = 0\n    for article in dev_data:\n        for para in article[\"paragraphs\"]:\n            for qa in para[\"qas\"]:\n                if count >= max_test:\n                    break\n                qid = qa[\"id\"]\n                question = qa[\"question\"]\n\n                retrieved = retrieve_top_k(question, all_ctx_emb, embedder, contexts, k=k, sim_threshold=sim_threshold)\n                answer = generate_answer(question, retrieved, tokenizer, model)\n                preds[qid] = answer\n\n                count += 1\n            if count >= max_test:\n                break\n        if count >= max_test:\n            break\n\n    filtered_dev_data = []\n    for article in dev_data:\n        new_article = {\"paragraphs\": []}\n        for para in article[\"paragraphs\"]:\n            new_para = {\"qas\": [], \"context\": para[\"context\"]}\n            for qa in para[\"qas\"]:\n                if qa[\"id\"] in preds:\n                    new_para[\"qas\"].append(qa)\n            if new_para[\"qas\"]:\n                new_article[\"paragraphs\"].append(new_para)\n        if new_article[\"paragraphs\"]:\n            filtered_dev_data.append(new_article)\n\n    exact_scores, f1_scores = get_raw_scores(filtered_dev_data, preds)\n    eval_results = make_eval_dict(exact_scores, f1_scores)\n    return eval_results\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_file = '/kaggle/input/squad-train/train-v2.0.json'\n    dev_file = '/kaggle/input/squad-dev/dev-v2.0.json'\n\n    sentence_transformer_name = 'sentence-transformers/all-mpnet-base-v2'\n    model_name = \"google/flan-t5-xl\"\n    batch_size_embed = 256\n\n    print(\"加载训练数据...\")\n    train_data = load_squad_dataset(train_file)\n    print(\"加载验证数据...\")\n    dev_data = load_squad_dataset(dev_file)\n\n    contexts = []\n    for article in train_data:\n        for para in article[\"paragraphs\"]:\n            contexts.append(para[\"context\"])\n    print(f\"训练集上下文数量: {len(contexts)}\")\n\n    print(\"加载句向量模型...\")\n    embedder = SentenceTransformer(sentence_transformer_name, device=device)\n\n    print(\"GPU上计算所有上下文的向量表示...\")\n    all_ctx_emb = build_context_embeddings(contexts, embedder, batch_size=batch_size_embed)\n\n    print(f\"加载生成模型和tokenizer: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n    model.eval()\n\n    # 尝试不同Top-K和相似度阈值组合\n    top_k_list = [2,1]\n    sim_threshold_list = [0.6, 0.5 , 0.4,0.3]\n\n    best_f1 = 0\n    best_params = None\n\n    for k in top_k_list:\n        for threshold in sim_threshold_list:\n            print(f\"评测参数: Top-K={k}, 相似度阈值={threshold}\")\n            results = evaluate_for_k(k, threshold, dev_data, all_ctx_emb, embedder, contexts, tokenizer, model)\n            print(f\"Exact: {results['exact']:.2f}, F1: {results['f1']:.2f}, Total: {results['total']}\")\n            if results['f1'] > best_f1:\n                best_f1 = results['f1']\n                best_params = (k, threshold)\n\n    print(f\"最佳参数 Top-K={best_params[0]}, 相似度阈值={best_params[1]}, F1={best_f1:.2f}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-10T17:34:42.904317Z","iopub.execute_input":"2025-08-10T17:34:42.904583Z","iopub.status.idle":"2025-08-10T18:27:24.939968Z","shell.execute_reply.started":"2025-08-10T17:34:42.904564Z","shell.execute_reply":"2025-08-10T18:27:24.939206Z"}},"outputs":[{"name":"stdout","text":"加载训练数据...\n加载验证数据...\n训练集上下文数量: 19035\n加载句向量模型...\nGPU上计算所有上下文的向量表示...\n","output_type":"stream"},{"name":"stderr","text":"Embedding contexts: 100%|██████████| 75/75 [04:14<00:00,  3.39s/it]\n","output_type":"stream"},{"name":"stdout","text":"加载生成模型和tokenizer: google/flan-t5-xl\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d827f6dc140849f494e8fff41ea8464f"}},"metadata":{}},{"name":"stdout","text":"评测参数: Top-K=2, 相似度阈值=0.6\nExact: 35.40, F1: 36.93, Total: 500\n评测参数: Top-K=2, 相似度阈值=0.5\nExact: 35.60, F1: 37.13, Total: 500\n评测参数: Top-K=2, 相似度阈值=0.4\nExact: 35.80, F1: 37.46, Total: 500\n评测参数: Top-K=2, 相似度阈值=0.3\nExact: 34.80, F1: 36.46, Total: 500\n评测参数: Top-K=1, 相似度阈值=0.6\nExact: 44.60, F1: 45.79, Total: 500\n评测参数: Top-K=1, 相似度阈值=0.5\nExact: 44.60, F1: 45.79, Total: 500\n评测参数: Top-K=1, 相似度阈值=0.4\nExact: 44.60, F1: 45.79, Total: 500\n评测参数: Top-K=1, 相似度阈值=0.3\nExact: 44.60, F1: 45.79, Total: 500\n最佳参数 Top-K=1, 相似度阈值=0.6, F1=45.79\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"从4分到20分，单靠阈值调节几乎不可能实现，故大概率是k往小，阈值往小","metadata":{}},{"cell_type":"markdown","source":"评测参数: Top-K=3, 相似度阈值=None\nExact: 18.40, F1: 20.44, Total: 500\n评测参数: Top-K=3, 相似度阈值=0.6\nExact: 19.60, F1: 21.64, Total: 500\n评测参数: Top-K=3, 相似度阈值=0.7","metadata":{}},{"cell_type":"markdown","source":"如果把k往大了调，成绩下降","metadata":{}},{"cell_type":"markdown","source":"评测参数: Top-K=1, 相似度阈值=0.2\nExact: 44.60, F1: 45.79, Total: 500","metadata":{}},{"cell_type":"markdown","source":"BM25融合向量相似度增强检索相关度 上下文先分句，再按字符数（1500）截断，减少噪声\n## BM25 + 向量检索融合（hybrid_search）\n\ntop n = 3 ,beam = 1,max token  = 32","metadata":{}},{"cell_type":"code","source":"import json\nimport collections\nimport re\nimport string\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\nfrom rank_bm25 import BM25Okapi\n\n# --------- SQuAD Evaluation Functions ---------\n\ndef normalize_answer(s):\n    if s is None:\n        return \"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    return 2 * precision * recall / (precision + recall)\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                qid = qa['id']\n                gold_answers = [a['text'] for a in qa.get('answers', []) if normalize_answer(a.get('text', ''))]\n                if not gold_answers:\n                    gold_answers = ['']  # no answer questions\n                a_pred = preds.get(qid, \"\")\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\ndef make_eval_dict(exact_scores, f1_scores):\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        ('exact', 100.0 * sum(exact_scores.values()) / total if total else 0),\n        ('f1', 100.0 * sum(f1_scores.values()) / total if total else 0),\n        ('total', total),\n    ])\n\n# --------- 上下文处理和检索 ---------\n\ndef build_bm25_index(contexts):\n    tokenized_corpus = [ctx.split() for ctx in contexts]\n    bm25 = BM25Okapi(tokenized_corpus)\n    return bm25\n\ndef hybrid_search(question, bm25, embedder, contexts, all_ctx_emb, k=1, bm25_top_n=5, sim_threshold=0.3):\n    tokenized_query = question.split()\n    bm25_scores = bm25.get_scores(tokenized_query)\n    bm25_top_idx = np.argsort(bm25_scores)[::-1][:bm25_top_n]\n    candidate_contexts = [contexts[i] for i in bm25_top_idx]\n    candidate_embs = all_ctx_emb[bm25_top_idx]\n\n    q_emb = embedder.encode([question], convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n    q_emb = np.asarray(q_emb, dtype=np.float32)\n\n    sims = np.dot(candidate_embs, q_emb.T).squeeze(1)\n\n    filtered_idxs = np.where(sims >= sim_threshold)[0]\n    if len(filtered_idxs) == 0:\n        filtered_idxs = np.arange(len(sims))\n\n    filtered_sims = sims[filtered_idxs]\n    filtered_bm25_idx = bm25_top_idx[filtered_idxs]\n\n    bm25_scores_filtered = np.array(bm25_scores)[filtered_bm25_idx]\n\n    # 归一化分数\n    bm25_norm = (bm25_scores_filtered - bm25_scores_filtered.min()) / (bm25_scores_filtered.max() - bm25_scores_filtered.min() + 1e-8)\n    sim_norm = (filtered_sims - filtered_sims.min()) / (filtered_sims.max() - filtered_sims.min() + 1e-8)\n\n    combined_scores = 0.5 * bm25_norm + 0.5 * sim_norm\n\n    topk_idx = np.argsort(combined_scores)[::-1][:k]\n    selected_contexts = [contexts[filtered_bm25_idx[i]] for i in topk_idx]\n    return selected_contexts\n\ndef generate_answer(question, contexts, tokenizer, model):\n    device = next(model.parameters()).device\n    context_text = \"\\n\".join(contexts)\n    prompt = f\"Question: {question}\\nContext:\\n{context_text}\\nAnswer (if no answer, respond with 'no answer'):\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=16,\n        num_beams=1,\n        do_sample=False,\n        no_repeat_ngram_size=3,\n        early_stopping=True\n    )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):]\n    answer = answer.strip()\n    if answer.lower() in [\"false\", \"no answer\", \"none\", \"\"]:\n        answer = \"\"\n    return answer\n\n# --------- 主流程 ---------\n\ndef load_squad_dataset(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data_json = json.load(f)\n    return data_json['data']\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_file = '/kaggle/input/squad-train/train-v2.0.json'\n    dev_file = '/kaggle/input/squad-dev/dev-v2.0.json'\n\n    sentence_transformer_name = 'sentence-transformers/all-mpnet-base-v2'\n    model_name = \"google/flan-t5-xl\"\n\n    print(\"加载训练数据...\")\n    train_data = load_squad_dataset(train_file)\n    print(\"加载验证数据...\")\n    dev_data = load_squad_dataset(dev_file)\n\n    print(\"提取训练集上下文...\")\n    contexts = []\n    for article in train_data:\n        for para in article[\"paragraphs\"]:\n            contexts.append(para[\"context\"])\n\n    print(f\"训练集上下文数量: {len(contexts)}\")\n\n    print(\"加载句向量模型...\")\n    embedder = SentenceTransformer(sentence_transformer_name, device=device)\n\n    print(\"GPU上计算所有上下文向量...\")\n    batch_size_embed = 256\n    all_ctx_emb = []\n    for i in tqdm(range(0, len(contexts), batch_size_embed), desc=\"Embedding contexts\"):\n        batch = contexts[i:i+batch_size_embed]\n        batch_emb = embedder.encode(batch, convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n        all_ctx_emb.append(batch_emb)\n    all_ctx_emb = np.vstack(all_ctx_emb)\n\n    print(\"构建BM25索引...\")\n    bm25 = build_bm25_index(contexts)\n\n    print(f\"加载生成模型和tokenizer: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n    model.eval()\n\n    k = 1\n    sim_threshold = 0.3\n    bm25_top_n = 5\n    max_test = 500\n\n    preds = {}\n    count = 0\n    print(\"开始评测...\")\n    for article in tqdm(dev_data, desc=\"Eval articles\"):\n        for para in article[\"paragraphs\"]:\n            for qa in para[\"qas\"]:\n                if count >= max_test:\n                    break\n                qid = qa[\"id\"]\n                question = qa[\"question\"]\n\n                retrieved_contexts = hybrid_search(question, bm25, embedder, contexts, all_ctx_emb, k=k, bm25_top_n=bm25_top_n, sim_threshold=sim_threshold)\n                answer = generate_answer(question, retrieved_contexts, tokenizer, model)\n                preds[qid] = answer\n\n                count += 1\n            if count >= max_test:\n                break\n        if count >= max_test:\n            break\n\n    # 过滤验证集，保留预测题目\n    filtered_dev_data = []\n    for article in dev_data:\n        new_article = {\"paragraphs\": []}\n        for para in article[\"paragraphs\"]:\n            new_para = {\"qas\": [], \"context\": para[\"context\"]}\n            for qa in para[\"qas\"]:\n                if qa[\"id\"] in preds:\n                    new_para[\"qas\"].append(qa)\n            if new_para[\"qas\"]:\n                new_article[\"paragraphs\"].append(new_para)\n        if new_article[\"paragraphs\"]:\n            filtered_dev_data.append(new_article)\n\n    exact_scores, f1_scores = get_raw_scores(filtered_dev_data, preds)\n    eval_results = make_eval_dict(exact_scores, f1_scores)\n\n    print(f\"评测结果: Exact={eval_results['exact']:.2f}, F1={eval_results['f1']:.2f}, 总样本数={eval_results['total']}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T06:58:49.416862Z","iopub.execute_input":"2025-08-11T06:58:49.417662Z","iopub.status.idle":"2025-08-11T07:08:13.475381Z","shell.execute_reply.started":"2025-08-11T06:58:49.417633Z","shell.execute_reply":"2025-08-11T07:08:13.474446Z"}},"outputs":[{"name":"stdout","text":"加载训练数据...\n加载验证数据...\n提取训练集上下文...\n训练集上下文数量: 19035\n加载句向量模型...\nGPU上计算所有上下文向量...\n","output_type":"stream"},{"name":"stderr","text":"Embedding contexts: 100%|██████████| 75/75 [04:59<00:00,  3.99s/it]\n","output_type":"stream"},{"name":"stdout","text":"构建BM25索引...\n加载生成模型和tokenizer: google/flan-t5-xl\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddb13eee4a524e9cad2c2960c04d1c24"}},"metadata":{}},{"name":"stdout","text":"开始评测...\n","output_type":"stream"},{"name":"stderr","text":"Eval articles:   0%|          | 0/35 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:433: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\nEval articles:   3%|▎         | 1/35 [04:05<2:19:03, 245.41s/it]","output_type":"stream"},{"name":"stdout","text":"评测结果: Exact=46.60, F1=46.94, 总样本数=500\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"大轮测","metadata":{}},{"cell_type":"code","source":"import json\nimport collections\nimport re\nimport string\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\nfrom rank_bm25 import BM25Okapi\nimport nltk\n\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\n\n# --------- SQuAD Evaluation Functions ---------\n\ndef normalize_answer(s):\n    if s is None:\n        return \"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    return 2 * precision * recall / (precision + recall)\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                qid = qa['id']\n                gold_answers = [a['text'] for a in qa.get('answers', []) if normalize_answer(a.get('text', ''))]\n                if not gold_answers:\n                    gold_answers = ['']  # no answer questions\n                a_pred = preds.get(qid, \"\")\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\ndef make_eval_dict(exact_scores, f1_scores):\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        ('exact', 100.0 * sum(exact_scores.values()) / total if total else 0),\n        ('f1', 100.0 * sum(f1_scores.values()) / total if total else 0),\n        ('total', total),\n    ])\n\n# --------- 上下文拆分和检索 ---------\n\ndef chunk_context(text, max_tokens=100):\n    # 简单按句子拆分，且合并成块，每块近似max_tokens词\n    sentences = sent_tokenize(text)\n    chunks = []\n    current_chunk = []\n    current_len = 0\n    for sent in sentences:\n        sent_len = len(sent.split())\n        if current_len + sent_len > max_tokens and current_chunk:\n            chunks.append(' '.join(current_chunk))\n            current_chunk = [sent]\n            current_len = sent_len\n        else:\n            current_chunk.append(sent)\n            current_len += sent_len\n    if current_chunk:\n        chunks.append(' '.join(current_chunk))\n    return chunks\n\ndef build_bm25_index(context_chunks):\n    tokenized_corpus = [ctx.split() for ctx in context_chunks]\n    bm25 = BM25Okapi(tokenized_corpus)\n    return bm25\n\ndef hybrid_search(question, bm25, embedder, context_chunks, all_ctx_emb,\n                  k=1, bm25_top_n=5, sim_threshold=0.3):\n    tokenized_query = question.split()\n    bm25_scores = bm25.get_scores(tokenized_query)\n    bm25_top_idx = np.argsort(bm25_scores)[::-1][:bm25_top_n]\n    candidate_contexts = [context_chunks[i] for i in bm25_top_idx]\n    candidate_embs = all_ctx_emb[bm25_top_idx]\n\n    q_emb = embedder.encode([question], convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n    q_emb = np.asarray(q_emb, dtype=np.float32)\n\n    sims = np.dot(candidate_embs, q_emb.T).squeeze(1)\n\n    filtered_idxs = np.where(sims >= sim_threshold)[0]\n    if len(filtered_idxs) == 0:\n        filtered_idxs = np.arange(len(sims))\n\n    filtered_sims = sims[filtered_idxs]\n    filtered_bm25_idx = bm25_top_idx[filtered_idxs]\n\n    bm25_scores_filtered = np.array(bm25_scores)[filtered_bm25_idx]\n\n    bm25_norm = (bm25_scores_filtered - bm25_scores_filtered.min()) / (bm25_scores_filtered.max() - bm25_scores_filtered.min() + 1e-8)\n    sim_norm = (filtered_sims - filtered_sims.min()) / (filtered_sims.max() - filtered_sims.min() + 1e-8)\n\n    combined_scores = 0.5 * bm25_norm + 0.5 * sim_norm\n\n    topk_idx = np.argsort(combined_scores)[::-1][:k]\n    selected_contexts = [context_chunks[filtered_bm25_idx[i]] for i in topk_idx]\n    return selected_contexts\n\ndef generate_answer(question, contexts, tokenizer, model, max_new_tokens=16, num_beams=1):\n    device = next(model.parameters()).device\n    context_text = \"\\n\".join(contexts)\n    prompt = f\"Question: {question}\\nContext:\\n{context_text}\\nAnswer (if no answer, respond with 'no answer'):\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        num_beams=num_beams,\n        do_sample=False,\n        no_repeat_ngram_size=3,\n        early_stopping=True\n    )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):]\n    answer = answer.strip()\n    if answer.lower() in [\"false\", \"no answer\", \"none\", \"\"]:\n        answer = \"\"\n    return answer\n\ndef load_squad_dataset(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data_json = json.load(f)\n    return data_json['data']\n\ndef run_evaluation(dev_data, bm25, embedder, context_chunks, all_ctx_emb, tokenizer, model,\n                   k=1, sim_threshold=0.3,\n                   bm25_top_n=5, num_beams=1, max_new_tokens=16, max_test=500):\n\n    preds = {}\n    count = 0\n    total_qas = sum(len(para[\"qas\"]) for article in dev_data for para in article[\"paragraphs\"])\n    max_test = min(max_test, total_qas)\n\n    with tqdm(total=max_test, desc=f\"Eval QAs top_n={bm25_top_n} beam={num_beams} tokens={max_new_tokens}\") as pbar:\n        for article in dev_data:\n            for para in article[\"paragraphs\"]:\n                for qa in para[\"qas\"]:\n                    if count >= max_test:\n                        break\n                    qid = qa[\"id\"]\n                    question = qa[\"question\"]\n\n                    retrieved_contexts = hybrid_search(\n                        question, bm25, embedder, context_chunks, all_ctx_emb,\n                        k=k, bm25_top_n=bm25_top_n, sim_threshold=sim_threshold)\n                    answer = generate_answer(\n                        question, retrieved_contexts, tokenizer, model,\n                        max_new_tokens=max_new_tokens, num_beams=num_beams)\n                    preds[qid] = answer\n\n                    count += 1\n                    pbar.update(1)\n                if count >= max_test:\n                    break\n            if count >= max_test:\n                break\n\n    # 过滤验证集，保留预测题目\n    filtered_dev_data = []\n    for article in dev_data:\n        new_article = {\"paragraphs\": []}\n        for para in article[\"paragraphs\"]:\n            new_para = {\"qas\": [], \"context\": para[\"context\"]}\n            for qa in para[\"qas\"]:\n                if qa[\"id\"] in preds:\n                    new_para[\"qas\"].append(qa)\n            if new_para[\"qas\"]:\n                new_article[\"paragraphs\"].append(new_para)\n        if new_article[\"paragraphs\"]:\n            filtered_dev_data.append(new_article)\n\n    exact_scores, f1_scores = get_raw_scores(filtered_dev_data, preds)\n    eval_results = make_eval_dict(exact_scores, f1_scores)\n    return eval_results\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_file = '/kaggle/input/squad-train/train-v2.0.json'\n    dev_file = '/kaggle/input/squad-dev/dev-v2.0.json'\n\n    sentence_transformer_name = 'sentence-transformers/all-mpnet-base-v2'\n    model_name = \"google/flan-t5-xl\"\n\n    print(\"加载训练数据...\")\n    train_data = load_squad_dataset(train_file)\n    print(\"加载验证数据...\")\n    dev_data = load_squad_dataset(dev_file)\n\n    print(\"拆分上下文为chunk...\")\n    context_chunks = []\n    for article in train_data:\n        for para in article[\"paragraphs\"]:\n            chunks = chunk_context(para[\"context\"], max_tokens=100)  # 每chunk约100词\n            context_chunks.extend(chunks)\n    print(f\"拆分后上下文chunk数量: {len(context_chunks)}\")\n\n    print(\"加载句向量模型...\")\n    embedder = SentenceTransformer(sentence_transformer_name, device=device)\n\n    print(\"GPU上计算所有chunk向量...\")\n    batch_size_embed = 256\n    all_ctx_emb = []\n    for i in tqdm(range(0, len(context_chunks), batch_size_embed), desc=\"Embedding chunks\"):\n        batch = context_chunks[i:i+batch_size_embed]\n        batch_emb = embedder.encode(batch, convert_to_numpy=True, device=device, normalize_embeddings=True, show_progress_bar=False)\n        all_ctx_emb.append(batch_emb)\n    all_ctx_emb = np.vstack(all_ctx_emb)\n\n    print(\"构建BM25索引...\")\n    bm25 = build_bm25_index(context_chunks)\n\n    print(f\"加载生成模型和tokenizer: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n    model.eval()\n\n    # 大轮测参数空间\n    param_grid = {\n        'bm25_top_n': [4, 6],\n        'num_beams': [2, 3],\n        'max_new_tokens': [64, 16],\n    }\n\n    k = 1\n    sim_threshold = 0.3\n    max_test = 500\n\n    for top_n in param_grid['bm25_top_n']:\n        for beam in param_grid['num_beams']:\n            for max_tokens in param_grid['max_new_tokens']:\n                print(f\"\\n===== 测试参数组合 top_n={top_n}, beam={beam}, max_tokens={max_tokens} =====\")\n                results = run_evaluation(\n                    dev_data, bm25, embedder, context_chunks, all_ctx_emb, tokenizer, model,\n                    k=k,\n                    sim_threshold=sim_threshold,\n                    bm25_top_n=top_n,\n                    num_beams=beam,\n                    max_new_tokens=max_tokens,\n                    max_test=max_test\n                )\n                print(f\"结果: Exact={results['exact']:.2f}, F1={results['f1']:.2f}, 测试样本数={results['total']}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T07:17:52.948643Z","iopub.execute_input":"2025-08-11T07:17:52.949214Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"加载训练数据...\n加载验证数据...\n拆分上下文为chunk...\n拆分后上下文chunk数量: 32102\n加载句向量模型...\nGPU上计算所有chunk向量...\n","output_type":"stream"},{"name":"stderr","text":"Embedding chunks: 100%|██████████| 126/126 [04:56<00:00,  2.35s/it]\n","output_type":"stream"},{"name":"stdout","text":"构建BM25索引...\n加载生成模型和tokenizer: google/flan-t5-xl\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"807a21b3ab3341fc88c2f9a51027dab0"}},"metadata":{}},{"name":"stdout","text":"\n===== 测试参数组合 top_n=4, beam=2, max_tokens=64 =====\n","output_type":"stream"},{"name":"stderr","text":"Eval QAs top_n=4 beam=2 tokens=64: 100%|██████████| 500/500 [04:34<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"结果: Exact=48.20, F1=48.50, 测试样本数=500\n\n===== 测试参数组合 top_n=4, beam=2, max_tokens=16 =====\n","output_type":"stream"},{"name":"stderr","text":"Eval QAs top_n=4 beam=2 tokens=16: 100%|██████████| 500/500 [04:34<00:00,  1.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"结果: Exact=48.20, F1=48.50, 测试样本数=500\n\n===== 测试参数组合 top_n=4, beam=3, max_tokens=64 =====\n","output_type":"stream"},{"name":"stderr","text":"Eval QAs top_n=4 beam=3 tokens=64:  86%|████████▋ | 432/500 [04:21<00:42,  1.61it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"   top_n = 4\n    ，beam = 6\n    ，max_tokens = 16\n    ，k = 1\n    ，sim_threshold = 0.3\n    ，max_test = 500","metadata":{}},{"cell_type":"markdown","source":"调节max char","metadata":{}},{"cell_type":"code","source":"\n\nimport json\nimport collections\nimport re\nimport string\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\nfrom rank_bm25 import BM25Okapi\nimport nltk\n\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\n\n# --------- SQuAD Evaluation Functions ---------\n\ndef normalize_answer(s):\n    if s is None:\n        return \"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    return 2 * precision * recall / (precision + recall)\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                qid = qa['id']\n                gold_answers = [a['text'] for a in qa.get('answers', []) if normalize_answer(a.get('text', ''))]\n                if not gold_answers:\n                    gold_answers = ['']  # no answer questions\n                a_pred = preds.get(qid, \"\")\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\ndef make_eval_dict(exact_scores, f1_scores):\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        ('exact', 100.0 * sum(exact_scores.values()) / total if total else 0),\n        ('f1', 100.0 * sum(f1_scores.values()) / total if total else 0),\n        ('total', total),\n    ])\n\n# --------- 上下文拆分和检索 ---------\n\ndef chunk_context(text, max_tokens=100):\n    sentences = sent_tokenize(text)\n    chunks = []\n    current_chunk = []\n    current_len = 0\n    for sent in sentences:\n        sent_len = len(sent.split())\n        if current_len + sent_len > max_tokens and current_chunk:\n            chunks.append(' '.join(current_chunk))\n            current_chunk = [sent]\n            current_len = sent_len\n        else:\n            current_chunk.append(sent)\n            current_len += sent_len\n    if current_chunk:\n        chunks.append(' '.join(current_chunk))\n    return chunks\n\ndef build_bm25_index(context_chunks):\n    tokenized_corpus = [ctx.split() for ctx in context_chunks]\n    bm25 = BM25Okapi(tokenized_corpus)\n    return bm25\n\ndef hybrid_search(question, bm25, embedder, context_chunks, all_ctx_emb,\n                  k=1, bm25_top_n=5, sim_threshold=0.3):\n    tokenized_query = question.split()\n    bm25_scores = bm25.get_scores(tokenized_query)\n    bm25_top_idx = np.argsort(bm25_scores)[::-1][:bm25_top_n]\n    candidate_contexts = [context_chunks[i] for i in bm25_top_idx]\n    candidate_embs = all_ctx_emb[bm25_top_idx]\n\n    q_emb = embedder.encode([question], convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n    q_emb = np.asarray(q_emb, dtype=np.float32)\n\n    sims = np.dot(candidate_embs, q_emb.T).squeeze(1)\n\n    filtered_idxs = np.where(sims >= sim_threshold)[0]\n    if len(filtered_idxs) == 0:\n        filtered_idxs = np.arange(len(sims))\n\n    filtered_sims = sims[filtered_idxs]\n    filtered_bm25_idx = bm25_top_idx[filtered_idxs]\n\n    bm25_scores_filtered = np.array(bm25_scores)[filtered_bm25_idx]\n\n    bm25_norm = (bm25_scores_filtered - bm25_scores_filtered.min()) / (bm25_scores_filtered.max() - bm25_scores_filtered.min() + 1e-8)\n    sim_norm = (filtered_sims - filtered_sims.min()) / (filtered_sims.max() - filtered_sims.min() + 1e-8)\n\n    combined_scores = 0.5 * bm25_norm + 0.5 * sim_norm\n\n    topk_idx = np.argsort(combined_scores)[::-1][:k]\n    selected_contexts = [context_chunks[filtered_bm25_idx[i]] for i in topk_idx]\n    return selected_contexts\n\ndef generate_answer(question, contexts, tokenizer, model, max_new_tokens=16, num_beams=1, max_context_chars=1500):\n    device = next(model.parameters()).device\n\n    total_chars = 0\n    limited_contexts = []\n    for c in contexts:\n        if total_chars + len(c) > max_context_chars:\n            break\n        limited_contexts.append(c)\n        total_chars += len(c)\n\n    context_text = \"\\n\".join(limited_contexts)\n    prompt = f\"Question: {question}\\nContext:\\n{context_text}\\nAnswer (if no answer, respond with 'no answer'):\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        num_beams=num_beams,\n        do_sample=False,\n        no_repeat_ngram_size=3,\n        early_stopping=True\n    )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):]\n    answer = answer.strip()\n    if answer.lower() in [\"false\", \"no answer\", \"none\", \"\"]:\n        answer = \"\"\n    return answer\n\ndef load_squad_dataset(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data_json = json.load(f)\n    return data_json['data']\n\ndef run_evaluation(dev_data, bm25, embedder, context_chunks, all_ctx_emb, tokenizer, model,\n                   k=1, sim_threshold=0.3,\n                   bm25_top_n=4, num_beams=4, max_new_tokens=16,\n                   max_context_chars=1500,\n                   max_test=500):\n\n    preds = {}\n    count = 0\n    total_qas = sum(len(para[\"qas\"]) for article in dev_data for para in article[\"paragraphs\"])\n    max_test = min(max_test, total_qas)\n\n    with tqdm(total=max_test, desc=f\"Eval QAs top_n={bm25_top_n} beam={num_beams} tokens={max_new_tokens} max_chars={max_context_chars}\") as pbar:\n        for article in dev_data:\n            for para in article[\"paragraphs\"]:\n                for qa in para[\"qas\"]:\n                    if count >= max_test:\n                        break\n                    qid = qa[\"id\"]\n                    question = qa[\"question\"]\n\n                    retrieved_contexts = hybrid_search(\n                        question, bm25, embedder, context_chunks, all_ctx_emb,\n                        k=k, bm25_top_n=bm25_top_n, sim_threshold=sim_threshold)\n                    answer = generate_answer(\n                        question, retrieved_contexts, tokenizer, model,\n                        max_new_tokens=max_new_tokens, num_beams=num_beams,\n                        max_context_chars=max_context_chars)\n                    preds[qid] = answer\n\n                    count += 1\n                    pbar.update(1)\n                if count >= max_test:\n                    break\n            if count >= max_test:\n                break\n\n    filtered_dev_data = []\n    for article in dev_data:\n        new_article = {\"paragraphs\": []}\n        for para in article[\"paragraphs\"]:\n            new_para = {\"qas\": [], \"context\": para[\"context\"]}\n            for qa in para[\"qas\"]:\n                if qa[\"id\"] in preds:\n                    new_para[\"qas\"].append(qa)\n            if new_para[\"qas\"]:\n                new_article[\"paragraphs\"].append(new_para)\n        if new_article[\"paragraphs\"]:\n            filtered_dev_data.append(new_article)\n\n    exact_scores, f1_scores = get_raw_scores(filtered_dev_data, preds)\n    eval_results = make_eval_dict(exact_scores, f1_scores)\n    return eval_results\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_file = '/kaggle/input/squad-train/train-v2.0.json'\n    dev_file = '/kaggle/input/squad-dev/dev-v2.0.json'\n\n    sentence_transformer_name = 'sentence-transformers/all-mpnet-base-v2'\n    model_name = \"google/flan-t5-xl\"\n\n    print(\"加载训练数据...\")\n    train_data = load_squad_dataset(train_file)\n    print(\"加载验证数据...\")\n    dev_data = load_squad_dataset(dev_file)\n\n    print(\"拆分上下文为chunk...\")\n    context_chunks = []\n    for article in train_data:\n        for para in article[\"paragraphs\"]:\n            chunks = chunk_context(para[\"context\"], max_tokens=100)\n            context_chunks.extend(chunks)\n    print(f\"拆分后上下文chunk数量: {len(context_chunks)}\")\n\n    print(\"加载句向量模型...\")\n    embedder = SentenceTransformer(sentence_transformer_name, device=device)\n\n    print(\"GPU上计算所有chunk向量...\")\n    batch_size_embed = 256\n    all_ctx_emb = []\n    for i in tqdm(range(0, len(context_chunks), batch_size_embed), desc=\"Embedding chunks\"):\n        batch = context_chunks[i:i+batch_size_embed]\n        batch_emb = embedder.encode(batch, convert_to_numpy=True, device=device, normalize_embeddings=True, show_progress_bar=False)\n        all_ctx_emb.append(batch_emb)\n    all_ctx_emb = np.vstack(all_ctx_emb)\n\n    print(\"构建BM25索引...\")\n    bm25 = build_bm25_index(context_chunks)\n\n    print(f\"加载生成模型和tokenizer: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n    model.eval()\n\n    max_context_chars_list = [5,10,20,30]\n\n    for max_chars in max_context_chars_list:\n        print(f\"\\n===== 测试 max_context_chars = {max_chars} =====\")\n        results = run_evaluation(\n            dev_data, bm25, embedder, context_chunks, all_ctx_emb, tokenizer, model,\n            k=1,\n            sim_threshold=0.3,\n            bm25_top_n=4,\n            num_beams=8,\n            max_new_tokens=16,\n            max_context_chars=max_chars,\n            max_test=500\n        )\n        print(f\"结果: Exact={results['exact']:.2f}, F1={results['f1']:.2f}, 测试样本数={results['total']}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T08:58:55.585975Z","iopub.execute_input":"2025-08-11T08:58:55.586274Z","execution_failed":"2025-08-11T09:14:27.232Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"加载训练数据...\n加载验证数据...\n拆分上下文为chunk...\n拆分后上下文chunk数量: 32102\n加载句向量模型...\nGPU上计算所有chunk向量...\n","output_type":"stream"},{"name":"stderr","text":"Embedding chunks: 100%|██████████| 126/126 [04:56<00:00,  2.35s/it]\n","output_type":"stream"},{"name":"stdout","text":"构建BM25索引...\n加载生成模型和tokenizer: google/flan-t5-xl\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c01d575b1ba48e7a9cb87515e1f456c"}},"metadata":{}},{"name":"stdout","text":"\n===== 测试 max_context_chars = 5 =====\n","output_type":"stream"},{"name":"stderr","text":"Eval QAs top_n=4 beam=8 tokens=16 max_chars=5: 100%|██████████| 500/500 [04:57<00:00,  1.68it/s]\n","output_type":"stream"},{"name":"stdout","text":"结果: Exact=52.20, F1=52.41, 测试样本数=500\n\n===== 测试 max_context_chars = 10 =====\n","output_type":"stream"},{"name":"stderr","text":"Eval QAs top_n=4 beam=8 tokens=16 max_chars=10: 100%|██████████| 500/500 [04:55<00:00,  1.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"结果: Exact=52.20, F1=52.41, 测试样本数=500\n\n===== 测试 max_context_chars = 20 =====\n","output_type":"stream"},{"name":"stderr","text":"Eval QAs top_n=4 beam=8 tokens=16 max_chars=20:   8%|▊         | 39/500 [00:20<03:33,  2.16it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"Eval QAs top_n=4 beam=8 tokens=16 max_chars=50[04:54<00:00,  1.70it/s]\n结果: Exact=52.20, F1=52.41, 测试样本数=500\n\n===== 测试 max_context_chars = 100 =====\nEval QAs top_n=4 beam=8 tokens=16 max_chars=100[04:54<00:00,  1.70it/s]\n结果: Exact=52.20, F1=52.41, 测试样本数=500","metadata":{}},{"cell_type":"markdown","source":"# beam与tmp","metadata":{}},{"cell_type":"code","source":"import json\nimport collections\nimport re\nimport string\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom tqdm import tqdm\nfrom rank_bm25 import BM25Okapi\nimport nltk\n\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize\n\n# --------- SQuAD Evaluation Functions ---------\ndef normalize_answer(s):\n    if s is None:\n        return \"\"\n    def remove_articles(text):\n        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n    def white_space_fix(text):\n        return ' '.join(text.split())\n    def remove_punc(text):\n        exclude = set(string.punctuation)\n        return ''.join(ch for ch in text if ch not in exclude)\n    def lower(text):\n        return text.lower()\n    return white_space_fix(remove_articles(remove_punc(lower(s))))\n\ndef get_tokens(s):\n    if not s:\n        return []\n    return normalize_answer(s).split()\n\ndef compute_exact(a_gold, a_pred):\n    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n\ndef compute_f1(a_gold, a_pred):\n    gold_toks = get_tokens(a_gold)\n    pred_toks = get_tokens(a_pred)\n    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n    num_same = sum(common.values())\n    if len(gold_toks) == 0 or len(pred_toks) == 0:\n        return int(gold_toks == pred_toks)\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(pred_toks)\n    recall = 1.0 * num_same / len(gold_toks)\n    return 2 * precision * recall / (precision + recall)\n\ndef get_raw_scores(dataset, preds):\n    exact_scores = {}\n    f1_scores = {}\n    for article in dataset:\n        for p in article['paragraphs']:\n            for qa in p['qas']:\n                qid = qa['id']\n                gold_answers = [a['text'] for a in qa.get('answers', []) if normalize_answer(a.get('text', ''))]\n                if not gold_answers:\n                    gold_answers = ['']\n                a_pred = preds.get(qid, \"\")\n                exact_scores[qid] = max(compute_exact(a, a_pred) for a in gold_answers)\n                f1_scores[qid] = max(compute_f1(a, a_pred) for a in gold_answers)\n    return exact_scores, f1_scores\n\ndef make_eval_dict(exact_scores, f1_scores):\n    total = len(exact_scores)\n    return collections.OrderedDict([\n        ('exact', 100.0 * sum(exact_scores.values()) / total if total else 0),\n        ('f1', 100.0 * sum(f1_scores.values()) / total if total else 0),\n        ('total', total),\n    ])\n\n# --------- 上下文拆分和检索 ---------\ndef chunk_context(text, max_tokens=100):\n    sentences = sent_tokenize(text)\n    chunks = []\n    current_chunk = []\n    current_len = 0\n    for sent in sentences:\n        sent_len = len(sent.split())\n        if current_len + sent_len > max_tokens and current_chunk:\n            chunks.append(' '.join(current_chunk))\n            current_chunk = [sent]\n            current_len = sent_len\n        else:\n            current_chunk.append(sent)\n            current_len += sent_len\n    if current_chunk:\n        chunks.append(' '.join(current_chunk))\n    return chunks\n\ndef build_bm25_index(context_chunks):\n    tokenized_corpus = [ctx.split() for ctx in context_chunks]\n    bm25 = BM25Okapi(tokenized_corpus)\n    return bm25\n\ndef hybrid_search(question, bm25, embedder, context_chunks, all_ctx_emb,\n                  k=1, bm25_top_n=5, sim_threshold=0.3):\n    tokenized_query = question.split()\n    bm25_scores = bm25.get_scores(tokenized_query)\n    bm25_top_idx = np.argsort(bm25_scores)[::-1][:bm25_top_n]\n    candidate_contexts = [context_chunks[i] for i in bm25_top_idx]\n    candidate_embs = all_ctx_emb[bm25_top_idx]\n\n    q_emb = embedder.encode([question], convert_to_numpy=True, device='cuda', normalize_embeddings=True, show_progress_bar=False)\n    q_emb = np.asarray(q_emb, dtype=np.float32)\n\n    sims = np.dot(candidate_embs, q_emb.T).squeeze(1)\n\n    filtered_idxs = np.where(sims >= sim_threshold)[0]\n    if len(filtered_idxs) == 0:\n        filtered_idxs = np.arange(len(sims))\n\n    filtered_sims = sims[filtered_idxs]\n    filtered_bm25_idx = bm25_top_idx[filtered_idxs]\n\n    bm25_scores_filtered = np.array(bm25_scores)[filtered_bm25_idx]\n\n    bm25_norm = (bm25_scores_filtered - bm25_scores_filtered.min()) / (bm25_scores_filtered.max() - bm25_scores_filtered.min() + 1e-8)\n    sim_norm = (filtered_sims - filtered_sims.min()) / (filtered_sims.max() - filtered_sims.min() + 1e-8)\n\n    combined_scores = 0.5 * bm25_norm + 0.5 * sim_norm\n\n    topk_idx = np.argsort(combined_scores)[::-1][:k]\n    selected_contexts = [context_chunks[filtered_bm25_idx[i]] for i in topk_idx]\n    return selected_contexts\n\ndef generate_answer(question, contexts, tokenizer, model, max_new_tokens=16, num_beams=1, temperature=0.0, max_context_chars=1500):\n    device = next(model.parameters()).device\n    total_chars = 0\n    limited_contexts = []\n    for c in contexts:\n        if total_chars + len(c) > max_context_chars:\n            break\n        limited_contexts.append(c)\n        total_chars += len(c)\n\n    context_text = \"\\n\".join(limited_contexts)\n    prompt = f\"Question: {question}\\nContext:\\n{context_text}\\nAnswer (if no answer, respond with 'no answer'):\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    outputs = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        num_beams=num_beams,\n        temperature=temperature,\n        do_sample=(temperature > 0),\n        no_repeat_ngram_size=3,\n        early_stopping=True\n    )\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    if answer.startswith(prompt):\n        answer = answer[len(prompt):]\n    answer = answer.strip()\n    if answer.lower() in [\"false\", \"no answer\", \"none\", \"\"]:\n        answer = \"\"\n    return answer\n\ndef load_squad_dataset(path):\n    with open(path, 'r', encoding='utf-8') as f:\n        data_json = json.load(f)\n    return data_json['data']\n\ndef run_evaluation(dev_data, bm25, embedder, context_chunks, all_ctx_emb, tokenizer, model,\n                   k=1, sim_threshold=0.3,\n                   bm25_top_n=4, num_beams=4, temperature=0.0,\n                   max_new_tokens=16,\n                   max_context_chars=1500,\n                   max_test=500):\n\n    preds = {}\n    count = 0\n    total_qas = sum(len(para[\"qas\"]) for article in dev_data for para in article[\"paragraphs\"])\n    max_test = min(max_test, total_qas)\n\n    with tqdm(total=max_test, desc=f\"Eval QAs beam={num_beams} temp={temperature}\") as pbar:\n        for article in dev_data:\n            for para in article[\"paragraphs\"]:\n                for qa in para[\"qas\"]:\n                    if count >= max_test:\n                        break\n                    qid = qa[\"id\"]\n                    question = qa[\"question\"]\n\n                    retrieved_contexts = hybrid_search(\n                        question, bm25, embedder, context_chunks, all_ctx_emb,\n                        k=k, bm25_top_n=bm25_top_n, sim_threshold=sim_threshold)\n                    answer = generate_answer(\n                        question, retrieved_contexts, tokenizer, model,\n                        max_new_tokens=max_new_tokens, num_beams=num_beams,\n                        temperature=temperature,\n                        max_context_chars=max_context_chars)\n                    preds[qid] = answer\n\n                    count += 1\n                    pbar.update(1)\n                if count >= max_test:\n                    break\n            if count >= max_test:\n                break\n\n    filtered_dev_data = []\n    for article in dev_data:\n        new_article = {\"paragraphs\": []}\n        for para in article[\"paragraphs\"]:\n            new_para = {\"qas\": [], \"context\": para[\"context\"]}\n            for qa in para[\"qas\"]:\n                if qa[\"id\"] in preds:\n                    new_para[\"qas\"].append(qa)\n            if new_para[\"qas\"]:\n                new_article[\"paragraphs\"].append(new_para)\n        if new_article[\"paragraphs\"]:\n            filtered_dev_data.append(new_article)\n\n    exact_scores, f1_scores = get_raw_scores(filtered_dev_data, preds)\n    eval_results = make_eval_dict(exact_scores, f1_scores)\n    return eval_results\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_file = '/kaggle/input/squad-train/train-v2.0.json'\n    dev_file = '/kaggle/input/squad-dev/dev-v2.0.json'\n\n    sentence_transformer_name = 'sentence-transformers/all-mpnet-base-v2'\n    model_name = \"google/flan-t5-xl\"\n\n    print(\"加载训练数据...\")\n    train_data = load_squad_dataset(train_file)\n    print(\"加载验证数据...\")\n    dev_data = load_squad_dataset(dev_file)\n\n    print(\"拆分上下文为chunk...\")\n    context_chunks = []\n    for article in train_data:\n        for para in article[\"paragraphs\"]:\n            chunks = chunk_context(para[\"context\"], max_tokens=100)\n            context_chunks.extend(chunks)\n    print(f\"拆分后上下文chunk数量: {len(context_chunks)}\")\n\n    print(\"加载句向量模型...\")\n    embedder = SentenceTransformer(sentence_transformer_name, device=device)\n\n    print(\"GPU上计算所有chunk向量...\")\n    batch_size_embed = 256\n    all_ctx_emb = []\n    for i in tqdm(range(0, len(context_chunks), batch_size_embed), desc=\"Embedding chunks\"):\n        batch = context_chunks[i:i+batch_size_embed]\n        batch_emb = embedder.encode(batch, convert_to_numpy=True, device=device, normalize_embeddings=True, show_progress_bar=False)\n        all_ctx_emb.append(batch_emb)\n    all_ctx_emb = np.vstack(all_ctx_emb)\n\n    print(\"构建BM25索引...\")\n    bm25 = build_bm25_index(context_chunks)\n\n    print(f\"加载生成模型和tokenizer: {model_name}\")\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n    model.eval()\n\n    beam_sizes = [16, 20, 25]\n    temperatures = [0.0, 0.3, 0.7, 1.0]\n\n    for beam in beam_sizes:\n        for temp in temperatures:\n            print(f\"\\n===== 测试 beam={beam}, temperature={temp} =====\")\n            results = run_evaluation(\n                dev_data, bm25, embedder, context_chunks, all_ctx_emb, tokenizer, model,\n                k=1,\n                sim_threshold=0.3,\n                bm25_top_n=4,\n                num_beams=beam,\n                temperature=temp,\n                max_new_tokens=16,\n                max_context_chars=10,\n                max_test=500\n            )\n            print(f\"结果: Exact={results['exact']:.2f}, F1={results['f1']:.2f}, 测试样本数={results['total']}\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T09:19:10.239734Z","iopub.execute_input":"2025-08-11T09:19:10.240087Z","iopub.status.idle":"2025-08-11T09:47:50.725525Z","shell.execute_reply.started":"2025-08-11T09:19:10.240054Z","shell.execute_reply":"2025-08-11T09:47:50.724170Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"加载训练数据...\n加载验证数据...\n拆分上下文为chunk...\n拆分后上下文chunk数量: 32102\n加载句向量模型...\nGPU上计算所有chunk向量...\n","output_type":"stream"},{"name":"stderr","text":"Embedding chunks: 100%|██████████| 126/126 [04:45<00:00,  2.27s/it]\n","output_type":"stream"},{"name":"stdout","text":"构建BM25索引...\n加载生成模型和tokenizer: google/flan-t5-xl\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51d2e0505220413ab55f7cdc5589a852"}},"metadata":{}},{"name":"stdout","text":"\n===== 测试 beam=16, temperature=0.0 =====\n","output_type":"stream"},{"name":"stderr","text":"Eval QAs beam=16 temp=0.0:   0%|          | 0/500 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nEval QAs beam=16 temp=0.0: 100%|██████████| 500/500 [05:17<00:00,  1.57it/s]\n","output_type":"stream"},{"name":"stdout","text":"结果: Exact=52.20, F1=52.40, 测试样本数=500\n\n===== 测试 beam=16, temperature=0.3 =====\n","output_type":"stream"},{"name":"stderr","text":"Eval QAs beam=16 temp=0.3: 100%|██████████| 500/500 [04:03<00:00,  2.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"结果: Exact=52.20, F1=52.33, 测试样本数=500\n\n===== 测试 beam=16, temperature=0.7 =====\n","output_type":"stream"},{"name":"stderr","text":"Eval QAs beam=16 temp=0.7: 100%|██████████| 500/500 [04:07<00:00,  2.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"结果: Exact=52.40, F1=52.56, 测试样本数=500\n\n===== 测试 beam=16, temperature=1.0 =====\n","output_type":"stream"},{"name":"stderr","text":"Eval QAs beam=16 temp=1.0: 100%|██████████| 500/500 [04:16<00:00,  1.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"结果: Exact=52.20, F1=52.33, 测试样本数=500\n\n===== 测试 beam=20, temperature=0.0 =====\n","output_type":"stream"},{"name":"stderr","text":"Eval QAs beam=20 temp=0.0: 100%|██████████| 500/500 [05:35<00:00,  1.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"结果: Exact=52.20, F1=52.40, 测试样本数=500\n\n===== 测试 beam=20, temperature=0.3 =====\n","output_type":"stream"},{"name":"stderr","text":"Eval QAs beam=20 temp=0.3:   5%|▌         | 27/500 [00:12<03:41,  2.14it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_918/2554639557.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_918/2554639557.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemperatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n===== 测试 beam={beam}, temperature={temp} =====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             results = run_evaluation(\n\u001b[0m\u001b[1;32m    272\u001b[0m                 \u001b[0mdev_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbm25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_ctx_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_918/2554639557.py\u001b[0m in \u001b[0;36mrun_evaluation\u001b[0;34m(dev_data, bm25, embedder, context_chunks, all_ctx_emb, tokenizer, model, k, sim_threshold, bm25_top_n, num_beams, temperature, max_new_tokens, max_context_chars, max_test)\u001b[0m\n\u001b[1;32m    190\u001b[0m                         \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbm25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_ctx_emb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                         k=k, bm25_top_n=bm25_top_n, sim_threshold=sim_threshold)\n\u001b[0;32m--> 192\u001b[0;31m                     answer = generate_answer(\n\u001b[0m\u001b[1;32m    193\u001b[0m                         \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretrieved_contexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                         \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_beams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_918/2554639557.py\u001b[0m in \u001b[0;36mgenerate_answer\u001b[0;34m(question, contexts, tokenizer, model, max_new_tokens, num_beams, temperature, max_context_chars)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     outputs = model.generate(\n\u001b[0m\u001b[1;32m    147\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m             \u001b[0;31m# 14. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1595\u001b[0;31m             return self.beam_sample(\n\u001b[0m\u001b[1;32m   1596\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mbeam_sample\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3274\u001b[0m             \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_inputs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3276\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   3277\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3278\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1742\u001b[0m         \u001b[0;31m# Decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1744\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1108\u001b[0m                 )\n\u001b[1;32m   1109\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1111\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mquery_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m             cross_attention_outputs = self.layer[1](\n\u001b[0m\u001b[1;32m    725\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                 \u001b[0mkey_value_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":1}]}